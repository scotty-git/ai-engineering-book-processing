{
  "id": "ch07",
  "title": "Chapter 7. Finetuning",
  "sections": [
    {
      "type": "chapter_title",
      "content": "Chapter 7. Finetuning",
      "id": "chapter-title",
      "raw_html": "<h1><span class=\"label\">Chapter 7. </span>Finetuning</h1>"
    },
    {
      "type": "paragraph",
      "content": "Finetuning is the process of adapting a model to a specific task by further training the whole model or part of the model. Chapters 5 and 6 discuss prompt-based methods, which adapt a model by giving it instructions, context, and tools. Finetuning adapts a model by adjusting its weights.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-type=\"indexterm\" id=\"ch07.html0\"></a>Finetuning is the process of adapting a model to a specific task by further training the whole model or part of the model. Chapters <a data-type=\"xref\" data-xrefstyle=\"select:labelnumber\" href=\"ch05.html#ch05a_prompt_engineering_1730156991195551\">5</a> and <a data-type=\"xref\" data-xrefstyle=\"select:labelnumber\" href=\"ch06.html#ch06_rag_and_agents_1730157386571386\">6</a> discuss prompt-based methods, which adapt a model by giving it instructions, context, and tools. Finetuning adapts a model by adjusting its weights.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Finetuning can enhance various aspects of a model. It can improve the model’s domain-specific capabilities, such as coding or medical question answering, and can also strengthen its safety. However, it is most often used to improve the model’s instruction-following ability, particularly to ensure it adheres to specific output styles and formats.",
      "raw_html": "<p>Finetuning can enhance various aspects of a model. It can improve the model’s domain-specific capabilities, such as coding or medical question answering, and can also strengthen its safety. However, it is most often used to improve the model’s instruction-following ability, particularly to ensure it adheres to specific output styles and formats.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While finetuning can help create models that are more customized to your needs, it also requires more up-front investment. A question I hear very often is when to finetune and when to do RAG. After an overview of finetuning, this chapter will discuss the reasons for finetuning and the reasons for not finetuning, as well as a simple framework for thinking about choosing between finetuning and alternate methods.",
      "raw_html": "<p>While finetuning can help create models that are more customized to your needs, it also requires more up-front investment. A question I hear very often is when to finetune and when to do RAG. After an overview of finetuning, this chapter will discuss the reasons for finetuning and the reasons for not finetuning, as well as a simple framework for thinking about choosing between finetuning and alternate methods. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Compared to prompt-based methods, finetuning incurs a much higher memory footprint. At the scale of today’s foundation models, naive finetuning often requires more memory than what’s available on a single GPU. This makes finetuning expensive and challenging to do. As discussed throughout this chapter, reducing memory requirements is a primary motivation for many finetuning techniques. This chapter dedicates one section to outlining factors contributing to a model’s memory footprint, which is important for understanding these techniques.",
      "raw_html": "<p>Compared to prompt-based methods, finetuning incurs a much higher memory footprint. At the scale of today’s foundation models, naive finetuning often requires more memory than what’s available on a single GPU. This makes finetuning expensive and challenging to do. As discussed throughout this chapter, reducing memory requirements is a primary motivation for many finetuning techniques. This chapter dedicates one section to outlining factors contributing to a model’s memory footprint, which is important for understanding these techniques.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A memory-efficient approach that has become dominant in the finetuning space is PEFT (parameter-efficient finetuning). This chapter explores PEFT and how it differs from traditional finetuning; this chapter also provides an overview of its evolving techniques. I’ll focus particularly on one compelling category: adapter-based techniques.",
      "raw_html": "<p>A memory-efficient approach that has become dominant in the finetuning space is PEFT (parameter-efficient finetuning). This chapter explores PEFT and how it differs from traditional finetuning; this chapter also provides an overview of its evolving techniques. I’ll focus particularly on one compelling category: adapter-based <span class=\"keep-together\">techniques.</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "With prompt-based methods, knowledge about how ML models operate under the hood is recommended but not strictly necessary. However, finetuning brings you to the realm of model training, where ML knowledge is required. ML basics are beyond the scope of this book. If you want a quick refresh, the book’s GitHub repository has pointers to helpful resources. In this chapter, I’ll cover a few core concepts immediately relevant to the discussion.",
      "raw_html": "<p>With prompt-based methods, knowledge about how ML models operate under the hood is recommended but not strictly necessary. However, finetuning brings you to the realm of model training, where ML knowledge is required. ML basics are beyond the scope of this book. If you want a quick refresh, the book’s <a href=\"https://github.com/chiphuyen/aie-book\">GitHub repository</a> has pointers to helpful resources. In this chapter, I’ll cover a few core concepts immediately relevant to the discussion.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This chapter is the most technically challenging one for me to write, not because of the complexity of the concepts, but because of the broad scope these concepts cover. I suspect it might also be technically challenging to read. If, at any point, you feel like you’re diving too deep into details that aren’t relevant to your work, feel free to skip.",
      "raw_html": "<p>This chapter is the most technically challenging one for me to write, not because of the complexity of the concepts, but because of the broad scope these concepts cover. I suspect it might also be technically challenging to read. If, at any point, you feel like you’re diving too deep into details that aren’t relevant to your work, feel free to skip.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "There’s a lot to discuss. Let’s dive in!",
      "raw_html": "<p>There’s a lot to discuss. Let’s dive in!</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Finetuning Overview",
      "id": "heading-7",
      "raw_html": "<h1>Finetuning Overview</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "To finetune, you start with a base model that has some, but not all, of the capabilities you need. The goal of finetuning is to get this model to perform well enough for your specific task.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"overview\" data-type=\"indexterm\" id=\"ch07.html1\"></a>To finetune, you start with a base model that has some, but not all, of the capabilities you need. The goal of finetuning is to get this model to perform well enough for your specific task.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Finetuning is one way to do transfer learning, a concept first introduced by Bozinovski and Fulgosi in 1976. Transfer learning focuses on how to transfer the knowledge gained from one task to accelerate learning for a new, related task. This is conceptually similar to how humans transfer skills: for example, knowing how to play the piano can make it easier to learn another musical instrument.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"transfer learning\" data-type=\"indexterm\" id=\"id1360\"></a>Finetuning is one way to do <em>transfer learning</em>, a concept first introduced by <a href=\"https://oreil.ly/Udw0Z\">Bozinovski and Fulgosi</a> in 1976. Transfer learning focuses on how to transfer the knowledge gained from one task to accelerate learning for a new, related task. This is conceptually similar to how humans transfer skills: for example, knowing how to play the piano can make it easier to learn another musical instrument.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "An early large-scale success in transfer learning was Google’s multilingual translation system (Johnson et. al, 2016). The model transferred its knowledge of Portuguese–English and English–Spanish translation to directly translate Portuguese to Spanish, even though there were no Portuguese–Spanish examples in the training data.",
      "raw_html": "<p>An early large-scale success in transfer learning was Google’s multilingual translation system (<a href=\"https://arxiv.org/abs/1611.04558\">Johnson et. al, 2016</a>). The model transferred its knowledge of Portuguese–English and English–Spanish translation to directly translate Portuguese to Spanish, even though there were no Portuguese–Spanish examples in the training data.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Since the early days of deep learning, transfer learning has offered a solution for tasks with limited or expensive training data. By training a base model on tasks with abundant data, you can then transfer that knowledge to a target task.",
      "raw_html": "<p>Since the early days of deep learning, transfer learning has offered a solution for tasks with limited or expensive training data. By training a base model on tasks with abundant data, you can then transfer that knowledge to a target task.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "For LLMs, knowledge gained from pre-training on text completion (a task with abundant data) is transferred to more specialized tasks, like legal question answering or text-to-SQL, which often have less available data. This capability for transfer learning makes foundation models particularly valuable.",
      "raw_html": "<p>For LLMs, knowledge gained from pre-training on text completion (a task with abundant data) is transferred to more specialized tasks, like legal question answering or text-to-SQL, which often have less available data. This capability for transfer learning makes foundation models particularly valuable.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Transfer learning improves sample efficiency, allowing a model to learn the same behavior with fewer examples. A sample-efficient model learns effectively from fewer samples. For example, while training a model from scratch for legal question answering may need millions of examples, finetuning a good base model might only require a few hundred.",
      "raw_html": "<p class=\"pagebreak-before\">Transfer learning improves <em>sample efficiency</em>, allowing a model to learn the same behavior with fewer examples. A <em>sample-efficient</em> model learns effectively from fewer samples. For example, while training a model from scratch for legal question answering may need millions of examples, finetuning a good base model might only require a few hundred.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Ideally, much of what the model needs to learn is already present in the base model, and finetuning just refines the model’s behavior. OpenAI’s InstructGPT paper (2022) suggested viewing finetuning as unlocking the capabilities a model already has but that are difficult for users to access via prompting alone.",
      "raw_html": "<p>Ideally, much of what the model needs to learn is already present in the base model, and finetuning just refines the model’s behavior. OpenAI’s <a href=\"https://oreil.ly/5-5lw\">InstructGPT paper</a> (2022) suggested viewing finetuning as unlocking the capabilities a model already has but that are difficult for users to access via prompting alone.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "Finetuning isn’t the only way to do transfer learning. Another approach is feature-based transfer. In this approach, a model is trained to extract features from the data, usually as embedding vectors, which are then used by another model. I mention feature-based transfer briefly in Chapter 2, when discussing how part of a foundation model can be reused for a classification task by adding a classifier head.\n\nFeature-based transfer is very common in computer vision. For instance, in the second half of the 2010s, many people used models trained on the ImagetNet dataset to extract features from images and use these features in other computer vision tasks such as object detection or image segmentation.",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p><a contenteditable=\"false\" data-primary=\"feature-based transfers\" data-type=\"indexterm\" id=\"id1361\"></a>Finetuning isn’t the only way to do transfer learning. Another approach is <em>feature-based transfer</em>. In this approach, a model is trained to extract features from the data, usually as embedding vectors, which are then used by another model. I mention feature-based transfer briefly in <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a>, when discussing how part of a foundation model can be reused for a classification task by <em>adding a classifier head</em>.</p>\n<p>Feature-based transfer is very common in computer vision. For instance, in the second half of the 2010s, many people used models trained on the ImagetNet dataset to extract features from images and use these features in other computer vision tasks such as object detection or image segmentation.</p>\n</div>"
    },
    {
      "type": "paragraph",
      "content": "Finetuning is part of a model’s training process. It’s an extension of model pre-training. Because any training that happens after pre-training is finetuning, finetuning can take many different forms. Chapter 2 already discussed two types of finetuning: supervised finetuning and preference finetuning. Let’s do a quick recap of these methods and how you might leverage them as an application developer.",
      "raw_html": "<p>Finetuning is part of a model’s training process. It’s an extension of model pre-training. Because any training that happens after pre-training is finetuning, finetuning can take many different forms. <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a> already discussed two types of finetuning: <a contenteditable=\"false\" data-primary=\"preference finetuning\" data-type=\"indexterm\" id=\"id1362\"></a><a contenteditable=\"false\" data-primary=\"SFT (supervised finetuning)\" data-type=\"indexterm\" id=\"id1363\"></a><a contenteditable=\"false\" data-primary=\"supervised finetuning (SFT)\" data-type=\"indexterm\" id=\"id1364\"></a>supervised finetuning and preference finetuning. Let’s do a quick recap of these methods and how you might leverage them as an application developer.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Recall that a model’s training process starts with pre-training, which is usually done with self-supervision. Self-supervision allows the model to learn from a large amount of unlabeled data. For language models, self-supervised data is typically just sequences of text that don’t need annotations.",
      "raw_html": "<p>Recall that a model’s training process starts with <em>pre-training</em>, which is usually done with self-supervision. Self-supervision allows the model to learn from a large amount of unlabeled data. For language models, self-supervised data is typically just <em>sequences of text</em> that don’t need annotations.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Before finetuning this pre-trained model with expensive task-specific data, you can finetune it with self-supervision using cheap task-related data. For example, to finetune a model for legal question answering, before finetuning it on expensive annotated (question, answer) data, you can finetune it on raw legal documents. Similarly, to finetune a model to do book summarization in Vietnamese, you can first finetune it on a large collection of Vietnamese text. Self-supervised finetuning is also called continued pre-training.",
      "raw_html": "<p>Before finetuning this pre-trained model with expensive task-specific data, you can finetune it with self-supervision using cheap task-related data. For example, to finetune a model for legal question answering, before finetuning it on expensive annotated (question, answer) data, you can finetune it on raw legal documents. Similarly, to finetune a model to do book summarization in Vietnamese, you can first finetune it on a large collection of Vietnamese text. <em>Self-supervised finetuning</em> is also called <em>continued pre-training</em>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "As discussed in Chapter 1, language models can be autoregressive or masked. An autoregressive model predicts the next token in a sequence using the previous tokens as the context. A masked model fills in the blank using the tokens both before and after it. Similarly, with supervised finetuning, you can also finetune a model to predict the next token or fill in the blank. The latter, also known as infilling finetuning, is especially useful for tasks such as text editing and code debugging. You can finetune a model for infilling even if it was pre-trained autoregressively.",
      "raw_html": "<p class=\"pagebreak-before\">As discussed in <a data-type=\"xref\" href=\"ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319\">Chapter 1</a>, language models can be autoregressive or masked. An autoregressive model predicts the next token in a sequence using the previous tokens as the context. A masked model fills in the blank using the tokens both before and after it. Similarly, with supervised finetuning, you can also finetune a model to predict the next token or fill in the blank. The latter, also known as <em>infilling finetuning</em>, is especially useful for tasks such as text editing and code debugging. You can finetune a model for infilling even if it was pre-trained autoregressively.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The massive amount of data a model can learn from during self-supervised learning outfits the model with a rich understanding of the world, but it might be hard for users to extract that knowledge for their tasks, or the way the model behaves might be misaligned with human preference. Supervised finetuning uses high-quality annotated data to refine the model to align with human usage and preference.",
      "raw_html": "<p>The massive amount of data a model can learn from during self-supervised learning outfits the model with a rich understanding of the world, but it might be hard for users to extract that knowledge for their tasks, or the way the model behaves might be misaligned with human preference. Supervised finetuning uses high-quality annotated data to refine the model to align with human usage and preference.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "During supervised finetuning, the model is trained using (input, output) pairs: the input can be an instruction and the output can be a response. A response can be open-ended, such as for the task of book summarization. A response can be also close-ended, such as for a classification task. High-quality instruction data can be challenging and expensive to create, especially for instructions that require factual consistency, domain expertise, or political correctness. Chapter 8 discusses how to acquire instruction data.",
      "raw_html": "<p>During <em>supervised finetuning</em>, the model is trained using (input, output) pairs: the input can be an instruction and the output can be a response. A response can be open-ended, such as for the task of book summarization. A response can be also close-ended, such as for a classification task. High-quality instruction data can be challenging and expensive to create, especially for instructions that require factual consistency, domain expertise, or political correctness. <a data-type=\"xref\" href=\"ch08.html#ch08_dataset_engineering_1730130932019888\">Chapter 8</a> discusses how to acquire instruction data.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "A model can also be finetuned with reinforcement learning to generate responses that maximize human preference. Preference finetuning requires comparative data that typically follows the format (instruction, winning response, losing response).",
      "raw_html": "<p>A model can also be finetuned with reinforcement learning to generate responses that maximize human preference. Preference finetuning requires comparative data that typically follows the format (instruction, winning response, losing response).</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "It’s possible to finetune a model to extend its context length. Long-context finetuning typically requires modifying the model’s architecture, such as adjusting the positional embeddings. A long sequence means more possible positions for tokens, and positional embeddings should be able to handle them. Compared to other finetuning techniques, long-context finetuning is harder to do. The resulting model might also degrade on shorter sequences.",
      "raw_html": "<p>It’s possible to finetune a model to extend its context length. <em>Long-context finetuning</em> typically requires modifying the model’s architecture, such as adjusting the positional embeddings. A long sequence means more possible positions for tokens, and positional embeddings should be able to handle them. Compared to other finetuning techniques, long-context finetuning is harder to do. The resulting model might also degrade on shorter sequences.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Figure 7-1 shows the making of different Code Llama models (Rozière et al., 2024), from the base model Llama 2, using different finetuning techniques. Using long-context finetuning, they were able to increase the model’s maximum context length from 4,096 tokens to 16,384 tokens to accommodate longer code files. In the image, instruction finetuning refers to supervised finetuning.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch07a_figure_1_1730160615799658\">Figure 7-1</a> <a contenteditable=\"false\" data-primary=\"Llama\" data-secondary=\"finetuning\" data-type=\"indexterm\" id=\"id1365\"></a>shows the making of different Code Llama models (<a href=\"https://arxiv.org/abs/2308.12950\">Rozière et al., 2024</a>), from the base model Llama 2, using different finetuning techniques. Using long-context finetuning, they were able to increase the model’s maximum context length from 4,096 tokens to 16,384 tokens to accommodate longer code files. In the image, instruction finetuning refers to supervised <span class=\"keep-together\">finetuning.</span></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Finetuning can be done by both model developers and application developers. Model developers typically post-train a model with different finetuning techniques before releasing it. A model developer might also release different model versions, each finetuned to a different extent, so that application developers can choose the version that works best for them.",
      "raw_html": "<p>Finetuning can be done by both model developers and application developers. Model developers typically post-train a model with different finetuning techniques before releasing it. A model developer might also release different model versions, each finetuned to a different extent, so that application developers can choose the version that works best for them.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07a_figure_1_1730160615799658",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07a_figure_1_1730160615799658\">\n<img alt=\"A diagram of a program\n\nDescription automatically generated\" src=\"assets/aien_0701.png\"/>\n<h6><span class=\"label\">Figure 7-1. </span>Different finetuning techniques used to make different Code Llama models. Image from the Rozière et al. (2024). Adapted from an original image licensed under CC BY 4.0.</h6>\n</div></figure>",
      "image": "aien_0701.png",
      "alt": "A diagram of a program\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0701.png",
      "caption": {
        "label": "Figure 7-1.",
        "text": "Different finetuning techniques used to make different Code Llama models. Image from the Rozière et al. (2024). Adapted from an original image licensed under CC BY 4.0."
      }
    },
    {
      "type": "paragraph",
      "content": "As an application developer, you might finetune a pre-trained model, but most likely, you’ll finetune a model that has been post-trained. The more refined a model is and the more relevant its knowledge is to your task, the less work you’ll have to do to adapt it.",
      "raw_html": "<p>As an application developer, you might finetune a pre-trained model, but most likely, you’ll finetune a model that has been post-trained. The more refined a model is and the more relevant its knowledge is to your task, the less work you’ll have to do to adapt it.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html1\" data-type=\"indexterm\" id=\"id1366\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "When to Finetune",
      "id": "heading-7",
      "raw_html": "<h1>When to Finetune</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "Before jumping into different finetuning techniques, it’s necessary to consider whether finetuning is the right option for you. Compared to prompt-based methods, finetuning requires significantly more resources, not just in data and hardware, but also in ML talent. Therefore, finetuning is generally attempted after extensive experiments with prompt-based methods. However, finetuning and prompting aren’t mutually exclusive. Real-world problems often require both approaches.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"when to finetune\" data-type=\"indexterm\" id=\"ch07.html2\"></a>Before jumping into different finetuning techniques, it’s necessary to consider whether finetuning is the right option for you. Compared to prompt-based methods, finetuning requires significantly more resources, not just in data and hardware, but also in ML talent. Therefore, finetuning is generally attempted <em>after</em> extensive experiments with prompt-based methods. However, finetuning and prompting aren’t mutually exclusive. Real-world problems often require both approaches.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Reasons to Finetune",
      "id": "heading-7",
      "raw_html": "<h2>Reasons to Finetune</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "The primary reason for finetuning is to improve a model’s quality, in terms of both general capabilities and task-specific capabilities. Finetuning is commonly used to improve a model’s ability to generate outputs following specific structures, such as JSON or YAML formats.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"when to finetune\" data-tertiary=\"reasons to finetune\" data-type=\"indexterm\" id=\"id1367\"></a>The primary reason for finetuning is to improve a model’s quality, in terms of both general capabilities and task-specific capabilities. Finetuning is commonly used to improve a model’s ability to generate outputs following specific structures, such as JSON or YAML formats.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A general-purpose model that performs well on a wide range of benchmarks might not perform well on your specific task. If the model you want to use wasn’t sufficiently trained on your task, finetuning it with your data can be especially useful.",
      "raw_html": "<p>A general-purpose model that performs well on a wide range of benchmarks might not perform well on your specific task. If the model you want to use wasn’t sufficiently trained on your task, finetuning it with your data can be especially useful.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "For example, an out-of-the-box model might be good at converting from text to the standard SQL dialect but might fail with a less common SQL dialect. In this case, finetuning this model on data containing this SQL dialect will help. Similarly, if the model works well on standard SQL for common queries but often fails for customer-specific queries, finetuning the model on customer-specific queries might help.",
      "raw_html": "<p>For example, an out-of-the-box model might be good at converting from text to the standard SQL dialect but might fail with a less common SQL dialect. In this case, finetuning this model on data containing this SQL dialect will help. Similarly, if the model works well on standard SQL for common queries but often fails for customer-specific queries, finetuning the model on customer-specific queries might help.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One especially interesting use case of finetuning is bias mitigation. The idea is that if the base model perpetuates certain biases from its training data, exposing it to carefully curated data during finetuning can counteract these biases (Wang and Russakovsky, 2023). For example, if a model consistently assigns CEOs male-sounding names, finetuning it on a dataset with many female CEOs can mitigate this bias. Garimella et al. (2022) found that finetuning BERT-like language models on text authored by women can reduce these models’ gender biases, while finetuning them on texts by African authors can reduce racial biases.",
      "raw_html": "<p>One especially interesting use case of finetuning is bias mitigation. The idea is that if the base model perpetuates certain biases from its training data, exposing it to carefully curated data during finetuning can counteract these biases (<a href=\"https://oreil.ly/iPwB_\">Wang and Russakovsky, 2023</a>). For example, if a model consistently assigns CEOs male-sounding names, finetuning it on a dataset with many female CEOs can mitigate this bias. <a href=\"https://oreil.ly/RoPL4\">Garimella et al. (2022)</a> found that finetuning BERT-like language models on text authored by women can reduce these models’ gender biases, while finetuning them on texts by African authors can reduce racial biases.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "You can finetune a big model to make it even better, but finetuning smaller models is much more common. Smaller models require less memory, and, therefore, are easier to finetune. They are also cheaper and faster to use in production.",
      "raw_html": "<p>You can finetune a big model to make it even better, but finetuning smaller models is much more common. Smaller models require less memory, and, therefore, are easier to finetune. They are also cheaper and faster to use in production.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A common approach is to finetune a small model to imitate the behavior of a larger model using data generated by this large model. Because this approach distills the larger model’s knowledge into the smaller model, it’s called distillation. This is discussed in Chapter 8 together with other data synthesis techniques.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"distillation\" data-type=\"indexterm\" id=\"id1368\"></a>A common approach is to finetune a small model to imitate the behavior of a larger model using data generated by this large model. Because this approach distills the larger model’s knowledge into the smaller model, it’s called <em>distillation</em>. This is discussed in <a data-type=\"xref\" href=\"ch08.html#ch08_dataset_engineering_1730130932019888\">Chapter 8</a> together with other data synthesis techniques.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "A small model, finetuned on a specific task, might outperform a much larger out-of-the-box model on that task. For example, Grammarly found that their finetuned Flan-T5 models (Chung et al., 2022) outperformed a GPT-3 variant specialized in text editing across a wide range of writing assistant tasks despite being 60 times smaller. The finetuning process used only 82,000 (instruction, output) pairs, which is smaller than the data typically needed to train a text-editing model from scratch.",
      "raw_html": "<p>A small model, finetuned on a specific task, might outperform a much larger out-of-the-box model on that task. For example, Grammarly found that their finetuned Flan-T5 models (<a href=\"https://arxiv.org/abs/2210.11416\">Chung et al., 2022</a>) outperformed a GPT-3 variant specialized in text editing across a wide range of writing assistant tasks despite being 60 times smaller. The finetuning process used only 82,000 (instruction, output) pairs, which is smaller than the data typically needed to train a text-editing model from scratch.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In the early days of foundation models, when the strongest models were commercial with limited finetuning access, there weren’t many competitive models available for finetuning. However, as the open source community proliferates with high-quality models of all sizes, tailored for a wide variety of domains, finetuning has become a lot more viable and attractive.",
      "raw_html": "<p>In the early days of foundation models, when the strongest models were commercial with limited finetuning access, there weren’t many competitive models available for finetuning. However, as the open source community proliferates with high-quality models of all sizes, tailored for a wide variety of domains, finetuning has become a lot more viable and attractive.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Reasons Not to Finetune",
      "id": "heading-7",
      "raw_html": "<h2>Reasons Not to Finetune</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "While finetuning can improve a model in many ways, many of these improvements can also be achieved, to a certain extent, without finetuning. Finetuning can improve a model’s performance, but so do carefully crafted prompts and context. Finetuning can help with structured outputs, but many other techniques, as discussed in Chapter 2, can also do that.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"when to finetune\" data-tertiary=\"reasons not to finetune\" data-type=\"indexterm\" id=\"ch07.html3\"></a>While finetuning can improve a model in many ways, many of these improvements can also be achieved, to a certain extent, without finetuning. Finetuning can improve a model’s performance, but so do carefully crafted prompts and context. Finetuning can help with structured outputs, but many other techniques, as discussed in <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a>, can also do that.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "First, while finetuning a model for a specific task can improve its performance for that task, it can degrade its performance for other tasks.1 This can be frustrating when you intend this model for an application that expects diverse prompts.",
      "raw_html": "<p>First, while finetuning a model for a specific task can improve its performance for that task, it can degrade its performance for other tasks.<sup><a data-type=\"noteref\" href=\"ch07.html#id1369\" id=\"id1369-marker\">1</a></sup> This can be frustrating when you intend this model for an application that expects diverse prompts.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Imagine you need a model for three types of queries: product recommendations, changing orders, and general feedback. Originally, the model works well for product recommendations and general feedback but poorly for changing orders. To fix this, you finetune the model on a dataset of (query, response) pairs about changing orders. The finetuned model might indeed perform better for this type of query, but worse for the two other tasks.",
      "raw_html": "<p>Imagine you need a model for three types of queries: product recommendations, changing orders, and general feedback. Originally, the model works well for product recommendations and general feedback but poorly for changing orders. To fix this, you finetune the model on a dataset of (query, response) pairs about changing orders. The finetuned model might indeed perform better for this type of query, but worse for the two other tasks.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "What do you do in this situation? You can finetune the model on all the queries you care about, not just changing orders. If you can’t seem to get a model to perform well on all your tasks, consider using separate models for different tasks. If you wish to combine these separate models into one to make serving them easier, you can also consider merging them together, as discussed later in this chapter.",
      "raw_html": "<p>What do you do in this situation? You can finetune the model on all the queries you care about, not just changing orders. If you can’t seem to get a model to perform well on all your tasks, consider using separate models for different tasks. If you wish to combine these separate models into one to make serving them easier, you can also consider merging them together, as discussed later in this chapter.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If you’re just starting to experiment with a project, finetuning is rarely the first thing you should attempt. Finetuning requires high up-front investments and continual maintenance. First, you need data. Annotated data can be slow and expensive to acquire manually, especially for tasks that demand critical thinking and domain expertise. Open source data and AI-generated data can mitigate the cost, but their effectiveness is highly variable.",
      "raw_html": "<p>If you’re just starting to experiment with a project, finetuning is rarely the first thing you should attempt. Finetuning requires high up-front investments and continual maintenance. First, you need data. Annotated data can be slow and expensive to acquire manually, especially for tasks that demand critical thinking and domain expertise. Open source data and AI-generated data can mitigate the cost, but their effectiveness is highly variable.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Second, finetuning requires the knowledge of how to train models. You need to evaluate base models to choose one to finetune. Depending on your needs and resources, options might be limited. While finetuning frameworks and APIs can automate many steps in the actual finetuning process, you still need to understand the different training knobs you can tweak, monitor the learning process, and debug when something is wrong. For example, you need to understand how an optimizer works, what learning rate to use, how much training data is needed, how to address overfitting/underfitting, and how to evaluate your models throughout the process.",
      "raw_html": "<p>Second, finetuning requires the knowledge of how to train models. You need to evaluate base models to choose one to finetune. Depending on your needs and resources, options might be limited. While finetuning frameworks and APIs can automate many steps in the actual finetuning process, you still need to understand the different training knobs you can tweak, monitor the learning process, and debug when something is wrong. For example, you need to understand how an optimizer works, what learning rate to use, how much training data is needed, how to address overfitting/underfitting, and how to evaluate your models throughout the process.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Third, once you have a finetuned model, you’ll need to figure out how to serve it. Will you host it yourself or use an API service? As discussed in Chapter 9, inference optimization for large models, especially LLMs, isn’t trivial. Finetuning requires less of a technical leap if you’re already hosting your models in-house and familiar with how to operate models.",
      "raw_html": "<p>Third, once you have a finetuned model, you’ll need to figure out how to serve it. Will you host it yourself or use an API service? As discussed in <a data-type=\"xref\" href=\"ch09.html#ch09_inference_optimization_1730130963006301\">Chapter 9</a>, inference optimization for large models, especially LLMs, isn’t trivial. Finetuning requires less of a technical leap if you’re already hosting your models in-house and familiar with how to operate models.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "More importantly, you need to establish a policy and budget for monitoring, maintaining, and updating your model. As you iterate on your finetuned model, new base models are being developed at a rapid pace. These base models may improve faster than you can enhance your finetuned model. If a new base model outperforms your finetuned model on your specific task, how significant does the performance improvement have to be before you switch to the new base model? What if a new base model doesn’t immediately outperform your existing model but has the potential to do so after finetuning—would you experiment with it?",
      "raw_html": "<p>More importantly, you need to establish a policy and budget for monitoring, maintaining, and updating your model. As you iterate on your finetuned model, new base models are being developed at a rapid pace. These base models may improve faster than you can enhance your finetuned model. If a new base model outperforms your finetuned model on your specific task, how significant does the performance improvement have to be before you switch to the new base model? What if a new base model doesn’t immediately outperform your existing model but has the potential to do so after finetuning—would you experiment with it?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In many cases, switching to a better model would provide only a small incremental improvement, and your task might be given a lower priority than projects with larger returns, like enabling new use cases.2",
      "raw_html": "<p>In many cases, switching to a better model would provide only a small incremental improvement, and your task might be given a lower priority than projects with larger returns, like enabling new use cases.<sup><a data-type=\"noteref\" href=\"ch07.html#id1370\" id=\"id1370-marker\">2</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "AI engineering experiments should start with prompting, following the best practices discussed in Chapter 6. Explore more advanced solutions only if prompting alone proves inadequate. Ensure you have thoroughly tested various prompts, as a model’s performance can vary greatly with different prompts.",
      "raw_html": "<p>AI engineering experiments should start with prompting, following the best practices discussed in <a data-type=\"xref\" href=\"ch06.html#ch06_rag_and_agents_1730157386571386\">Chapter 6</a>. Explore more advanced solutions only if prompting alone proves inadequate. Ensure you have thoroughly tested various prompts, as a model’s performance can vary greatly with different prompts.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Many practitioners I’ve spoken with share a similar story that goes like this. Someone complains that prompting is ineffective and insists on finetuning. Upon investigation, it turns out that prompt experiments were minimal and unsystematic. Instructions were unclear, examples didn’t represent actual data, and metrics were poorly defined. After refining the prompt experiment process, the prompt quality improved enough to be sufficient for their application.3",
      "raw_html": "<p>Many practitioners I’ve spoken with share a similar story that goes like this. Someone complains that prompting is ineffective and insists on finetuning. Upon investigation, it turns out that prompt experiments were minimal and unsystematic. Instructions were unclear, examples didn’t represent actual data, and metrics were poorly defined. After refining the prompt experiment process, the prompt quality improved enough to be sufficient for their application.<sup><a data-type=\"noteref\" href=\"ch07.html#id1371\" id=\"id1371-marker\">3</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "aside",
      "content": "Finetuning Domain-Specific Tasks\nBeware of the argument that general-purpose models don’t work well for domain-specific tasks, and, therefore, you must finetune or train models for your specific tasks. As general-purpose models become more capable, they also become better at domain-specific tasks and can outperform the domain-specific models.\nAn interesting early specialized model is BloombergGPT, which was introduced by Bloomberg in March 2023. The strongest models on the market then were all proprietary, and Bloomberg wanted a mid-size model that performed well on financial tasks and could be hosted in-house for use cases with sensitive data. The model, with 50 billion parameters, required 1.3 million A100 GPU hours for training. The estimated cost of the compute was between $1.3 million and $2.6 million, excluding data costs (Wu et al., 2023).\nIn the same month, OpenAI released GPT-4-0314.4 Research by Li et al. (2023) demonstrated that GPT-4-0314 significantly outperformed BloombergGPT across various financial benchmarks. Table 7-1 provides details of two such benchmarks.\n\nTable 7-1. General-purpose models like GPT-4 can outperform financial models in financial domains.\n\n\nModel\nFiQA sentiment analysis\n(weighted F1)\nConvFinQA\n(accuracy)\n\n\n\n\nGPT-4-0314 (zero-shot)\n87.15\n76.48\n\n\nBloombergGPT\n75.07\n43.41\n\n\n\nSince then, several mid-size models with performance comparable to GPT-4 have been released, including Claude 3.5 Sonnet (70B parameters), Llama 3-70B-Instruct, and Qwen2-72B-Instruct. The latter two are open weight and can be self-hosted.\nBecause benchmarks are insufficient to capture real-world performance, it’s possible that BloombergGPT works well for Bloomberg for their specific use cases. The Bloomberg team certainly gained invaluable experience through training this model, which might enable them to better develop and operate future models.",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"ch07a_finetuning_domain_specific_tasks_1730160615813045\">\n<h1>Finetuning Domain-Specific Tasks</h1>\n<p><a contenteditable=\"false\" data-primary=\"domain-specific task finetuning\" data-type=\"indexterm\" id=\"id1372\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"domain-specific tasks\" data-type=\"indexterm\" id=\"id1373\"></a>Beware of the argument that general-purpose models don’t work well for domain-specific tasks, and, therefore, you must finetune or train models for your specific tasks. As general-purpose models become more capable, they also become better at domain-specific tasks and can outperform the domain-specific models.</p>\n<p>An interesting early specialized model is BloombergGPT, which was introduced by Bloomberg in March 2023. The strongest models on the market then were all proprietary, and Bloomberg wanted a mid-size model that performed well on financial tasks and could be hosted in-house for use cases with sensitive data. The model, with 50 billion parameters, required 1.3 million A100 GPU hours for training. The estimated cost of the compute was between $1.3 million and $2.6 million, excluding data costs (<a href=\"https://arxiv.org/abs/2303.17564\">Wu et al., 2023</a>).</p>\n<p>In the same month, OpenAI released GPT-4-0314.<sup><a data-type=\"noteref\" href=\"ch07.html#id1374\" id=\"id1374-marker\">4</a></sup> Research by <a href=\"https://arxiv.org/abs/2305.05862\">Li et al. (2023)</a> demonstrated that GPT-4-0314 significantly outperformed BloombergGPT across various financial benchmarks. <a data-type=\"xref\" href=\"#ch07a_table_1_1730160615803945\">Table 7-1</a> provides details of two such benchmarks.</p>\n<table id=\"ch07a_table_1_1730160615803945\">\n<caption><span class=\"label\">Table 7-1. </span>General-purpose models like GPT-4 can outperform financial models in <span class=\"keep-together\">financial domains</span>.</caption>\n<thead>\n<tr>\n<th>Model</th>\n<th>FiQA sentiment analysis<br/>\n(weighted F1)</th>\n<th>ConvFinQA<br/>\n(accuracy)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GPT-4-0314 (zero-shot)</td>\n<td>87.15</td>\n<td>76.48</td>\n</tr>\n<tr>\n<td>BloombergGPT</td>\n<td>75.07</td>\n<td>43.41</td>\n</tr>\n</tbody>\n</table>\n<p>Since then, several mid-size models with performance comparable to GPT-4 have been released, including <a href=\"https://oreil.ly/J-soV\">Claude 3.5 Sonnet </a>(70B parameters), <a href=\"https://oreil.ly/6lt6-\">Llama 3-70B-Instruct</a>, and <a href=\"https://oreil.ly/HZnfa\">Qwen2-72B-Instruct</a>. The latter two are open weight and can be self-hosted.</p>\n<p>Because benchmarks are insufficient to capture real-world performance, it’s possible that BloombergGPT works well for Bloomberg for their specific use cases. The Bloomberg team certainly gained invaluable experience through training this model, which might enable them to better develop and operate future models.</p>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Finetuning Domain-Specific Tasks",
      "id": "heading-7",
      "raw_html": "<h1>Finetuning Domain-Specific Tasks</h1>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "Beware of the argument that general-purpose models don’t work well for domain-specific tasks, and, therefore, you must finetune or train models for your specific tasks. As general-purpose models become more capable, they also become better at domain-specific tasks and can outperform the domain-specific models.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"domain-specific task finetuning\" data-type=\"indexterm\" id=\"id1372\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"domain-specific tasks\" data-type=\"indexterm\" id=\"id1373\"></a>Beware of the argument that general-purpose models don’t work well for domain-specific tasks, and, therefore, you must finetune or train models for your specific tasks. As general-purpose models become more capable, they also become better at domain-specific tasks and can outperform the domain-specific models.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "An interesting early specialized model is BloombergGPT, which was introduced by Bloomberg in March 2023. The strongest models on the market then were all proprietary, and Bloomberg wanted a mid-size model that performed well on financial tasks and could be hosted in-house for use cases with sensitive data. The model, with 50 billion parameters, required 1.3 million A100 GPU hours for training. The estimated cost of the compute was between $1.3 million and $2.6 million, excluding data costs (Wu et al., 2023).",
      "raw_html": "<p>An interesting early specialized model is BloombergGPT, which was introduced by Bloomberg in March 2023. The strongest models on the market then were all proprietary, and Bloomberg wanted a mid-size model that performed well on financial tasks and could be hosted in-house for use cases with sensitive data. The model, with 50 billion parameters, required 1.3 million A100 GPU hours for training. The estimated cost of the compute was between $1.3 million and $2.6 million, excluding data costs (<a href=\"https://arxiv.org/abs/2303.17564\">Wu et al., 2023</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In the same month, OpenAI released GPT-4-0314.4 Research by Li et al. (2023) demonstrated that GPT-4-0314 significantly outperformed BloombergGPT across various financial benchmarks. Table 7-1 provides details of two such benchmarks.",
      "raw_html": "<p>In the same month, OpenAI released GPT-4-0314.<sup><a data-type=\"noteref\" href=\"ch07.html#id1374\" id=\"id1374-marker\">4</a></sup> Research by <a href=\"https://arxiv.org/abs/2305.05862\">Li et al. (2023)</a> demonstrated that GPT-4-0314 significantly outperformed BloombergGPT across various financial benchmarks. <a data-type=\"xref\" href=\"#ch07a_table_1_1730160615803945\">Table 7-1</a> provides details of two such benchmarks.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch07a_table_1_1730160615803945",
      "raw_html": "<table id=\"ch07a_table_1_1730160615803945\">\n<caption><span class=\"label\">Table 7-1. </span>General-purpose models like GPT-4 can outperform financial models in <span class=\"keep-together\">financial domains</span>.</caption>\n<thead>\n<tr>\n<th>Model</th>\n<th>FiQA sentiment analysis<br/>\n(weighted F1)</th>\n<th>ConvFinQA<br/>\n(accuracy)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GPT-4-0314 (zero-shot)</td>\n<td>87.15</td>\n<td>76.48</td>\n</tr>\n<tr>\n<td>BloombergGPT</td>\n<td>75.07</td>\n<td>43.41</td>\n</tr>\n</tbody>\n</table>",
      "caption": {
        "label": "Table 7-1.",
        "text": "General-purpose models like GPT-4 can outperform financial models in financial domains."
      },
      "headers": [
        "Model",
        "FiQA sentiment analysis\n(weighted F1)",
        "ConvFinQA\n(accuracy)"
      ],
      "rows": [
        [
          "GPT-4-0314 (zero-shot)",
          "87.15",
          "76.48"
        ],
        [
          "BloombergGPT",
          "75.07",
          "43.41"
        ]
      ],
      "row_count": 2,
      "column_count": 3
    },
    {
      "type": "paragraph",
      "content": "Since then, several mid-size models with performance comparable to GPT-4 have been released, including Claude 3.5 Sonnet (70B parameters), Llama 3-70B-Instruct, and Qwen2-72B-Instruct. The latter two are open weight and can be self-hosted.",
      "raw_html": "<p>Since then, several mid-size models with performance comparable to GPT-4 have been released, including <a href=\"https://oreil.ly/J-soV\">Claude 3.5 Sonnet </a>(70B parameters), <a href=\"https://oreil.ly/6lt6-\">Llama 3-70B-Instruct</a>, and <a href=\"https://oreil.ly/HZnfa\">Qwen2-72B-Instruct</a>. The latter two are open weight and can be self-hosted.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because benchmarks are insufficient to capture real-world performance, it’s possible that BloombergGPT works well for Bloomberg for their specific use cases. The Bloomberg team certainly gained invaluable experience through training this model, which might enable them to better develop and operate future models.",
      "raw_html": "<p>Because benchmarks are insufficient to capture real-world performance, it’s possible that BloombergGPT works well for Bloomberg for their specific use cases. The Bloomberg team certainly gained invaluable experience through training this model, which might enable them to better develop and operate future models.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Both finetuning and prompting experiments require systematic processes. Doing prompt experiments enables developers to build an evaluation pipeline, data annotation guideline, and experiment tracking practices that will be stepping stones for finetuning.",
      "raw_html": "<p>Both finetuning and prompting experiments require systematic processes. Doing prompt experiments enables developers to build an evaluation pipeline, data annotation guideline, and experiment tracking practices that will be stepping stones for <span class=\"keep-together\">finetuning.</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One benefit of finetuning, before prompt caching was introduced, was that it can help optimize token usage. The more examples you add to a prompt, the more input tokens the model will use, which increases both latency and cost. Instead of including your examples in each prompt, you can finetune a model on these examples. This allows you to use shorter prompts with the finetuned model, as shown in Figure 7-2.",
      "raw_html": "<p>One benefit of finetuning, before prompt caching was introduced, was that it can help optimize token usage. The more examples you add to a prompt, the more input tokens the model will use, which increases both latency and cost. Instead of including your examples in each prompt, you can finetune a model on these examples. This allows you to use shorter prompts with the finetuned model, as shown in <a data-type=\"xref\" href=\"#ch07a_figure_2_1730160615799676\">Figure 7-2</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "With prompt caching, where repetitive prompt segments can be cached for reuse, this is no longer a strong benefit. Prompt caching is discussed further in Chapter 9. However, the number of examples you can use with a prompt is still limited by the maximum context length. With finetuning, there’s no limit to how many examples you can use.",
      "raw_html": "<p>With prompt caching, where repetitive prompt segments can be cached for reuse, this is no longer a strong benefit. Prompt caching is discussed further in <a data-type=\"xref\" href=\"ch09.html#ch09_inference_optimization_1730130963006301\">Chapter 9</a>. However, the number of examples you can use with a prompt is still limited by the maximum context length. With finetuning, there’s no limit to how many examples you can use.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html3\" data-type=\"indexterm\" id=\"id1375\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07a_figure_2_1730160615799676",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07a_figure_2_1730160615799676\">\n<img alt=\"A diagram of a model\n\nDescription automatically generated\" src=\"assets/aien_0702.png\"/>\n<h6><span class=\"label\">Figure 7-2. </span>Instead of including examples in each prompt, which increases cost and latency, you finetune a model on these examples.</h6>\n</div></figure>",
      "image": "aien_0702.png",
      "alt": "A diagram of a model\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0702.png",
      "caption": {
        "label": "Figure 7-2.",
        "text": "Instead of including examples in each prompt, which increases cost and latency, you finetune a model on these examples."
      }
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Finetuning and RAG",
      "id": "heading-7",
      "raw_html": "<h2>Finetuning and RAG</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "Once you’ve maximized the performance gains from prompting, you might wonder whether to do RAG or finetuning next. The answer depends on whether your model’s failures are information-based or behavior-based.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"finetuning and RAG\" data-type=\"indexterm\" id=\"ch07.html6\"></a><a contenteditable=\"false\" data-primary=\"RAG (retrieval-augmented generation)\" data-secondary=\"finetuning and\" data-type=\"indexterm\" id=\"ch07.html7\"></a>Once you’ve maximized the performance gains from prompting, you might wonder whether to do RAG or finetuning next. The answer depends on whether your model’s failures are information-based or behavior-based. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If the model fails because it lacks information, a RAG system that gives the model access to the relevant sources of information can help. Information-based failures happen when the outputs are factually wrong or outdated. Here are two example scenarios in which information-based failures happen:",
      "raw_html": "<p><em>If the model fails because it lacks information, a RAG system that gives the model access to the relevant sources of information can help</em>. Information-based failures happen when the outputs are factually wrong or outdated. Here are two example scenarios in which information-based failures happen:</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "The model doesn’t have the information.",
          "definition": "Public models are unlikely to have information private to you or your organization. When a model doesn’t have the information, it either tells you so or hallucinates an answer."
        },
        {
          "term": "The model has outdated information.",
          "definition": "If you ask: “How many studio albums has Taylor Swift released?” and the correct answer is 11, but the model answers 10, it can be because the model’s cut-off date was before the release of the latest album."
        }
      ],
      "raw_html": "<dl>\n<dt>The model doesn’t have the information.</dt>\n<dd>\n<p>Public models are unlikely to have information private to you or your organization. When a model doesn’t have the information, it either tells you so or hallucinates an answer.</p>\n</dd>\n<dt>The model has outdated information.</dt>\n<dd>\n<p>If you ask: “How many studio albums has Taylor Swift released?” and the correct answer is 11, but the model answers 10, it can be because the model’s cut-off date was before the release of the latest album.</p>\n</dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "Public models are unlikely to have information private to you or your organization. When a model doesn’t have the information, it either tells you so or hallucinates an answer.",
      "raw_html": "<p>Public models are unlikely to have information private to you or your organization. When a model doesn’t have the information, it either tells you so or hallucinates an answer.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If you ask: “How many studio albums has Taylor Swift released?” and the correct answer is 11, but the model answers 10, it can be because the model’s cut-off date was before the release of the latest album.",
      "raw_html": "<p>If you ask: “How many studio albums has Taylor Swift released?” and the correct answer is 11, but the model answers 10, it can be because the model’s cut-off date was before the release of the latest album.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The paper “Fine-Tuning or Retrieval?” by Ovadia et al. (2024) demonstrated that for tasks that require up-to-date information, such as questions about current events, RAG outperformed finetuned models. Not only that, RAG with the base model outperformed RAG with finetuned models, as shown in Table 7-2. This finding indicates that while finetuning can enhance a model’s performance on a specific task, it may also lead to a decline in performance in other areas.",
      "raw_html": "<p>The paper <a href=\"https://oreil.ly/t9HTH\">“Fine-Tuning or Retrieval?”</a> by Ovadia et al. (2024) demonstrated that for tasks that require up-to-date information, such as questions about current events, RAG outperformed finetuned models. Not only that, RAG with the base model <span class=\"keep-together\">outperformed</span> RAG with finetuned models, as shown in <a data-type=\"xref\" href=\"#ch07a_table_2_1730160615803962\">Table 7-2</a>. This finding indicates that <em>while finetuning can enhance a model’s performance on a specific task, it may also lead to a decline in performance in other areas.</em></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "table",
      "id": "ch07a_table_2_1730160615803962",
      "raw_html": "<table id=\"ch07a_table_2_1730160615803962\">\n<caption><span class=\"label\">Table 7-2. </span>RAG outperforms finetuning on a question-answering task about current events, curated by Ovadia et al. (2024). FT-reg and FT-par refer to two different finetuning approaches the author used.</caption>\n<thead>\n<tr>\n<th> </th>\n<th>Base model</th>\n<th>Base model + RAG</th>\n<th>FT-reg</th>\n<th>FT-par</th>\n<th>FT-reg + RAG</th>\n<th>FT-par + RAG</th>\n</tr>\n</thead>\n<tr>\n<td>Mistral-7B</td>\n<td>0.481</td>\n<td>0.875</td>\n<td>0.504</td>\n<td>0.588</td>\n<td>0.810</td>\n<td>0.830</td>\n</tr>\n<tr>\n<td>Llama 2-7B</td>\n<td>0.353</td>\n<td>0.585</td>\n<td>0.219</td>\n<td>0.392</td>\n<td>0.326</td>\n<td>0.520</td>\n</tr>\n<tr>\n<td>Orca 2-7B</td>\n<td>0.456</td>\n<td>0.876</td>\n<td>0.511</td>\n<td>0.566</td>\n<td>0.820</td>\n<td>0.826</td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 7-2.",
        "text": "RAG outperforms finetuning on a question-answering task about current events, curated by Ovadia et al. (2024). FT-reg and FT-par refer to two different finetuning approaches the author used."
      },
      "headers": [
        "",
        "Base model",
        "Base model + RAG",
        "FT-reg",
        "FT-par",
        "FT-reg + RAG",
        "FT-par + RAG"
      ],
      "rows": [
        [
          "Mistral-7B",
          "0.481",
          "0.875",
          "0.504",
          "0.588",
          "0.810",
          "0.830"
        ],
        [
          "Llama 2-7B",
          "0.353",
          "0.585",
          "0.219",
          "0.392",
          "0.326",
          "0.520"
        ],
        [
          "Orca 2-7B",
          "0.456",
          "0.876",
          "0.511",
          "0.566",
          "0.820",
          "0.826"
        ]
      ],
      "row_count": 3,
      "column_count": 7
    },
    {
      "type": "paragraph",
      "content": "On the other hand, if the model has behavioral issues, finetuning might help. One behavioral issue is when the model’s outputs are factually correct but irrelevant to the task. For example, you ask the model to generate technical specifications for a software project to provide to your engineering teams. While accurate, the generated specs lack the details your teams need. Finetuning the model with well-defined technical specifications can make the outputs more relevant.",
      "raw_html": "<p>On the other hand, <em>if the model has behavioral issues, finetuning might help</em>. One behavioral issue is when the model’s outputs are factually correct but irrelevant to the task. For example, you ask the model to generate technical specifications for a software project to provide to your engineering teams. While accurate, the generated specs lack the details your teams need. Finetuning the model with well-defined technical specifications can make the outputs more relevant.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Another issue is when it fails to follow the expected output format. For example, if you asked the model to write HTML code, but the generated code didn’t compile, it might be because the model wasn’t sufficiently exposed to HTML in its training data. You can correct this by exposing the model to more HTML code during finetuning.",
      "raw_html": "<p>Another issue is when it fails to follow the expected output format. For example, if you asked the model to write HTML code, but the generated code didn’t compile, it might be because the model wasn’t sufficiently exposed to HTML in its training data. You can correct this by exposing the model to more HTML code during finetuning.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Semantic parsing is a category of tasks whose success hinges on the model’s ability to generate outputs in the expected format and, therefore, often requires finetuning. Semantic parsing is discussed briefly in Chapters 2 and 6. As a reminder, semantic parsing means converting natural language into a structured format like JSON. Strong off-the-shelf models are generally good for common, less complex syntaxes like JSON, YAML, and regex. However, they might not be as good for syntaxes with fewer available examples on the internet, such as a domain-specific language for a less popular tool or a complex syntax.",
      "raw_html": "<p>Semantic parsing is a category of tasks whose success hinges on the model’s ability to generate outputs in the expected format and, therefore, often requires finetuning. Semantic parsing is discussed briefly in Chapters <a data-type=\"xref\" data-xrefstyle=\"select:labelnumber\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">2</a> and <a data-type=\"xref\" data-xrefstyle=\"select:labelnumber\" href=\"ch06.html#ch06_rag_and_agents_1730157386571386\">6</a>. As a reminder, semantic parsing means converting natural language into a structured format like JSON. Strong off-the-shelf models are generally good for common, less complex syntaxes like JSON, YAML, and regex. However, they might not be as good for syntaxes with fewer available examples on the internet, such as a domain-specific language for a less popular tool or a complex syntax.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In short, finetuning is for form, and RAG is for facts. A RAG system gives your model external knowledge to construct more accurate and informative answers. A RAG system can help mitigate your model’s hallucinations. Finetuning, on the other hand, helps your model understand and follow syntaxes and styles.5 While finetuning can potentially reduce hallucinations if done with enough high-quality data, it can also worsen hallucinations if the data quality is low.",
      "raw_html": "<p><em>In short, finetuning is for form, and RAG is for facts</em>. A RAG system gives your model external knowledge to construct more accurate and informative answers. <a contenteditable=\"false\" data-primary=\"hallucinations\" data-secondary=\"and finetuning\" data-secondary-sortas=\"finetuning\" data-type=\"indexterm\" id=\"id1376\"></a>A RAG system can help mitigate your model’s hallucinations. Finetuning, on the other hand, helps your model understand and follow syntaxes and styles.<sup><a data-type=\"noteref\" href=\"ch07.html#id1377\" id=\"id1377-marker\">5</a></sup> While finetuning can potentially reduce hallucinations if done with enough high-quality data, it can also worsen hallucinations if the data quality is low.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "If your model has both information and behavior issues, start with RAG. RAG is typically easier since you won’t have to worry about curating training data or hosting the finetuned models. When doing RAG, start with simple term-based solutions such as BM25 instead of jumping straight into something that requires vector databases.",
      "raw_html": "<p>If your model has both information and behavior issues, start with RAG. RAG is typically easier since you won’t have to worry about curating training data or hosting the finetuned models. When doing RAG, start with simple term-based solutions such as BM25 instead of jumping straight into something that requires vector databases.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "RAG can also introduce a more significant performance boost than finetuning. Ovadia et al. (2024) showed that for almost all question categories in the MMLU benchmark, RAG outperforms finetuning for three different models: Mistral 7B, Llama 2-7B, and Orca 2-7B.",
      "raw_html": "<p>RAG can also introduce a more significant performance boost than finetuning. Ovadia et al. (2024) showed that for almost all question categories in the <a href=\"https://arxiv.org/abs/2009.03300\">MMLU benchmark</a>, RAG outperforms finetuning for three different models: Mistral 7B, Llama 2-7B, and Orca 2-7B.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "However, RAG and finetuning aren’t mutually exclusive. They can sometimes be used together to maximize your application’s performance. In the same experiment, Ovadia et al. (2024) showed that incorporating RAG on top of a finetuned model can boost its performance on the MMLU benchmark 43% of the time. It’s important to note that in this experiment, using RAG with finetuned models doesn’t improve the performance 57% of the time, compared to using RAG alone.",
      "raw_html": "<p>However, RAG and finetuning aren’t mutually exclusive. They can sometimes be used together to maximize your application’s performance. In the same experiment, <a href=\"https://oreil.ly/t9HTH\">Ovadia et al. (2024)</a> showed that incorporating RAG on top of a finetuned model can boost its performance on the MMLU benchmark 43% of the time. It’s important to note that in this experiment, using RAG with finetuned models doesn’t improve the performance 57% of the time, compared to using RAG alone.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "There’s no universal workflow for all applications. Figure 7-3 shows some paths an application development process might follow over time. The arrow indicates what next step you might try. This figure is inspired by an example workflow shown by OpenAI (2023).",
      "raw_html": "<p>There’s no universal workflow for all applications. <a data-type=\"xref\" href=\"#ch07a_figure_3_1730160615799691\">Figure 7-3</a> shows some paths an application development process might follow over time. The arrow indicates what next step you might try. This figure is inspired by an example workflow shown by <a href=\"https://oreil.ly/Ny1WI\">OpenAI</a> (2023).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07a_figure_3_1730160615799691",
      "raw_html": "<figure class=\"width-75\"><div class=\"figure\" id=\"ch07a_figure_3_1730160615799691\">\n<img alt=\"A diagram of a process\n\nDescription automatically generated\" src=\"assets/aien_0703.png\"/>\n<h6><span class=\"label\">Figure 7-3. </span>Example application development flows. After simple retrieval (such as term-based retrieval), whether to experiment with more complex retrieval (such as hybrid search) or finetuning depends on each application and its failure modes.</h6>\n</div></figure>",
      "image": "aien_0703.png",
      "alt": "A diagram of a process\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0703.png",
      "caption": {
        "label": "Figure 7-3.",
        "text": "Example application development flows. After simple retrieval (such as term-based retrieval), whether to experiment with more complex retrieval (such as hybrid search) or finetuning depends on each application and its failure modes."
      }
    },
    {
      "type": "paragraph",
      "content": "So the workflow to adapt a model to a task might work as follows. Note that before any of the adaptation steps, you should define your evaluation criteria and design your evaluation pipeline, as discussed in Chapter 4. This evaluation pipeline is what you’ll use to benchmark your progress as you develop your application. Evaluation doesn’t happen only in the beginning. It should be present during every step of the process:",
      "raw_html": "<p>So the workflow to adapt a model to a task might work as follows. Note that before any of the adaptation steps, you should define your evaluation criteria and design your evaluation pipeline, as discussed in <a data-type=\"xref\" href=\"ch04.html#ch04_evaluate_ai_systems_1730130866187863\">Chapter 4</a>. This evaluation pipeline is what you’ll use to benchmark your progress as you develop your application. Evaluation doesn’t happen only in the beginning. It should be present during every step of the process:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Try to get a model to perform your task with prompting alone. Use the prompt engineering best practices covered in Chapter 5, including systematically versioning your prompts.",
        "Add more examples to the prompt. Depending on the use case, the number of examples needed might be between 1 and 50.",
        "If your model frequently fails due to missing information, connect it to data sources that can supply relevant information. When starting with RAG, begin by using basic retrieval methods like term-based search. Even with simple retrieval, adding relevant and accurate knowledge should lead to some improvement in your model’s performance.",
        "Depending on your model’s failure modes, you might explore one of these next steps:\n\nIf the model continues having information-based failures, you might want to try even more advanced RAG methods, such as embedding-based retrieval.\nIf the model continues having behavioral issues, such as it keeps generating irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based retrieval increases inference complexity by introducing additional components into the pipeline, while finetuning increases the complexity of model development but leaves inference unchanged.",
        "Combine both RAG and finetuning for even more performance boost."
      ],
      "raw_html": "<ol>\n<li><p>Try to get a model to perform your task with prompting alone. Use the prompt engineering best practices covered in <a data-type=\"xref\" href=\"ch05.html#ch05a_prompt_engineering_1730156991195551\">Chapter 5</a>, including systematically versioning your prompts.</p></li>\n<li><p>Add more examples to the prompt. Depending on the use case, the number of examples needed might be between 1 and 50.</p></li>\n<li><p>If your model frequently fails due to missing information, connect it to data sources that can supply relevant information. When starting with RAG, begin by using basic retrieval methods like term-based search. Even with simple retrieval, adding relevant and accurate knowledge should lead to some improvement in your model’s performance.</p></li>\n<li><p>Depending on your model’s failure modes, you might explore one of these next steps:</p>\n<ol>\n<li><p>If the model continues having information-based failures, you might want to try even more advanced RAG methods, such as embedding-based retrieval.</p></li>\n<li><p>If the model continues having behavioral issues, such as it keeps generating irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based retrieval increases inference complexity by introducing additional components into the pipeline, while finetuning increases the complexity of model development but leaves inference unchanged.</p></li>\n</ol>\n</li>\n<li><p>Combine both RAG and finetuning for even more performance boost.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Try to get a model to perform your task with prompting alone. Use the prompt engineering best practices covered in Chapter 5, including systematically versioning your prompts.",
      "raw_html": "<p>Try to get a model to perform your task with prompting alone. Use the prompt engineering best practices covered in <a data-type=\"xref\" href=\"ch05.html#ch05a_prompt_engineering_1730156991195551\">Chapter 5</a>, including systematically versioning your prompts.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Add more examples to the prompt. Depending on the use case, the number of examples needed might be between 1 and 50.",
      "raw_html": "<p>Add more examples to the prompt. Depending on the use case, the number of examples needed might be between 1 and 50.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If your model frequently fails due to missing information, connect it to data sources that can supply relevant information. When starting with RAG, begin by using basic retrieval methods like term-based search. Even with simple retrieval, adding relevant and accurate knowledge should lead to some improvement in your model’s performance.",
      "raw_html": "<p>If your model frequently fails due to missing information, connect it to data sources that can supply relevant information. When starting with RAG, begin by using basic retrieval methods like term-based search. Even with simple retrieval, adding relevant and accurate knowledge should lead to some improvement in your model’s performance.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Depending on your model’s failure modes, you might explore one of these next steps:",
      "raw_html": "<p>Depending on your model’s failure modes, you might explore one of these next steps:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "If the model continues having information-based failures, you might want to try even more advanced RAG methods, such as embedding-based retrieval.",
        "If the model continues having behavioral issues, such as it keeps generating irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based retrieval increases inference complexity by introducing additional components into the pipeline, while finetuning increases the complexity of model development but leaves inference unchanged."
      ],
      "raw_html": "<ol>\n<li><p>If the model continues having information-based failures, you might want to try even more advanced RAG methods, such as embedding-based retrieval.</p></li>\n<li><p>If the model continues having behavioral issues, such as it keeps generating irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based retrieval increases inference complexity by introducing additional components into the pipeline, while finetuning increases the complexity of model development but leaves inference unchanged.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "If the model continues having information-based failures, you might want to try even more advanced RAG methods, such as embedding-based retrieval.",
      "raw_html": "<p>If the model continues having information-based failures, you might want to try even more advanced RAG methods, such as embedding-based retrieval.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If the model continues having behavioral issues, such as it keeps generating irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based retrieval increases inference complexity by introducing additional components into the pipeline, while finetuning increases the complexity of model development but leaves inference unchanged.",
      "raw_html": "<p>If the model continues having behavioral issues, such as it keeps generating irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based retrieval increases inference complexity by introducing additional components into the pipeline, while finetuning increases the complexity of model development but leaves inference unchanged.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Combine both RAG and finetuning for even more performance boost.",
      "raw_html": "<p>Combine both RAG and finetuning for even more performance boost.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If, after considering all the pros and cons of finetuning and other alternate techniques, you decide to finetune your model, the rest of the chapter is for you. First, let’s look into the number one challenge of finetuning: its memory bottleneck.",
      "raw_html": "<p>If, after considering all the pros and cons of finetuning and other alternate techniques, you decide to finetune your model, the rest of the chapter is for you. First, let’s look into the number one challenge of finetuning: its memory bottleneck<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html7\" data-type=\"indexterm\" id=\"id1378\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html6\" data-type=\"indexterm\" id=\"id1379\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html2\" data-type=\"indexterm\" id=\"id1380\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Memory Bottlenecks",
      "id": "heading-7",
      "raw_html": "<h1>Memory Bottlenecks</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "Because finetuning is memory-intensive, many finetuning techniques aim to minimize their memory footprint. Understanding what causes this memory bottleneck is necessary to understand why and how these techniques work. This understanding, in turn, can help you select a finetuning method that works best for you.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"memory\" data-type=\"indexterm\" id=\"ch07.html8\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"memory bottlenecks\" data-type=\"indexterm\" id=\"ch07.html9\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-type=\"indexterm\" id=\"ch07.html10\"></a>Because finetuning is memory-intensive, many finetuning techniques aim to minimize their memory footprint. Understanding what causes this memory bottleneck is necessary to understand why and how these techniques work. This understanding, in turn, can help you select a finetuning method that works best for you.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Besides explaining finetuning’s memory bottleneck, this section also introduces formulas for back-of-the-napkin calculation of the memory usage of each model. This calculation is useful in estimating what hardware you’d need to serve or finetune a model.",
      "raw_html": "<p>Besides explaining finetuning’s memory bottleneck, this section also introduces formulas for back-of-the-napkin calculation of the memory usage of each model. This calculation is useful in estimating what hardware you’d need to serve or finetune a model.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because memory calculation requires a breakdown of low-level ML and computing concepts, this section is technically dense. If you’re already familiar with these concepts, feel free to skip them.",
      "raw_html": "<p>Because memory calculation requires a breakdown of low-level ML and computing concepts, this section is technically dense. If you’re already familiar with these concepts, feel free to skip them.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "aside",
      "content": "Key Takeaways for Understanding Memory Bottlenecks\nIf you decide to skip this section, here are a few key takeaways. If you find any of these takeaways unfamiliar, the concepts in this section should help explain it:\n\nBecause of the scale of foundation models, memory is a bottleneck for working with them, both for inference and for finetuning. The memory needed for finetuning is typically much higher than the memory needed for inference because of the way neural networks are trained.\nThe key contributors to a model’s memory footprint during finetuning are its number of parameters, its number of trainable parameters, and its numerical representations.\nThe more trainable parameters, the higher the memory footprint. You can reduce memory requirement for finetuning by reducing the number of trainable parameters. Reducing the number of trainable parameters is the motivation for PEFT, parameter-efficient finetuning.\nQuantization refers to the practice of converting a model from a format with more bits to a format with fewer bits. Quantization is a straightforward and efficient way to reduce a model’s memory footprint. For a model of 13 billion parameters, using FP32 means 4 bytes per weight or 52 GB for the whole weights. If you can reduce each value to only 2 bytes, the memory needed for the model’s weights decreases to 26 GB. \nInference is typically done using as few bits as possible, such as 16 bits, 8 bits, and even 4 bits.\nTraining is more sensitive to numerical precision, so it’s harder to train a model in lower precision. Training is typically done in mixed precision, with some operations done in higher precision (e.g., 32-bit) and some in lower precision (e.g., 16-bit or 8-bit).",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"id1381\">\n<h1>Key Takeaways for Understanding Memory Bottlenecks</h1>\n<p>If you decide to skip this section, here are a few key takeaways. If you find any of these takeaways unfamiliar, the concepts in this section should help explain it:</p>\n<ol>\n<li><p>Because of the scale of foundation models, memory is a bottleneck for working with them, both for inference and for finetuning. The memory needed for finetuning is typically much higher than the memory needed for inference because of the way neural networks are trained.</p></li>\n<li><p>The key contributors to a model’s memory footprint during finetuning are its number of parameters, its number of trainable parameters, and its numerical representations.</p></li>\n<li><p>The more trainable parameters, the higher the memory footprint. You can reduce memory requirement for finetuning by reducing the number of trainable parameters. Reducing the number of trainable parameters is the motivation for PEFT, parameter-efficient finetuning.</p></li>\n<li><p>Quantization refers to the practice of converting a model from a format with more bits to a format with fewer bits. Quantization is a straightforward and efficient way to reduce a model’s memory footprint. For a model of 13 billion parameters, using FP32 means 4 bytes per weight or 52 GB for the whole weights. If you can reduce each value to only 2 bytes, the memory needed for the model’s weights decreases to 26 GB. </p></li>\n<li><p>Inference is typically done using as few bits as possible, such as 16 bits, 8 bits, and even 4 bits.</p></li>\n<li><p>Training is more sensitive to numerical precision, so it’s harder to train a model in lower precision. Training is typically done in mixed precision, with some operations done in higher precision (e.g., 32-bit) and some in lower precision (e.g., 16-bit or 8-bit).</p></li>\n</ol>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Key Takeaways for Understanding Memory Bottlenecks",
      "id": "heading-7",
      "raw_html": "<h1>Key Takeaways for Understanding Memory Bottlenecks</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "If you decide to skip this section, here are a few key takeaways. If you find any of these takeaways unfamiliar, the concepts in this section should help explain it:",
      "raw_html": "<p>If you decide to skip this section, here are a few key takeaways. If you find any of these takeaways unfamiliar, the concepts in this section should help explain it:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Because of the scale of foundation models, memory is a bottleneck for working with them, both for inference and for finetuning. The memory needed for finetuning is typically much higher than the memory needed for inference because of the way neural networks are trained.",
        "The key contributors to a model’s memory footprint during finetuning are its number of parameters, its number of trainable parameters, and its numerical representations.",
        "The more trainable parameters, the higher the memory footprint. You can reduce memory requirement for finetuning by reducing the number of trainable parameters. Reducing the number of trainable parameters is the motivation for PEFT, parameter-efficient finetuning.",
        "Quantization refers to the practice of converting a model from a format with more bits to a format with fewer bits. Quantization is a straightforward and efficient way to reduce a model’s memory footprint. For a model of 13 billion parameters, using FP32 means 4 bytes per weight or 52 GB for the whole weights. If you can reduce each value to only 2 bytes, the memory needed for the model’s weights decreases to 26 GB.",
        "Inference is typically done using as few bits as possible, such as 16 bits, 8 bits, and even 4 bits.",
        "Training is more sensitive to numerical precision, so it’s harder to train a model in lower precision. Training is typically done in mixed precision, with some operations done in higher precision (e.g., 32-bit) and some in lower precision (e.g., 16-bit or 8-bit)."
      ],
      "raw_html": "<ol>\n<li><p>Because of the scale of foundation models, memory is a bottleneck for working with them, both for inference and for finetuning. The memory needed for finetuning is typically much higher than the memory needed for inference because of the way neural networks are trained.</p></li>\n<li><p>The key contributors to a model’s memory footprint during finetuning are its number of parameters, its number of trainable parameters, and its numerical representations.</p></li>\n<li><p>The more trainable parameters, the higher the memory footprint. You can reduce memory requirement for finetuning by reducing the number of trainable parameters. Reducing the number of trainable parameters is the motivation for PEFT, parameter-efficient finetuning.</p></li>\n<li><p>Quantization refers to the practice of converting a model from a format with more bits to a format with fewer bits. Quantization is a straightforward and efficient way to reduce a model’s memory footprint. For a model of 13 billion parameters, using FP32 means 4 bytes per weight or 52 GB for the whole weights. If you can reduce each value to only 2 bytes, the memory needed for the model’s weights decreases to 26 GB. </p></li>\n<li><p>Inference is typically done using as few bits as possible, such as 16 bits, 8 bits, and even 4 bits.</p></li>\n<li><p>Training is more sensitive to numerical precision, so it’s harder to train a model in lower precision. Training is typically done in mixed precision, with some operations done in higher precision (e.g., 32-bit) and some in lower precision (e.g., 16-bit or 8-bit).</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Because of the scale of foundation models, memory is a bottleneck for working with them, both for inference and for finetuning. The memory needed for finetuning is typically much higher than the memory needed for inference because of the way neural networks are trained.",
      "raw_html": "<p>Because of the scale of foundation models, memory is a bottleneck for working with them, both for inference and for finetuning. The memory needed for finetuning is typically much higher than the memory needed for inference because of the way neural networks are trained.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The key contributors to a model’s memory footprint during finetuning are its number of parameters, its number of trainable parameters, and its numerical representations.",
      "raw_html": "<p>The key contributors to a model’s memory footprint during finetuning are its number of parameters, its number of trainable parameters, and its numerical representations.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The more trainable parameters, the higher the memory footprint. You can reduce memory requirement for finetuning by reducing the number of trainable parameters. Reducing the number of trainable parameters is the motivation for PEFT, parameter-efficient finetuning.",
      "raw_html": "<p>The more trainable parameters, the higher the memory footprint. You can reduce memory requirement for finetuning by reducing the number of trainable parameters. Reducing the number of trainable parameters is the motivation for PEFT, parameter-efficient finetuning.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Quantization refers to the practice of converting a model from a format with more bits to a format with fewer bits. Quantization is a straightforward and efficient way to reduce a model’s memory footprint. For a model of 13 billion parameters, using FP32 means 4 bytes per weight or 52 GB for the whole weights. If you can reduce each value to only 2 bytes, the memory needed for the model’s weights decreases to 26 GB.",
      "raw_html": "<p>Quantization refers to the practice of converting a model from a format with more bits to a format with fewer bits. Quantization is a straightforward and efficient way to reduce a model’s memory footprint. For a model of 13 billion parameters, using FP32 means 4 bytes per weight or 52 GB for the whole weights. If you can reduce each value to only 2 bytes, the memory needed for the model’s weights decreases to 26 GB. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Inference is typically done using as few bits as possible, such as 16 bits, 8 bits, and even 4 bits.",
      "raw_html": "<p>Inference is typically done using as few bits as possible, such as 16 bits, 8 bits, and even 4 bits.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Training is more sensitive to numerical precision, so it’s harder to train a model in lower precision. Training is typically done in mixed precision, with some operations done in higher precision (e.g., 32-bit) and some in lower precision (e.g., 16-bit or 8-bit).",
      "raw_html": "<p>Training is more sensitive to numerical precision, so it’s harder to train a model in lower precision. Training is typically done in mixed precision, with some operations done in higher precision (e.g., 32-bit) and some in lower precision (e.g., 16-bit or 8-bit).</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Backpropagation and Trainable Parameters",
      "id": "heading-7",
      "raw_html": "<h2>Backpropagation and Trainable Parameters</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "A key factor that determines a model’s memory footprint during finetuning is its number of trainable parameters. A trainable parameter is a parameter that can be updated during finetuning. During pre-training, all model parameters are updated. During inference, no model parameters are updated. During finetuning, some or all model parameters may be updated. The parameters that are kept unchanged are frozen parameters.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"backpropagation\" data-type=\"indexterm\" id=\"ch07.html11\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"memory bottlenecks\" data-tertiary=\"backpropagation and trainable parameters\" data-type=\"indexterm\" id=\"ch07.html12\"></a><a contenteditable=\"false\" data-primary=\"trainable parameters\" data-type=\"indexterm\" id=\"ch07.html13\"></a>A key factor that determines a model’s memory footprint during finetuning is its number of <em>trainable parameters</em>. A trainable parameter is a parameter that can be updated during finetuning. During pre-training, all model parameters are updated. During inference, no model parameters are updated. During finetuning, some or all model parameters may be updated. The parameters that are kept unchanged are <em>frozen parameters</em>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The memory needed for each trainable parameter results from the way a model is trained. As of this writing, neural networks are typically trained using a mechanism called backpropagation.6 With backpropagation, each training step consists of two phases:",
      "raw_html": "<p>The memory needed for each trainable parameter results from the way a model is trained. As of this writing, neural networks are typically trained using a mechanism called <em>backpropagation</em>.<sup><a data-type=\"noteref\" href=\"ch07.html#id1382\" id=\"id1382-marker\">6</a></sup> With backpropagation, each training step consists of two phases:</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Forward pass: the process of computing the output from the input.",
        "Backward pass: the process of updating the model’s weights using the aggregated signals from the forward pass."
      ],
      "raw_html": "<ol>\n<li><p>Forward pass: the process of computing the output from the input.</p></li>\n<li><p>Backward pass: the process of updating the model’s weights using the aggregated signals from the forward pass.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Forward pass: the process of computing the output from the input.",
      "raw_html": "<p>Forward pass: the process of computing the output from the input.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Backward pass: the process of updating the model’s weights using the aggregated signals from the forward pass.",
      "raw_html": "<p>Backward pass: the process of updating the model’s weights using the aggregated signals from the forward pass.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "During inference, only the forward pass is executed. During training, both passes are executed. At a high level, the backward pass works as follows:",
      "raw_html": "<p>During inference, only the forward pass is executed. During training, both passes are executed. At a high level, the backward pass works as follows:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Compare the computed output from the forward pass against the expected output (ground truth). If they are different, the model made a mistake, and the parameters need to be adjusted. The difference between the computed output and the expected output is called the loss.",
        "Compute how much each trainable parameter contributes to the mistake. This value is called the gradient. Mathematically, gradients are computed by taking the derivative of the loss with respect to each trainable parameter. There’s one gradient value per trainable parameter.7 If a parameter has a high gradient, it significantly contributes to the loss and should be adjusted more.",
        "Adjust trainable parameter values using their corresponding gradient. How much each parameter should be readjusted, given its gradient value, is determined by the optimizer. Common optimizers include SGD (stochastic gradient descent) and Adam. For transformer-based models, Adam is, by far, the most widely used optimizer."
      ],
      "raw_html": "<ol>\n<li><p>Compare the computed output from the forward pass against the expected output (ground truth). If they are different, the model made a mistake, and the parameters need to be adjusted. The difference between the computed output and the expected output is called the <em>loss</em>.</p></li>\n<li><p>Compute how much each trainable parameter contributes to the mistake. This value is called the <em>gradient</em>. Mathematically, gradients are computed by taking the derivative of the loss with respect to each trainable parameter. There’s one gradient value per trainable parameter.<sup><a data-type=\"noteref\" href=\"ch07.html#id1383\" id=\"id1383-marker\">7</a></sup> If a parameter has a high gradient, it significantly contributes to the loss and should be adjusted more.</p></li>\n<li><p>Adjust trainable parameter values using their corresponding gradient. How much each parameter should be readjusted, given its gradient value, is determined by the <em>optimizer</em>. Common optimizers include SGD (stochastic gradient descent) and Adam. For transformer-based models, Adam is, by far, the most widely used optimizer.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Compare the computed output from the forward pass against the expected output (ground truth). If they are different, the model made a mistake, and the parameters need to be adjusted. The difference between the computed output and the expected output is called the loss.",
      "raw_html": "<p>Compare the computed output from the forward pass against the expected output (ground truth). If they are different, the model made a mistake, and the parameters need to be adjusted. The difference between the computed output and the expected output is called the <em>loss</em>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Compute how much each trainable parameter contributes to the mistake. This value is called the gradient. Mathematically, gradients are computed by taking the derivative of the loss with respect to each trainable parameter. There’s one gradient value per trainable parameter.7 If a parameter has a high gradient, it significantly contributes to the loss and should be adjusted more.",
      "raw_html": "<p>Compute how much each trainable parameter contributes to the mistake. This value is called the <em>gradient</em>. Mathematically, gradients are computed by taking the derivative of the loss with respect to each trainable parameter. There’s one gradient value per trainable parameter.<sup><a data-type=\"noteref\" href=\"ch07.html#id1383\" id=\"id1383-marker\">7</a></sup> If a parameter has a high gradient, it significantly contributes to the loss and should be adjusted more.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Adjust trainable parameter values using their corresponding gradient. How much each parameter should be readjusted, given its gradient value, is determined by the optimizer. Common optimizers include SGD (stochastic gradient descent) and Adam. For transformer-based models, Adam is, by far, the most widely used optimizer.",
      "raw_html": "<p>Adjust trainable parameter values using their corresponding gradient. How much each parameter should be readjusted, given its gradient value, is determined by the <em>optimizer</em>. Common optimizers include SGD (stochastic gradient descent) and Adam. For transformer-based models, Adam is, by far, the most widely used optimizer.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The forward and backward pass for a hypothetical neural network with three parameters and one nonlinear activation function is visualized in Figure 7-4. I use this dummy neural network to simplify the visualization.",
      "raw_html": "<p>The forward and backward pass for a hypothetical neural network with three parameters and one nonlinear activation function is visualized in <a data-type=\"xref\" href=\"#ch07b_figure_1_1730159634220258\">Figure 7-4</a>. I use this dummy neural network to simplify the visualization.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_1_1730159634220258",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_1_1730159634220258\">\n<img alt=\"A diagram of a flowchart\n\nDescription automatically generated\" src=\"assets/aien_0704.png\"/>\n<h6><span class=\"label\">Figure 7-4. </span>The forward and backward pass of a simple neural network.</h6>\n</div></figure>",
      "image": "aien_0704.png",
      "alt": "A diagram of a flowchart\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0704.png",
      "caption": {
        "label": "Figure 7-4.",
        "text": "The forward and backward pass of a simple neural network."
      }
    },
    {
      "type": "paragraph",
      "content": "During the backward pass, each trainable parameter comes with additional values, its gradient, and its optimizer states. Therefore, the more trainable parameters there are, the more memory is needed to store these additional values.",
      "raw_html": "<p>During the backward pass, each trainable parameter comes with additional values, its gradient, and its optimizer states. Therefore, the more trainable parameters there are, the more memory is needed to store these additional values.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html13\" data-type=\"indexterm\" id=\"id1384\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html12\" data-type=\"indexterm\" id=\"id1385\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html11\" data-type=\"indexterm\" id=\"id1386\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Memory Math",
      "id": "heading-7",
      "raw_html": "<h2>Memory Math</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "It’s useful to know how much memory a model needs so that you can use the right hardware for it. Often, you might already have the hardware and need to calculate whether you can afford to run a certain model. If a model requires 30 GB of memory to do inference, a chip with 24 GB of memory won’t be sufficient.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"memory bottlenecks\" data-tertiary=\"memory math\" data-type=\"indexterm\" id=\"ch07.html14\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"memory math\" data-type=\"indexterm\" id=\"ch07.html15\"></a><a contenteditable=\"false\" data-primary=\"memory math\" data-type=\"indexterm\" id=\"ch07.html16\"></a>It’s useful to know how much memory a model needs so that you can use the right hardware for it. Often, you might already have the hardware and need to calculate whether you can afford to run a certain model. If a model requires 30 GB of memory to do inference, a chip with 24 GB of memory won’t be sufficient.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A model’s memory footprint depends on the model as well as the workload and the different optimization techniques used to reduce its memory usage. Because it’s impossible to account for all optimization techniques and workloads, in this section, I’ll outline only the formulas for approximate calculations, which should give you a rough idea of how much memory you need to operate a model, both during inference and training.",
      "raw_html": "<p>A model’s memory footprint depends on the model as well as the workload and the different optimization techniques used to reduce its memory usage. Because it’s impossible to account for all optimization techniques and workloads, in this section, I’ll outline only the formulas for approximate calculations, which should give you a rough idea of how much memory you need to operate a model, both during inference and training.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "Inference and training having distinct memory profiles is one of the reasons for the divergence in chips for training and inference, as discussed in Chapter 9.",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p>Inference and training having distinct memory profiles is one of the reasons for the divergence in chips for training and inference, as discussed in <a data-type=\"xref\" href=\"ch09.html#ch09_inference_optimization_1730130963006301\">Chapter 9</a>.</p>\n</div>"
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Memory needed for inference",
      "id": "heading-7",
      "raw_html": "<h3>Memory needed for inference</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "During inference, only the forward pass is executed. The forward pass requires memory for the model’s weights. Let N be the model’s parameter count and M be the memory needed for each parameter; the memory needed to load the model’s parameters is:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"memory math\" data-tertiary=\"memory needed for inference\" data-type=\"indexterm\" id=\"id1387\"></a>During inference, only the forward pass is executed. The forward pass requires memory for the model’s weights. Let N be the model’s parameter count and M be the memory needed for each parameter; the memory needed to load the model’s parameters is:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "N × M",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">N × M</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "The forward pass also requires memory for activation values. Transformer models need memory for key-value vectors for the attention mechanism. The memory for both activation values and key-value vectors grows linearly with sequence length and batch size.",
      "raw_html": "<p>The forward pass also requires memory for activation values. <a contenteditable=\"false\" data-primary=\"key-value vectors\" data-type=\"indexterm\" id=\"id1388\"></a>Transformer models need memory for key-value vectors for the attention mechanism. The memory for both activation values and key-value vectors grows linearly with sequence length and batch size.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "For many applications, the memory for activation and key-value vectors can be assumed to be 20% of the memory for the model’s weights. If your application uses a longer context or larger batch size, the actual memory needed will be higher. This assumption brings the model’s memory footprint to:",
      "raw_html": "<p>For many applications, the memory for activation and key-value vectors can be assumed to be 20% of the memory for the model’s weights. If your application uses a longer context or larger batch size, the actual memory needed will be higher. This assumption brings the model’s memory footprint to:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "N × M × 1.2",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">N × M × 1.2</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "Consider a 13B-parameter model. If each parameter requires 2 bytes, the model’s weights will require 13B × 2 bytes = 26 GB. The total memory for inference will be 26 GB × 1.2 = 31.2 GB.",
      "raw_html": "<p>Consider a 13B-parameter model. If each parameter requires 2 bytes, the model’s weights will require 13B × 2 bytes = 26 GB. The total memory for inference will be 26 GB × 1.2 = 31.2 GB.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A model’s memory footprint grows rapidly with its size. As models become bigger, memory becomes a bottleneck for operating them.8 A 70B-parameter model with 2 bytes per parameter will require a whooping 140 GB of memory just for its weights.9",
      "raw_html": "<p>A model’s memory footprint grows rapidly with its size. As models become bigger, memory becomes a bottleneck for operating them.<sup><a data-type=\"noteref\" href=\"ch07.html#id1389\" id=\"id1389-marker\">8</a></sup> A 70B-parameter model with 2 bytes per parameter will require a whooping 140 GB of memory just for its weights.<sup><a data-type=\"noteref\" href=\"ch07.html#id1390\" id=\"id1390-marker\">9</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Memory needed for training",
      "id": "heading-7",
      "raw_html": "<h3>Memory needed for training</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "To train a model, you need memory for the model’s weights and activations, which has already been discussed. Additionally, you need memory for gradients and optimizer states, which scales with the number of trainable parameters.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"memory math\" data-tertiary=\"memory needed for training\" data-type=\"indexterm\" id=\"ch07.html17\"></a>To train a model, you need memory for the model’s weights and activations, which has already been discussed. Additionally, you need memory for gradients and optimizer states, which scales with the number of trainable parameters.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Overall, the memory needed for training is calculated as:",
      "raw_html": "<p>Overall, the memory needed for training is calculated as:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Training memory = model weights + activations + gradients + optimizer states"
      ],
      "raw_html": "<ul class=\"simplelist\">\n<li><p>Training memory = model weights + activations + gradients + optimizer states</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Training memory = model weights + activations + gradients + optimizer states",
      "raw_html": "<p>Training memory = model weights + activations + gradients + optimizer states</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Tip",
      "id": "heading-7",
      "raw_html": "<h6>Tip</h6>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "During the backward pass, each trainable parameter requires one value for gradient plus zero to two values for optimizer states, depending on the optimizer:",
      "raw_html": "<p>During the backward pass, each trainable parameter requires one value for gradient plus zero to two values for optimizer states, depending on the optimizer:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "A vanilla SGD optimizer has no state.",
        "A momentum optimizer stores one value per trainable parameter.",
        "An Adam optimizer stores two values per trainable parameter."
      ],
      "raw_html": "<ul>\n<li><p>A vanilla SGD optimizer has no state.</p></li>\n<li><p>A momentum optimizer stores one value per trainable <span class=\"keep-together\">parameter.</span></p></li>\n<li><p>An Adam optimizer stores two values per trainable parameter.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "A vanilla SGD optimizer has no state.",
      "raw_html": "<p>A vanilla SGD optimizer has no state.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A momentum optimizer stores one value per trainable parameter.",
      "raw_html": "<p>A momentum optimizer stores one value per trainable <span class=\"keep-together\">parameter.</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "An Adam optimizer stores two values per trainable parameter.",
      "raw_html": "<p>An Adam optimizer stores two values per trainable parameter.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Imagine you’re updating all parameters in a 13B-parameter model using the Adam optimizer. Because each trainable parameter has three values for its gradient and optimizer states, if it takes two bytes to store each value, the memory needed for gradients and optimizer states will be:",
      "raw_html": "<p>Imagine you’re updating all parameters in a 13B-parameter model using the Adam optimizer. Because each trainable parameter has three values for its gradient and optimizer states, if it takes two bytes to store each value, the memory needed for gradients and optimizer states will be:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "13 billion × 3 × 2 bytes = 78 GB",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">13 billion × 3 × 2 bytes = 78 GB</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "However, if you only have 1B trainable parameters, the memory needed for gradients and optimizer states will be only:",
      "raw_html": "<p>However, if you only have 1B trainable parameters, the memory needed for gradients and optimizer states will be only:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "1 billion × 3 × 2 bytes = 6 GB",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">1 billion × 3 × 2 bytes = 6 GB</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "One important thing to note is that in the previous formula, I assumed that the memory needed for activations is less than the memory needed for the model’s weights. However, in reality, the activation memory can be much larger. If activations are stored for gradient computation, the memory needed for activations can dwarf the memory needed for the model’s weights. Figure 7-5 shows the memory needed for activations compared to the memory needed for the model’s weights for different Megatron models at different scales, according to the paper “Reducing Activation Recomputation in Large Transformer Models”, by Korthikanti et al. (2022).",
      "raw_html": "<p>One important thing to note is that in the previous formula, I assumed that the memory needed for activations is less than the memory needed for the model’s weights. However, in reality, the activation memory can be much larger. If activations are stored for gradient computation, the memory needed for activations can dwarf the memory needed for the model’s weights. <a data-type=\"xref\" href=\"#ch07b_figure_2_1730159634220278\">Figure 7-5</a> shows the memory needed for activations compared to the memory needed for the model’s weights for different Megatron models at different scales, according to the paper <a href=\"https://arxiv.org/abs/2205.05198\">“Reducing Activation Recomputation in Large Transformer Models”</a>, by Korthikanti et al. (2022).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One way to reduce the memory needed for activations is not to store them. Instead of storing activations for reuse, you recompute activations when necessary. This technique is called gradient checkpointing or activation recomputation. While this reduces the memory requirements, it increases the time needed for training due to the recomputation.10",
      "raw_html": "<p>One way to reduce the memory needed for activations is not to store them. Instead of storing activations for reuse, you recompute activations when necessary. This technique is called <em>gradient checkpointing</em> or <em>activation recomputation</em>. While this reduces the memory requirements, it increases the time needed for training due to the <span class=\"keep-together\">recomputation</span><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html17\" data-type=\"indexterm\" id=\"id1391\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html16\" data-type=\"indexterm\" id=\"id1392\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html15\" data-type=\"indexterm\" id=\"id1393\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html14\" data-type=\"indexterm\" id=\"id1394\"></a><sup><a data-type=\"noteref\" href=\"ch07.html#id1395\" id=\"id1395-marker\">10</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch07b_figure_2_1730159634220278",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_2_1730159634220278\">\n<img alt=\"A graph of a graph\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0705.png\"/>\n<h6><span class=\"label\">Figure 7-5. </span>The memory needed for activations can dwarf the memory needed for the model’s weights. Image from Korthikanti et al., 2022.</h6>\n</div></figure>",
      "image": "aien_0705.png",
      "alt": "A graph of a graph\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0705.png",
      "caption": {
        "label": "Figure 7-5.",
        "text": "The memory needed for activations can dwarf the memory needed for the model’s weights. Image from Korthikanti et al., 2022."
      }
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Numerical Representations",
      "id": "heading-7",
      "raw_html": "<h2>Numerical Representations</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "In the memory calculation so far, I’ve assumed that each value takes up two bytes of memory. The memory required to represent each value in a model contributes directly to the model’s overall memory footprint. If you reduce the memory needed for each value by half, the memory needed for the model’s weights is also reduced by half.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"memory bottlenecks\" data-tertiary=\"numerical representations\" data-type=\"indexterm\" id=\"ch07.html18\"></a>In the memory calculation so far, I’ve assumed that each value takes up two bytes of memory. The memory required to represent each value in a model contributes directly to the model’s overall memory footprint. If you reduce the memory needed for each value by half, the memory needed for the model’s weights is also reduced by half.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Before discussing how to reduce the memory needed for each value, it’s useful to understand numerical representations. Numerical values in neural networks are traditionally represented as float numbers. The most common family of floating point formats is the FP family, which adheres to the Institute of Electrical and Electronics Engineers (IEEE) standard for Floating-Point Arithmetic (IEEE 754):",
      "raw_html": "<p>Before discussing how to reduce the memory needed for each value, it’s useful to understand numerical representations. Numerical values in neural networks are traditionally represented as <a href=\"https://en.wikipedia.org/wiki/Floating-point_arithmetic\">float numbers</a>. The most common family of floating point formats is the FP family, which adheres to the Institute of Electrical and Electronics Engineers (IEEE) standard for Floating-Point Arithmetic (<a href=\"https://en.wikipedia.org/wiki/IEEE_754\">IEEE 754</a>):</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "FP32 uses 32 bits (4 bytes) to represent a float. This format is called single precision.",
        "FP64 uses 64 bits (8 bytes) and is called double precision.",
        "FP16 uses 16 bits (2 bytes) and is called half precision."
      ],
      "raw_html": "<ul>\n<li><p>FP32 uses 32 bits (4 bytes) to represent a float. This format is called single <span class=\"keep-together\">precision.</span></p></li>\n<li><p>FP64 uses 64 bits (8 bytes) and is called double precision.</p></li>\n<li><p>FP16 uses 16 bits (2 bytes) and is called half precision.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "FP32 uses 32 bits (4 bytes) to represent a float. This format is called single precision.",
      "raw_html": "<p>FP32 uses 32 bits (4 bytes) to represent a float. This format is called single <span class=\"keep-together\">precision.</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "FP64 uses 64 bits (8 bytes) and is called double precision.",
      "raw_html": "<p>FP64 uses 64 bits (8 bytes) and is called double precision.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "FP16 uses 16 bits (2 bytes) and is called half precision.",
      "raw_html": "<p>FP16 uses 16 bits (2 bytes) and is called half precision.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While FP64 is still used in many computations—as of this writing, FP64 is the default format for NumPy and pandas—it’s rarely used in neural networks because of its memory footprint. FP32 and FP16 are more common. Other popular floating point formats in AI workloads include BF16 (BFloat16) and TF32 (TensorFloat-32). BF16 was designed by Google to optimize AI performance on TPUs and TF32 was designed by NVIDIA for GPUs.11",
      "raw_html": "<p class=\"pagebreak-before\">While FP64 is still used in many computations—as of this writing, FP64 is the default format for NumPy and pandas—it’s rarely used in neural networks because of its memory footprint. FP32 and FP16 are more common. Other popular floating point formats in AI workloads include <em>BF16</em> (BFloat16) and <em>TF32</em> (TensorFloat-32). BF16 was designed by Google to optimize AI performance on <a href=\"https://oreil.ly/BGXtn\">TPUs</a> and TF32 was designed by NVIDIA for <a href=\"https://oreil.ly/0pZgw\">GPUs</a>.<sup><a data-type=\"noteref\" href=\"ch07.html#id1396\" id=\"id1396-marker\">11</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Numbers can also be represented as integers. Even though not yet as common as floating formats, integer representations are becoming increasingly popular. Common integer formats are INT8 (8-bit integers) and INT4 (4-bit integers).12",
      "raw_html": "<p>Numbers can also be represented as integers. Even though not yet as common as floating formats, integer representations are becoming increasingly popular. Common integer formats are INT8 (8-bit integers) and INT4 (4-bit integers).<sup><a data-type=\"noteref\" href=\"ch07.html#id1397\" id=\"id1397-marker\">12</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Each float format usually has 1 bit to represent the number’s sign, i.e., negative or positive. The rest of the bits are split between range and precision:13",
      "raw_html": "<p>Each float format usually has 1 bit to represent the number’s sign, i.e., negative or positive. The rest of the bits are split between <em>range</em> and <em>precision</em>:<sup><a data-type=\"noteref\" href=\"ch07.html#id1398\" id=\"id1398-marker\">13</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Range",
          "definition": "The number of range bits determines the range of values the format can represent. More bits means a wider range. This is similar to how having more digits lets you represent a wider range of numbers."
        },
        {
          "term": "Precision",
          "definition": "The number of precision bits determines how precisely a number can be represented. Reducing the number of precision bits makes a number less precise. For example, if you convert 10.1234 to a format that can support only two decimal digits, this value becomes 10.12, which is less precise than the original value."
        }
      ],
      "raw_html": "<dl>\n<dt>Range</dt>\n<dd>\n<p><a contenteditable=\"false\" data-primary=\"range bits\" data-type=\"indexterm\" id=\"id1399\"></a>The number of range bits determines the range of values the format can represent. More bits means a wider range. This is similar to how having more digits lets you represent a wider range of numbers.</p>\n</dd>\n<dt>Precision</dt>\n<dd>\n<p><a contenteditable=\"false\" data-primary=\"precision bits\" data-type=\"indexterm\" id=\"id1400\"></a>The number of precision bits determines how precisely a number can be represented. Reducing the number of precision bits makes a number less precise. For example, if you convert 10.1234 to a format that can support only two decimal digits, this value becomes 10.12, which is less precise than the original value.</p>\n</dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "The number of range bits determines the range of values the format can represent. More bits means a wider range. This is similar to how having more digits lets you represent a wider range of numbers.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"range bits\" data-type=\"indexterm\" id=\"id1399\"></a>The number of range bits determines the range of values the format can represent. More bits means a wider range. This is similar to how having more digits lets you represent a wider range of numbers.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The number of precision bits determines how precisely a number can be represented. Reducing the number of precision bits makes a number less precise. For example, if you convert 10.1234 to a format that can support only two decimal digits, this value becomes 10.12, which is less precise than the original value.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"precision bits\" data-type=\"indexterm\" id=\"id1400\"></a>The number of precision bits determines how precisely a number can be represented. Reducing the number of precision bits makes a number less precise. For example, if you convert 10.1234 to a format that can support only two decimal digits, this value becomes 10.12, which is less precise than the original value.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Figure 7-6 shows different floating point formats along with their range and precision bits.14",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch07b_figure_3_1730159634220288\">Figure 7-6</a> shows different floating point formats along with their range and precision bits.<sup><a data-type=\"noteref\" href=\"ch07.html#id1401\" id=\"id1401-marker\">14</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_3_1730159634220288",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_3_1730159634220288\">\n<img alt=\"A graph with numbers and text\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0706.png\"/>\n<h6><span class=\"label\">Figure 7-6. </span>Different numerical formats with their range and precision.</h6>\n</div></figure>",
      "image": "aien_0706.png",
      "alt": "A graph with numbers and text\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0706.png",
      "caption": {
        "label": "Figure 7-6.",
        "text": "Different numerical formats with their range and precision."
      }
    },
    {
      "type": "paragraph",
      "content": "Formats with more bits are considered higher precision. Converting a number with a high-precision format into a low-precision format (e.g., from FP32 to FP16) means reducing its precision. Reducing precision can cause a value to change or result in errors. Table 7-3 shows how FP32 values can be converted into FP16, BF16, and TF32.",
      "raw_html": "<p>Formats with more bits are considered <em>higher precision</em>. Converting a number with a high-precision format into a low-precision format (e.g., from FP32 to FP16) means <em>reducing its precision</em>. Reducing precision can cause a value to change or result in errors. <a data-type=\"xref\" href=\"#ch07b_table_1_1730159634233580\">Table 7-3</a> shows how FP32 values can be converted into FP16, BF16, and TF32.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "table",
      "id": "ch07b_table_1_1730159634233580",
      "raw_html": "<table id=\"ch07b_table_1_1730159634233580\">\n<caption><span class=\"label\">Table 7-3. </span>Convert from FP32 values to lower-precision formats. Resultant inaccuracies are in italics.</caption>\n<thead>\n<tr>\n<th>FP32</th>\n<th>FP16</th>\n<th>BF16</th>\n<th>TF32</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0.0123456789</td>\n<td>0.01234<em>43603515625</em></td>\n<td>0.0123<em>291</em></td>\n<td>0.01234<em>43603515625</em></td>\n</tr>\n<tr>\n<td>0.123456789</td>\n<td>0.1234<em>7412109375</em></td>\n<td>0.123<em>535</em></td>\n<td>0.1234<em>130859375</em></td>\n</tr>\n<tr>\n<td>1.23456789</td>\n<td>1.234<em>375</em></td>\n<td>1.234<em>38</em></td>\n<td>1.234<em>375</em></td>\n</tr>\n<tr>\n<td>12.3456789</td>\n<td>12.34<em>375</em></td>\n<td>12.3<em>75</em></td>\n<td>12.34<em>375</em></td>\n</tr>\n<tr>\n<td>123.456789</td>\n<td>123.4<em>375</em></td>\n<td>123.<em>5</em></td>\n<td>123.4<em>375</em></td>\n</tr>\n<tr>\n<td>1234.56789</td>\n<td>123<em>5.0</em></td>\n<td>123<em>2.0</em></td>\n<td>1234.<em>0</em></td>\n</tr>\n<tr>\n<td>12345.6789</td>\n<td>1234<em>4.0</em></td>\n<td>123<em>52.0</em></td>\n<td>1234<em>4.0</em></td>\n</tr>\n<tr>\n<td>123456.789</td>\n<td><em>INF</em><sup><a data-type=\"noteref\" href=\"ch07.html#id1402\" id=\"id1402-marker\">a</a></sup></td>\n<td>123<em>392.0</em></td>\n<td>123456.<em>0</em></td>\n</tr>\n<tr>\n<td>1234567.89</td>\n<td>\n<em>INF</em>\n</td>\n<td>123<em>6990.0</em></td>\n<td>123<em>3920.0</em></td>\n</tr>\n</tbody>\n<tbody><tr class=\"footnotes\"><td colspan=\"4\"><p data-type=\"footnote\" id=\"id1402\"><sup><a href=\"ch07.html#id1402-marker\">a</a></sup> Values out of bound in FP16 are rounded to infinity.</p></td></tr></tbody></table>",
      "caption": {
        "label": "Table 7-3.",
        "text": "Convert from FP32 values to lower-precision formats. Resultant inaccuracies are in italics."
      },
      "headers": [
        "FP32",
        "FP16",
        "BF16",
        "TF32"
      ],
      "rows": [
        [
          "0.0123456789",
          "0.0123443603515625",
          "0.0123291",
          "0.0123443603515625"
        ],
        [
          "0.123456789",
          "0.12347412109375",
          "0.123535",
          "0.1234130859375"
        ],
        [
          "1.23456789",
          "1.234375",
          "1.23438",
          "1.234375"
        ],
        [
          "12.3456789",
          "12.34375",
          "12.375",
          "12.34375"
        ],
        [
          "123.456789",
          "123.4375",
          "123.5",
          "123.4375"
        ],
        [
          "1234.56789",
          "1235.0",
          "1232.0",
          "1234.0"
        ],
        [
          "12345.6789",
          "12344.0",
          "12352.0",
          "12344.0"
        ],
        [
          "123456.789",
          "INFa",
          "123392.0",
          "123456.0"
        ],
        [
          "1234567.89",
          "INF",
          "1236990.0",
          "1233920.0"
        ]
      ],
      "row_count": 9,
      "column_count": 4
    },
    {
      "type": "paragraph",
      "content": "a Values out of bound in FP16 are rounded to infinity.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1402\"><sup><a href=\"ch07.html#id1402-marker\">a</a></sup> Values out of bound in FP16 are rounded to infinity.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Note in Table 7-3 that even though BF16 and FP16 have the same number of bits, BF16 has more bits for range and fewer bits for precision. This allows BF16 to represent large values that are out-of-bound for FP16. However, this also makes BF16 less precise than FP16. For example, 1234.56789 is 1235.0 in FP16 (0.035% value change) but 1232.0 in BF16 (0.208% value change).",
      "raw_html": "<p>Note in <a data-type=\"xref\" href=\"#ch07b_table_1_1730159634233580\">Table 7-3</a> that even though BF16 and FP16 have the same number of bits, BF16 has more bits for range and fewer bits for precision. This allows BF16 to represent large values that are out-of-bound for FP16. However, this also makes BF16 less precise than FP16. For example, 1234.56789 is 1235.0 in FP16 (0.035% value change) but 1232.0 in BF16 (0.208% value change).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Warning",
      "id": "heading-7",
      "raw_html": "<h6>Warning</h6>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "When using a model, make sure to load the model in the format it’s intended for. Loading a model into the wrong numerical format can cause the model to change significantly. For example, Llama 2 had its weights set to BF16 when it came out. However, many teams loaded the model in FP16 and were subsequently frustrated to find the model’s quality much worse than advertised.15 While this misunderstanding wasted a lot of people’s time, the upside is that it forced many people to learn about numerical representations.",
      "raw_html": "<p>When using a model, make sure to load the model in the format it’s intended for. Loading a model into the wrong numerical <span class=\"keep-together\">format can</span> cause the model to change significantly. For example, Llama 2 had its weights set to BF16 when it came out. However, many teams loaded the model in FP16 and were subsequently frustrated to find the model’s quality much worse than advertised<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html18\" data-type=\"indexterm\" id=\"id1403\"></a>.<sup><a data-type=\"noteref\" href=\"ch07.html#id1404\" id=\"id1404-marker\">15</a></sup> While this misunderstanding wasted a lot of people’s time, the upside is that it forced many people to learn about numerical <span class=\"keep-together\">representations.</span></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The right format for you depends on the distribution of numerical values of your workload (such as the range of values you need), how sensitive your workload is to small numerical changes, and the underlying hardware.16",
      "raw_html": "<p>The right format for you depends on the distribution of numerical values of your workload (such as the range of values you need), how sensitive your workload is to small numerical changes, and the underlying hardware.<sup><a data-type=\"noteref\" href=\"ch07.html#id1405\" id=\"id1405-marker\">16</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Quantization",
      "id": "heading-7",
      "raw_html": "<h2>Quantization</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "The fewer bits needed to represent a model’s values, the lower the model’s memory footprint will be. A 10B-parameter model in a 32-bit format requires 40 GB for its weights, but the same model in a 16-bit format will require only 20 GB. Reducing precision, also known as quantization, is a cheap and extremely effective way to reduce a model’s memory footprint. It’s straightforward to do and generalizes over tasks and architectures. In the context of ML, low precision generally refers to any format with fewer bits than the standard FP32.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"memory bottlenecks\" data-tertiary=\"quantization\" data-type=\"indexterm\" id=\"ch07.html19\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"quantization\" data-type=\"indexterm\" id=\"ch07.html20\"></a><a contenteditable=\"false\" data-primary=\"quantization\" data-type=\"indexterm\" id=\"ch07.html21\"></a>The fewer bits needed to represent a model’s values, the lower the model’s memory footprint will be. A 10B-parameter model in a 32-bit format requires 40 GB for its weights, but the same model in a 16-bit format will require only 20 GB. Reducing precision, also known as quantization, is a cheap and extremely effective way to reduce a model’s memory footprint. It’s straightforward to do and generalizes over tasks and architectures. In the context of ML, low precision generally refers to any format with fewer bits than the standard FP32.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "aside",
      "content": "Quantization Versus Reduced Precision\nStrictly speaking, it’s quantization only if the target format is integer. However, in practice, quantization is used to refer to all techniques that convert values to a lower-precision format. In this book, I use quantization to refer to precision reduction, to keep it consistent with the literature.",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"ch07b_quantization_versus_reduced_precision_1730159634259565\">\n<h1>Quantization Versus Reduced Precision</h1>\n<p>Strictly speaking, it’s quantization only if the target format is integer. However, in practice, quantization is used to refer to all techniques that convert values to a lower-precision format. In this book, I use quantization to refer to precision reduction, to keep it consistent with the literature.</p>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Quantization Versus Reduced Precision",
      "id": "heading-7",
      "raw_html": "<h1>Quantization Versus Reduced Precision</h1>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "Strictly speaking, it’s quantization only if the target format is integer. However, in practice, quantization is used to refer to all techniques that convert values to a lower-precision format. In this book, I use quantization to refer to precision reduction, to keep it consistent with the literature.",
      "raw_html": "<p>Strictly speaking, it’s quantization only if the target format is integer. However, in practice, quantization is used to refer to all techniques that convert values to a lower-precision format. In this book, I use quantization to refer to precision reduction, to keep it consistent with the literature.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "To do quantization, you need to decide what to quantize and when:",
      "raw_html": "<p class=\"pagebreak-before\">To do quantization, you need to decide what to quantize and when:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "What to quantize",
          "definition": "Ideally, you want to quantize whatever is consuming most of your memory, but it also depends on what you can quantize without hurting performance too much. As discussed in “Memory Math”, major contributors to a model’s memory footprint during inference are the model’s weights and activations.17 Weight quantization is more common than activation quantization, since weight activation tends to have a more stable impact on performance with less accuracy loss."
        },
        {
          "term": "When to quantize",
          "definition": "Quantization can happen during training or post-training. Post-training quantization (PTQ) means quantizing a model after it’s been fully trained. PTQ is by far the most common. It’s also more relevant to AI application developers who don’t usually train models."
        }
      ],
      "raw_html": "<dl>\n<dt>What to quantize</dt>\n<dd>\n<p>Ideally, you want to quantize whatever is consuming most of your memory, but it also depends on what you can quantize without hurting performance too much. As discussed in <a data-type=\"xref\" href=\"#ch07b_memory_math_1730159634259402\">“Memory Math”</a>, major contributors to a model’s memory footprint during inference are the model’s weights and activations.<sup><a data-type=\"noteref\" href=\"ch07.html#id1406\" id=\"id1406-marker\">17</a></sup> Weight quantization is more common than activation quantization, since weight activation tends to have a more stable impact on performance with less accuracy loss.</p>\n</dd>\n<dt>When to quantize</dt>\n<dd>\n<p>Quantization can happen during training or post-training. Post-training quantization (PTQ) means quantizing a model after it’s been fully trained. PTQ is by far the most common. It’s also more relevant to AI application developers who don’t usually train models.</p>\n</dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "Ideally, you want to quantize whatever is consuming most of your memory, but it also depends on what you can quantize without hurting performance too much. As discussed in “Memory Math”, major contributors to a model’s memory footprint during inference are the model’s weights and activations.17 Weight quantization is more common than activation quantization, since weight activation tends to have a more stable impact on performance with less accuracy loss.",
      "raw_html": "<p>Ideally, you want to quantize whatever is consuming most of your memory, but it also depends on what you can quantize without hurting performance too much. As discussed in <a data-type=\"xref\" href=\"#ch07b_memory_math_1730159634259402\">“Memory Math”</a>, major contributors to a model’s memory footprint during inference are the model’s weights and activations.<sup><a data-type=\"noteref\" href=\"ch07.html#id1406\" id=\"id1406-marker\">17</a></sup> Weight quantization is more common than activation quantization, since weight activation tends to have a more stable impact on performance with less accuracy loss.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Quantization can happen during training or post-training. Post-training quantization (PTQ) means quantizing a model after it’s been fully trained. PTQ is by far the most common. It’s also more relevant to AI application developers who don’t usually train models.",
      "raw_html": "<p>Quantization can happen during training or post-training. Post-training quantization (PTQ) means quantizing a model after it’s been fully trained. PTQ is by far the most common. It’s also more relevant to AI application developers who don’t usually train models.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Inference quantization",
      "id": "heading-7",
      "raw_html": "<h3>Inference quantization</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "In the early days of deep learning, it was standard to train and serve models using 32 bits with FP32. Since the late 2010s, it has become increasingly common to serve models in 16 bits and in even lower precision. For example, Dettmers et al. (2022) have done excellent work quantizing LLMs into 8 bits with LLM.int8() and 4 bits with QLoRA (Dettmers et al., 2023).",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference quantization\" data-type=\"indexterm\" id=\"ch07.html22\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"quantization\" data-tertiary=\"inference quantization\" data-type=\"indexterm\" id=\"ch07.html23\"></a><a contenteditable=\"false\" data-primary=\"quantization\" data-secondary=\"inference quantization\" data-type=\"indexterm\" id=\"ch07.html24\"></a>In the early days of deep learning, it was standard to train and serve models using 32 bits with FP32. Since the late 2010s, it has become increasingly common to serve models in 16 bits and in even lower precision. For example, <a href=\"https://arxiv.org/abs/2208.07339\">Dettmers et al. (2022)</a> have done excellent work quantizing LLMs into 8 bits with LLM.int8() and 4 bits with QLoRA (<a href=\"https://arxiv.org/abs/2305.14314\">Dettmers et al., 2023</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A model can also be served in mixed precision, where values are reduced in precision when possible and maintained in higher precision when necessary. To serve models on the devices, Apple (2024) leveraged a quantization scheme that uses a mixture of 2-bit and 4-bit formats, averaging 3.5 bits-per-weight. Also in 2024, in anticipation of 4-bit neural networks, NVIDIA announced their new GPU architecture, Blackwell, that supports model inference in 4-bit float.",
      "raw_html": "<p>A model can also be served in <em>mixed precision</em>, where values are reduced in precision when possible and maintained in higher precision when necessary. To serve models on the devices, <a href=\"https://oreil.ly/lqLfv\">Apple</a> (2024) leveraged a quantization scheme that uses a mixture of 2-bit and 4-bit formats, averaging 3.5 bits-per-weight. Also in 2024, in anticipation of 4-bit neural networks, NVIDIA announced their new GPU architecture, <a href=\"https://oreil.ly/FIP9V\">Blackwell</a>, that supports model inference in 4-bit float.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Once you get to 8 bits and under, numerical representations get more tricky. You can keep parameter values as floats using one of the minifloat formats, such as FP8 (8 bits) and FP4 (4 bits).18 More commonly, however, parameter values are converted into an integer format, such as INT8 or INT4.",
      "raw_html": "<p>Once you get to 8 bits and under, numerical representations get more tricky. You can keep parameter values as floats using one of the <a href=\"https://en.wikipedia.org/wiki/Minifloat\">minifloat</a> formats, such as FP8 (8 bits) and FP4 (4 bits).<sup><a data-type=\"noteref\" href=\"ch07.html#id1407\" id=\"id1407-marker\">18</a></sup> More commonly, however, parameter values are converted into an integer format, such as INT8 or INT4.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Quantization is effective, but there’s a limit to how far it can go. You can’t have fewer than 1 bit per value, and some have attempted the 1-bit representation, e.g., BinaryConnect (Courbariaux et al., 2015), Xnor-Net (Rastegari et al., 2016), and BitNet (Wang et al., 2023).19",
      "raw_html": "<p class=\"pagebreak-before\">Quantization is effective, but there’s a limit to how far it can go. You can’t have fewer than 1 bit per value, and some have attempted the 1-bit representation, e.g., BinaryConnect (<a href=\"https://arxiv.org/abs/1511.00363\">Courbariaux et al., 2015</a>), Xnor-Net (<a href=\"https://arxiv.org/abs/1603.05279\">Rastegari et al., 2016</a>), and BitNet (<a href=\"https://arxiv.org/abs/2310.11453\">Wang et al., 2023</a>).<sup><a data-type=\"noteref\" href=\"ch07.html#id1408\" id=\"id1408-marker\">19</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In 2024, Microsoft researchers (Ma et al.) declared that we’re entering the era of 1-bit LLMs by introducing BitNet b1.58, a transformer-based language model that requires only 1.58 bits per parameter and whose performance is comparable to 16-bit Llama 2 (Touvron et al., 2023) up to 3.9B parameters, as shown in Table 7-4.",
      "raw_html": "<p>In 2024, Microsoft researchers (<a href=\"https://arxiv.org/abs/2402.17764\">Ma et al.</a>) declared that we’re entering the era of 1-bit LLMs by introducing BitNet b1.58, a transformer-based language model that requires only 1.58 bits per parameter and whose performance is comparable to 16-bit <a contenteditable=\"false\" data-primary=\"Llama\" data-secondary=\"inference quantization\" data-type=\"indexterm\" id=\"id1409\"></a>Llama 2 (<a href=\"https://arxiv.org/abs/2307.09288\">Touvron et al., 2023</a>) up to 3.9B parameters, as shown in <a data-type=\"xref\" href=\"#ch07b_table_2_1730159634233604\">Table 7-4</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch07b_table_2_1730159634233604",
      "raw_html": "<table id=\"ch07b_table_2_1730159634233604\">\n<caption><span class=\"label\">Table 7-4. </span>BitNet b1.58’s performance compared to that of Llama 2 16-bit on different benchmarks and at different model sizes, up to 3.9B parameters. Results from Ma et al. (2024).</caption>\n<thead>\n<tr>\n<th>Model</th>\n<th>Size</th>\n<th>ARCe</th>\n<th>ARCc</th>\n<th>HS</th>\n<th>BQ</th>\n<th>OQ</th>\n<th>PQ</th>\n<th>WGe</th>\n<th>Avg.</th>\n</tr>\n</thead>\n<tr>\n<td>Llama LLM</td>\n<td>700M</td>\n<td>54.7</td>\n<td>23.0</td>\n<td>37.0</td>\n<td>60.0</td>\n<td>20.2</td>\n<td>68.9</td>\n<td>54.8</td>\n<td>45.5</td>\n</tr>\n<tr>\n<td>BitNet b1.58</td>\n<td>700M</td>\n<td>51.8</td>\n<td>21.4</td>\n<td>35.1</td>\n<td>58.2</td>\n<td>20.0</td>\n<td>68.1</td>\n<td>55.2</td>\n<td>44.3</td>\n</tr>\n<tr>\n<td>Llama LLM</td>\n<td>1.3B</td>\n<td>56.9</td>\n<td>23.5</td>\n<td>38.5</td>\n<td>59.1</td>\n<td>21.6</td>\n<td>70.0</td>\n<td>53.9</td>\n<td>46.2</td>\n</tr>\n<tr>\n<td>BitNet b1.58</td>\n<td>1.3B</td>\n<td>54.9</td>\n<td>24.2</td>\n<td>37.7</td>\n<td>56.7</td>\n<td>19.6</td>\n<td>68.8</td>\n<td>55.8</td>\n<td>45.4</td>\n</tr>\n<tr>\n<td>Llama LLM</td>\n<td>3B</td>\n<td>62.1</td>\n<td>25.6</td>\n<td>43.3</td>\n<td>61.8</td>\n<td>24.6</td>\n<td>72.1</td>\n<td>58.2</td>\n<td>49.7</td>\n</tr>\n<tr>\n<td>BitNet b1.58</td>\n<td>3B</td>\n<td>61.4</td>\n<td>28.3</td>\n<td>42.9</td>\n<td>61.5</td>\n<td>26.6</td>\n<td>71.5</td>\n<td>59.3</td>\n<td>50.2</td>\n</tr>\n<tr>\n<td>BitNet b1.58</td>\n<td>3.9B</td>\n<td>64.2</td>\n<td>28.7</td>\n<td>44.2</td>\n<td>63.5</td>\n<td>24.2</td>\n<td>73.2</td>\n<td>60.5</td>\n<td>51.2</td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 7-4.",
        "text": "BitNet b1.58’s performance compared to that of Llama 2 16-bit on different benchmarks and at different model sizes, up to 3.9B parameters. Results from Ma et al. (2024)."
      },
      "headers": [
        "Model",
        "Size",
        "ARCe",
        "ARCc",
        "HS",
        "BQ",
        "OQ",
        "PQ",
        "WGe",
        "Avg."
      ],
      "rows": [
        [
          "Llama LLM",
          "700M",
          "54.7",
          "23.0",
          "37.0",
          "60.0",
          "20.2",
          "68.9",
          "54.8",
          "45.5"
        ],
        [
          "BitNet b1.58",
          "700M",
          "51.8",
          "21.4",
          "35.1",
          "58.2",
          "20.0",
          "68.1",
          "55.2",
          "44.3"
        ],
        [
          "Llama LLM",
          "1.3B",
          "56.9",
          "23.5",
          "38.5",
          "59.1",
          "21.6",
          "70.0",
          "53.9",
          "46.2"
        ],
        [
          "BitNet b1.58",
          "1.3B",
          "54.9",
          "24.2",
          "37.7",
          "56.7",
          "19.6",
          "68.8",
          "55.8",
          "45.4"
        ],
        [
          "Llama LLM",
          "3B",
          "62.1",
          "25.6",
          "43.3",
          "61.8",
          "24.6",
          "72.1",
          "58.2",
          "49.7"
        ],
        [
          "BitNet b1.58",
          "3B",
          "61.4",
          "28.3",
          "42.9",
          "61.5",
          "26.6",
          "71.5",
          "59.3",
          "50.2"
        ],
        [
          "BitNet b1.58",
          "3.9B",
          "64.2",
          "28.7",
          "44.2",
          "63.5",
          "24.2",
          "73.2",
          "60.5",
          "51.2"
        ]
      ],
      "row_count": 7,
      "column_count": 10
    },
    {
      "type": "paragraph",
      "content": "Reduced precision not only reduces the memory footprint but also often improves computation speed. First, it allows a larger batch size, enabling the model to process more inputs in parallel. Second, reduced precision speeds up computation, which further reduces inference latency and training time. To illustrate this, consider the addition of two numbers. If we perform the addition bit by bit, and each takes t nanoseconds, it’ll take 32t nanoseconds for 32 bits but only 16t nanoseconds for 16 bits. However, reducing precision doesn’t always reduce latency due to the added computation needed for format conversion.",
      "raw_html": "<p>Reduced precision not only reduces the memory footprint but also often improves computation speed. First, it allows a larger batch size, enabling the model to process more inputs in parallel. Second, reduced precision speeds up computation, which further reduces inference latency and training time. To illustrate this, consider the addition of two numbers. If we perform the addition bit by bit, and each takes <em>t</em> nanoseconds, it’ll take <em>32t</em> nanoseconds for 32 bits but only <em>16t</em> nanoseconds for 16 bits. However, reducing precision doesn’t always reduce latency due to the added computation needed for format conversion.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "There are downsides to reduced precision. Each conversion often causes a small value change, and many small changes can cause a big performance change. If a value is outside the range the reduced precision format can represent, it might be converted to infinity or an arbitrary value, causing the model’s quality to further degrade. How to reduce precision with minimal impact on model performance is an active area of research, pursued by model developers as well as by hardware makers and application developers.",
      "raw_html": "<p>There are downsides to reduced precision. Each conversion often causes a small value change, and many small changes can cause a big performance change. If a value is outside the range the reduced precision format can represent, it might be converted to infinity or an arbitrary value, causing the model’s quality to further degrade. How to reduce precision with minimal impact on model performance is an active area of research, pursued by model developers as well as by hardware makers and application developers. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Inference in lower precision has become a standard. A model is trained using a higher-precision format to maximize performance, then its precision is reduced for inference. Major ML frameworks, including PyTorch, TensorFlow, and Hugging Face’s transformers, offer PTQ for free with a few lines of code.",
      "raw_html": "<p>Inference in lower precision has become a standard. A model is trained using a higher-precision format to maximize performance, then its precision is reduced for inference. Major ML frameworks, including PyTorch, TensorFlow, and Hugging Face’s transformers, offer PTQ for free with a few lines of code.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Some edge devices only support quantized inference. Therefore, frameworks for on-device inference, such as TensorFlow Lite and PyTorch Mobile, also offer PTQ.",
      "raw_html": "<p>Some edge devices only support quantized inference. Therefore, frameworks for on-device inference, such as TensorFlow Lite and PyTorch Mobile, also offer PTQ.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html24\" data-type=\"indexterm\" id=\"id1410\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html23\" data-type=\"indexterm\" id=\"id1411\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html22\" data-type=\"indexterm\" id=\"id1412\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Training quantization",
      "id": "heading-7",
      "raw_html": "<h3>Training quantization</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Quantization during training is not yet as common as PTQ, but it’s gaining traction. There are two distinct goals for training quantization:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"quantization\" data-tertiary=\"training quantization\" data-type=\"indexterm\" id=\"ch07.html25\"></a><a contenteditable=\"false\" data-primary=\"quantization\" data-secondary=\"training quantization\" data-type=\"indexterm\" id=\"ch07.html26\"></a><a contenteditable=\"false\" data-primary=\"training quantization\" data-type=\"indexterm\" id=\"ch07.html27\"></a>Quantization during training is not yet as common as PTQ, but it’s gaining traction. There are two distinct goals for training quantization:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "To produce a model that can perform well in low precision during inference. This is to address the challenge that a model’s quality might degrade during post-training quantization.",
        "To reduce training time and cost. Quantization reduces a model’s memory footprint, allowing a model to be trained on cheaper hardware or allowing the training of a larger model on the same hardware. Quantization also speeds up computation, which further reduces costs."
      ],
      "raw_html": "<ol>\n<li><p>To produce a model that can perform well in low precision during inference. This is to address the challenge that a model’s quality might degrade during post-training quantization.</p></li>\n<li><p>To reduce training time and cost. Quantization reduces a model’s memory footprint, allowing a model to be trained on cheaper hardware or allowing the training of a larger model on the same hardware. Quantization also speeds up computation, which further reduces costs.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "To produce a model that can perform well in low precision during inference. This is to address the challenge that a model’s quality might degrade during post-training quantization.",
      "raw_html": "<p>To produce a model that can perform well in low precision during inference. This is to address the challenge that a model’s quality might degrade during post-training quantization.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "To reduce training time and cost. Quantization reduces a model’s memory footprint, allowing a model to be trained on cheaper hardware or allowing the training of a larger model on the same hardware. Quantization also speeds up computation, which further reduces costs.",
      "raw_html": "<p>To reduce training time and cost. Quantization reduces a model’s memory footprint, allowing a model to be trained on cheaper hardware or allowing the training of a larger model on the same hardware. Quantization also speeds up computation, which further reduces costs.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A quantization technique might help achieve one or both of these goals.",
      "raw_html": "<p>A quantization technique might help achieve one or both of these goals.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Quantization-aware training (QAT) aims to create a model with high quality in low precision for inference. With QAT, the model simulates low-precision (e.g., 8-bit) behavior during training, which allows the model to learn to produce high-quality outputs in low precision. However, QAT doesn’t reduce a model’s training time since its computations are still performed in high precision. QAT can even increase training time due to the extra work of simulating low-precision behavior.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"QAT (quantization-aware training)\" data-type=\"indexterm\" id=\"id1413\"></a><a contenteditable=\"false\" data-primary=\"quantization-aware training (QAT)\" data-type=\"indexterm\" id=\"id1414\"></a>Quantization-aware training (QAT) aims to create a model with high quality in low precision for inference. With QAT, the model simulates low-precision (e.g., 8-bit) behavior during training, which allows the model to learn to produce high-quality outputs in low precision. However, QAT doesn’t reduce a model’s training time since its computations are still performed in high precision. QAT can even increase training time due to the extra work of simulating low-precision behavior.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "On the other hand, training a model directly in lower precision can help with both goals. People attempted to train models in reduced precision as early as 2016; see Hubara et al. (2016) and Jacob et al. (2017). Character.AI (2024) shared that they were able to train their models entirely in INT8, which helped eliminate the training/serving precision mismatch while also significantly improving training efficiency. However, training in lower precision is harder to do, as backpropgation is more sensitive to lower precision.20",
      "raw_html": "<p>On the other hand, training a model directly in lower precision can help with both goals. People attempted to train models in reduced precision as early as 2016; see <a href=\"https://oreil.ly/D-wIG\">Hubara et al. (2016)</a> and <a href=\"https://arxiv.org/abs/1712.05877\">Jacob et al. (2017)</a>. <a href=\"https://oreil.ly/J7kVB\">Character.AI (2024)</a> shared that they were able to train their models entirely in INT8, which helped eliminate the training/serving precision mismatch while also significantly improving training <span class=\"keep-together\">efficiency.</span> However, training in lower precision is harder to do, as backpropgation is more sensitive to lower precision.<sup><a data-type=\"noteref\" href=\"ch07.html#id1415\" id=\"id1415-marker\">20</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Lower-precision training is often done in mixed precision, where a copy of the weights is kept in higher precision but other values, such as gradients and activations, are kept in lower precision.21 You can also have less-sensitive weight values computed in lower precision and more-sensitive weight values computed in higher precision. For example, LLM-QAT (Liu et al., 2023) quantizes weights and activations into 4 bits but keeps embeddings in 16 bits.",
      "raw_html": "<p>Lower-precision training is often done in <a href=\"https://oreil.ly/pBaQM\"><em>mixed precision</em></a>, where a copy of the weights is kept in higher precision but other values, such as <span class=\"keep-together\">gradients</span> and activations, are kept in lower precision.<sup><a data-type=\"noteref\" href=\"ch07.html#id1416\" id=\"id1416-marker\">21</a></sup> You can also have less-sensitive weight values computed in lower precision and more-sensitive weight values computed in higher precision. For example, LLM-QAT (<a href=\"https://arxiv.org/abs/2305.17888\">Liu et al., 2023</a>) quantizes weights and activations into 4 bits but keeps embeddings in 16 bits.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The portions of the model that should be in lower precision can be set automatically using the automatic mixed precision (AMP) functionality offered by many ML frameworks.",
      "raw_html": "<p>The portions of the model that should be in lower precision can be set automatically using the <a href=\"https://oreil.ly/JZRsd\"><em>automatic mixed precision</em></a><a contenteditable=\"false\" data-primary=\"AMP (automatic mixed precision)\" data-type=\"indexterm\" id=\"id1417\"></a><a contenteditable=\"false\" data-primary=\"automatic mixed precision (AMP)\" data-type=\"indexterm\" id=\"id1418\"></a> (AMP) functionality offered by many ML frameworks.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "It’s also possible to have different phases of training in different precision levels. For example, a model can be trained in higher precision but finetuned in lower precision. This is especially common with foundation models, where the team training a model from scratch might be an organization with sufficient compute for higher precision training. Once the model is published, developers with less compute access can finetune that model in lower precision.",
      "raw_html": "<p>It’s also possible to have different phases of training in different precision levels. For example, a model can be trained in higher precision but finetuned in lower precision. This is especially common with foundation models, where the team training a model from scratch might be an organization with sufficient compute for higher precision training. Once the model is published, developers with less compute access can finetune that model in lower <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html27\" data-type=\"indexterm\" id=\"id1419\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html26\" data-type=\"indexterm\" id=\"id1420\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html25\" data-type=\"indexterm\" id=\"id1421\"></a>precision<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html21\" data-type=\"indexterm\" id=\"id1422\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html20\" data-type=\"indexterm\" id=\"id1423\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html19\" data-type=\"indexterm\" id=\"id1424\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html10\" data-type=\"indexterm\" id=\"id1425\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html9\" data-type=\"indexterm\" id=\"id1426\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html8\" data-type=\"indexterm\" id=\"id1427\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Finetuning Techniques",
      "id": "heading-7",
      "raw_html": "<h1>Finetuning Techniques</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "I hope that the previous section has made clear why finetuning large-scale models is so memory-intensive. The more memory finetuning requires, the fewer people who can afford to do it. Techniques that reduce a model’s memory footprint make finetuning more accessible, allowing more people to adapt models to their applications. This section focuses on memory-efficient finetuning techniques, which centers around parameter-efficient finetuning.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"techniques\" data-type=\"indexterm\" id=\"ch07.html28\"></a>I hope that the previous section has made clear why finetuning large-scale models is so memory-intensive. The more memory finetuning requires, the fewer people who can afford to do it. Techniques that reduce a model’s memory footprint make finetuning more accessible, allowing more people to adapt models to their applications. This section focuses on memory-efficient finetuning techniques, which centers around parameter-efficient finetuning.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "I’ll also cover model merging, an exciting but more experimental approach to creating custom models. While model merging is generally not considered finetuning, I include it in this section because it’s complementary to finetuning. Finetuning tailors one model to specific needs. Model merging combines multiple models, often finetuned models, for the same purpose.",
      "raw_html": "<p>I’ll also cover model merging, an exciting but more experimental approach to creating custom models. While model merging is generally not considered finetuning, I include it in this section because it’s complementary to finetuning. Finetuning tailors one model to specific needs. Model merging combines multiple models, often finetuned models, for the same purpose.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While combining multiple models isn’t a new concept, new types of models and finetuning techniques have inspired many creative model-merging techniques, making this section especially fun to write about.",
      "raw_html": "<p>While combining multiple models isn’t a new concept, new types of models and finetuning techniques have inspired many creative model-merging techniques, making this section especially fun to write about.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Parameter-Efficient Finetuning",
      "id": "heading-7",
      "raw_html": "<h2>Parameter-Efficient Finetuning</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "In the early days of finetuning, models were small enough that people could finetune entire models. This approach is called full finetuning. In full finetuning, the number of trainable parameters is exactly the same as the number of parameters.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"techniques\" data-tertiary=\"parameter-efficient finetuning\" data-type=\"indexterm\" id=\"ch07.html29\"></a><a contenteditable=\"false\" data-primary=\"full finetuning\" data-type=\"indexterm\" id=\"ch07.html30\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-type=\"indexterm\" id=\"ch07.html31\"></a>In the early days of finetuning, models were small enough that people could finetune entire models. This approach is called <em>full finetuning</em>. In full finetuning, the number of trainable parameters is exactly the same as the number of parameters. </p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Full finetuning can look similar to training. The main difference is that training starts with randomized model weights, whereas finetuning starts with model weights that have been previously trained.",
      "raw_html": "<p>Full finetuning can look similar to training. The main difference is that training starts with randomized model weights, whereas finetuning starts with model weights that have been previously trained.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "As discussed in “Memory Math”, the more trainable parameters there are, the more memory is needed. Consider a 7B-parameter model:",
      "raw_html": "<p>As discussed in <a data-type=\"xref\" href=\"#ch07b_memory_math_1730159634259402\">“Memory Math”</a>, the more trainable parameters there are, the more memory is needed. Consider a 7B-parameter model:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "If you use a 16-bit format like FP16, loading the model’s weights alone requires 14 GB for memory.",
        "Full finetuning this model with the Adam optimizer, also in a 16-bit format, requires an additional 7B × 3 × 2 bytes = 42 GB of memory.",
        "The total memory needed for the model’s weights, gradients, and optimizer states is then 14 GB + 42 GB = 56 GB."
      ],
      "raw_html": "<ul>\n<li><p>If you use a 16-bit format like FP16, loading the model’s weights alone requires 14 GB for memory.</p></li>\n<li><p>Full finetuning this model with the Adam optimizer, also in a 16-bit format, requires an additional 7B × 3 × 2 bytes = 42 GB of memory.</p></li>\n<li><p>The total memory needed for the model’s weights, gradients, and optimizer states is then 14 GB + 42 GB = 56 GB.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "If you use a 16-bit format like FP16, loading the model’s weights alone requires 14 GB for memory.",
      "raw_html": "<p>If you use a 16-bit format like FP16, loading the model’s weights alone requires 14 GB for memory.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Full finetuning this model with the Adam optimizer, also in a 16-bit format, requires an additional 7B × 3 × 2 bytes = 42 GB of memory.",
      "raw_html": "<p>Full finetuning this model with the Adam optimizer, also in a 16-bit format, requires an additional 7B × 3 × 2 bytes = 42 GB of memory.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The total memory needed for the model’s weights, gradients, and optimizer states is then 14 GB + 42 GB = 56 GB.",
      "raw_html": "<p>The total memory needed for the model’s weights, gradients, and optimizer states is then 14 GB + 42 GB = 56 GB.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "56 GB exceeds the memory capacity of most consumer GPUs, which typically come with 12–24 GB of memory, with higher-end GPUs offering up to 48 GB. And this memory estimation doesn’t yet take into account the memory required for activations.",
      "raw_html": "<p>56 GB exceeds the memory capacity of most consumer GPUs, which typically come with 12–24 GB of memory, with higher-end GPUs offering up to 48 GB. And this memory estimation doesn’t yet take into account the memory required for <span class=\"keep-together\">activations.</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "To fit a model on a given hardware, you can either reduce the model’s memory footprint or find ways to use the hardware’s memory more efficiently. Techniques like quantization and PEFT help minimize the total memory footprint. Techniques that focus on making better use of hardware memory include CPU offloading. Instead of trying to fit the whole model on GPUs, you can offload the excess memory onto CPUs, as demonstrated by DeepSpeed (Rasley et al., 2020).",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p>To fit a model on a given hardware, you can either reduce the model’s memory footprint or find ways to use the hardware’s memory more efficiently. Techniques like quantization and PEFT help minimize the total memory footprint. Techniques that focus on making better use of hardware memory include <em>CPU offloading</em>. Instead of trying to fit the whole model on GPUs, you can offload the excess memory onto CPUs, as demonstrated by DeepSpeed (<a href=\"https://oreil.ly/Np1Hn\">Rasley et al., 2020</a>).</p>\n</div>"
    },
    {
      "type": "paragraph",
      "content": "We also haven’t touched on the fact that full finetuning, especially supervised finetuning and preference finetuning, typically requires a lot of high-quality annotated data that most people can’t afford. Due to the high memory and data requirements of full finetuning, people started doing partial finetuning. In partial finetuning, only some of the model’s parameters are updated. For example, if a model has ten layers, you might freeze the first nine layers and finetune only the last layer,22 reducing the number of trainable parameters to 10% of full finetuning.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"partial finetuning\" data-type=\"indexterm\" id=\"id1428\"></a>We also haven’t touched on the fact that full finetuning, especially supervised finetuning and preference finetuning, typically requires a lot of high-quality annotated data that most people can’t afford. Due to the high memory and data requirements of full finetuning, people started doing <em>partial finetuning</em>. In partial finetuning, only some of the model’s parameters are updated. For example, if a model has ten layers, you might freeze the first nine layers and finetune only the last layer,<sup><a data-type=\"noteref\" href=\"ch07.html#id1429\" id=\"id1429-marker\">22</a></sup> reducing the number of trainable parameters to 10% of full finetuning.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "While partial finetuning can reduce the memory footprint, it’s parameter-inefficient. Partial finetuning requires many trainable parameters to achieve performance close to that of full finetuning. A study by Houlsby et al. (2019) shows that with BERT large (Devlin et al., 2018), you’d need to update approximately 25% of the parameters to achieve performance comparable to that of full finetuning on the GLUE benchmark (Wang et al., 2018). Figure 7-7 shows the performance curve of partial finetuning with different numbers of trainable parameters.",
      "raw_html": "<p>While partial finetuning can reduce the memory footprint, it’s <em>parameter-inefficient</em>. Partial finetuning requires many trainable parameters to achieve performance close to that of full finetuning. A study by <a href=\"https://arxiv.org/abs/1902.00751\">Houlsby et al. (2019)</a> shows that with BERT large (<a href=\"https://arxiv.org/abs/1810.04805\">Devlin et al., 2018</a>), you’d need to update approximately 25% of the parameters to achieve performance comparable to that of full finetuning on the GLUE benchmark (<a href=\"https://arxiv.org/abs/1804.07461\">Wang et al., 2018</a>). <a data-type=\"xref\" href=\"#ch07b_figure_4_1730159634220299\">Figure 7-7</a> shows the performance curve of partial finetuning with different numbers of trainable parameters.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch07b_figure_4_1730159634220299",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_4_1730159634220299\">\n<img alt=\"A graph of a number of objects\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0707.png\"/>\n<h6><span class=\"label\">Figure 7-7. </span>The blue line shows that partial finetuning requires many trainable parameters to achieve a performance comparable to full finetuning. Image from Houlsby et al. (2019).</h6>\n</div></figure>",
      "image": "aien_0707.png",
      "alt": "A graph of a number of objects\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0707.png",
      "caption": {
        "label": "Figure 7-7.",
        "text": "The blue line shows that partial finetuning requires many trainable parameters to achieve a performance comparable to full finetuning. Image from Houlsby et al. (2019)."
      }
    },
    {
      "type": "paragraph",
      "content": "This brings up the question: How to achieve performance close to that of full finetuning while using significantly fewer trainable parameters? Finetuning techniques resulting from this quest are parameter-efficient. There’s no clear threshold that a finetuning method has to pass to be considered parameter-efficient. However, in general, a technique is considered parameter-efficient if it can achieve performance close to that of full finetuning while using several orders of magnitude fewer trainable parameters.",
      "raw_html": "<p>This brings up the question: How to achieve performance close to that of full finetuning while using significantly fewer trainable parameters? Finetuning techniques resulting from this quest are parameter-efficient. There’s no clear threshold that a finetuning method has to pass to be considered parameter-efficient. However, in general, a technique is considered parameter-efficient if it can achieve performance close to that of full finetuning while using several orders of magnitude fewer trainable parameters.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The idea of PEFT (parameter-efficient finetuning) was introduced by Houlsby et al. (2019). The authors showed that by inserting additional parameters into the model in the right places, you can achieve strong finetuning performance using a small number of trainable parameters. They inserted two adapter modules into each transformer block of a BERT model, as shown in Figure 7-8.",
      "raw_html": "<p>The idea of PEFT (parameter-efficient finetuning) was introduced by Houlsby et al. (2019). The authors showed that by inserting additional parameters into the model in the right places, you can achieve strong finetuning performance using a small number of trainable parameters. They inserted two adapter modules into each transformer block of a BERT model, as shown in <a data-type=\"xref\" href=\"#ch07b_figure_5_1730159634220312\">Figure 7-8</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_5_1730159634220312",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_5_1730159634220312\">\n<img alt=\"A diagram of a layer\n\nDescription automatically generated\" src=\"assets/aien_0708.png\"/>\n<h6><span class=\"label\">Figure 7-8. </span>By inserting two adapter modules into each transformer layer for a BERT model and updating only the adapters, Houlsby et al. (2019) were able to achieve strong finetuning performance using a small number of trainable parameters.</h6>\n</div></figure>",
      "image": "aien_0708.png",
      "alt": "A diagram of a layer\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0708.png",
      "caption": {
        "label": "Figure 7-8.",
        "text": "By inserting two adapter modules into each transformer layer for a BERT model and updating only the adapters, Houlsby et al. (2019) were able to achieve strong finetuning performance using a small number of trainable parameters."
      }
    },
    {
      "type": "paragraph",
      "content": "During finetuning, they kept the model’s original parameters unchanged and only updated the adapters. The number of trainable parameters is the number of parameters in the adapters. On the GLUE benchmark, they achieved a performance within 0.4% of full finetuning using only 3% of the number of trainable parameters. The orange line in Figure 7-7 shows the performance delta between full finetuning and finetuning using different adapter sizes.",
      "raw_html": "<p>During finetuning, they kept the model’s original parameters unchanged and only updated the adapters. The number of trainable parameters is the number of parameters in the adapters. On the GLUE benchmark, they achieved a performance within 0.4% of full finetuning using only 3% of the number of trainable parameters. The orange line in <a data-type=\"xref\" href=\"#ch07b_figure_4_1730159634220299\">Figure 7-7</a> shows the performance delta between full finetuning and finetuning using different adapter sizes.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "However, the downside of this approach is that it increases the inference latency of the finetuned model. The adapters introduce additional layers, which add more computational steps to the forward pass, slowing inference.",
      "raw_html": "<p>However, the downside of this approach is that it increases the inference latency of the finetuned model. The adapters introduce additional layers, which add more computational steps to the forward pass, slowing inference.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "PEFT enables finetuning on more affordable hardware, making it accessible to many more developers. PEFT methods are generally not only parameter-efficient but also sample-efficient. While full finetuning may need tens of thousands to millions of examples to achieve notable quality improvements, some PEFT methods can deliver strong performance with just a few thousand examples.",
      "raw_html": "<p>PEFT enables finetuning on more affordable hardware, making it accessible to many more developers. PEFT methods are generally not only parameter-efficient but also sample-efficient. While full finetuning may need tens of thousands to millions of examples to achieve notable quality improvements, some PEFT methods can deliver strong performance with just a few thousand examples.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Given PEFT’s obvious appeal, PEFT techniques are being rapidly developed. The next section will give an overview of these techniques before diving deeper into the most common PEFT technique: LoRA.",
      "raw_html": "<p>Given PEFT’s obvious appeal, PEFT techniques are being rapidly developed. The next section will give an overview of these techniques before diving deeper into the most common PEFT technique: LoRA.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "PEFT techniques",
      "id": "heading-7",
      "raw_html": "<h3>PEFT techniques</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "The existing prolific world of PEFT generally falls into two buckets: adapter-based methods and soft prompt-based methods. However, it’s likely that newer buckets will be introduced in the future.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"adapters\" data-secondary=\"PEFT techniques\" data-type=\"indexterm\" id=\"ch07.html32a\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"techniques\" data-tertiary=\"PEFT techniques\" data-type=\"indexterm\" id=\"ch07.html32\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-secondary=\"adapter-based/soft-prompt techniques\" data-type=\"indexterm\" id=\"ch07.html33\"></a>The existing prolific world of PEFT generally falls into two buckets: <em>adapter-based methods</em> and <em>soft prompt-based methods</em>. However, it’s likely that newer buckets will be introduced in the future.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Adapter-based methods refer to all methods that involve additional modules to the model weights, such as the one developed by Houlsby et al. (2019). Because adapter-based methods involve adding parameters, they are also called additive methods.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"adapter-based methods\" data-type=\"indexterm\" id=\"id1430\"></a><em>Adapter-based methods</em> refer to all methods that involve additional modules to the model weights, such as the one developed by <a href=\"https://arxiv.org/abs/1902.00751\">Houlsby et al. (2019)</a>. Because adapter-based methods involve adding parameters, they are also called <em>additive methods</em>. </p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "As of this writing, LoRA (Hu et al., 2021) is by far the most popular adapter-based method, and it will be the topic of the following section. Other adapter-based methods include BitFit (Zaken et al., 2021), which came out around the same time LoRA did. Newer adapter methods include IA3 (Liu et al., 2022), whose efficient mixed-task batching strategy makes it particularly attractive for multi-task finetuning. It’s been shown to outperform LoRA and even full finetuning in some cases. LongLoRA (Chen et al., 2023) is a LoRA variant that incorporates attention-modification techniques to expand context length.",
      "raw_html": "<p>As of this writing, LoRA (<a href=\"https://arxiv.org/abs/2106.09685\">Hu et al., 2021</a>) is by far the most popular adapter-based method, and it will be the topic of the following section. Other adapter-based methods include BitFit (<a href=\"https://arxiv.org/abs/2106.10199\">Zaken et al., 2021</a>), which came out around the same time LoRA did. Newer adapter methods include IA3 (<a href=\"https://oreil.ly/avDPk\">Liu et al., 2022</a>), whose efficient mixed-task batching strategy makes it particularly attractive for multi-task finetuning. It’s been shown to outperform LoRA and even full finetuning in some cases. LongLoRA (<a href=\"https://arxiv.org/abs/2309.12307\">Chen et al., 2023</a>) is a LoRA variant that incorporates attention-modification techniques to expand context length.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If adapter-based methods add trainable parameters to the model’s architecture, soft prompt-based methods modify how the model processes the input by introducing special trainable tokens. These additional tokens are fed into the model alongside the input tokens. They are called soft prompts because, like the inputs (hard prompts), soft prompts also guide the model’s behaviors. However, soft prompts differ from hard prompts in two ways:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"soft prompt-based PEFT methods\" data-type=\"indexterm\" id=\"ch07.html34\"></a>If adapter-based methods add trainable parameters to the model’s architecture, soft prompt-based methods modify how the model processes the input by introducing special trainable tokens. These additional tokens are fed into the model alongside the input tokens. They are called <em>soft prompts</em> because, like the inputs (hard prompts), soft prompts also guide the model’s behaviors. However, soft prompts differ from hard prompts in two ways:</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Hard prompts are human-readable. They typically contain discrete tokens such as “I”, “write”, “a”, and “lot”. In contrast, soft prompts are continuous vectors, resembling embedding vectors, and are not human-readable.",
        "Hard prompts are static and not trainable, whereas soft prompts can be optimized through backpropagation during the tuning process, allowing them to be adjusted for specific tasks."
      ],
      "raw_html": "<ul>\n<li><p>Hard prompts are human-readable. They typically contain <em>discrete</em> tokens such as “I”, “write”, “a”, and “lot”. In contrast, soft prompts are continuous vectors, resembling embedding vectors, and are not human-readable.</p></li>\n<li><p>Hard prompts are static and not trainable, whereas soft prompts can be optimized through backpropagation during the tuning process, allowing them to be adjusted for specific tasks.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Hard prompts are human-readable. They typically contain discrete tokens such as “I”, “write”, “a”, and “lot”. In contrast, soft prompts are continuous vectors, resembling embedding vectors, and are not human-readable.",
      "raw_html": "<p>Hard prompts are human-readable. They typically contain <em>discrete</em> tokens such as “I”, “write”, “a”, and “lot”. In contrast, soft prompts are continuous vectors, resembling embedding vectors, and are not human-readable.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Hard prompts are static and not trainable, whereas soft prompts can be optimized through backpropagation during the tuning process, allowing them to be adjusted for specific tasks.",
      "raw_html": "<p>Hard prompts are static and not trainable, whereas soft prompts can be optimized through backpropagation during the tuning process, allowing them to be adjusted for specific tasks.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Some people describe soft prompting as a crossover between prompt engineering and finetuning. Figure 7-9 visualizes how you can use soft prompts together with hard prompts to guide a model’s behaviors.",
      "raw_html": "<p class=\"pagebreak-before\">Some people describe soft prompting as a crossover between prompt engineering and finetuning. <a data-type=\"xref\" href=\"#ch07b_figure_6_1730159634220324\">Figure 7-9</a> visualizes how you can use soft prompts together with hard prompts to guide a model’s behaviors.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_6_1730159634220324",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_6_1730159634220324\">\n<img alt=\"A diagram of a model\n\nDescription automatically generated\" src=\"assets/aien_0709.png\"/>\n<h6><span class=\"label\">Figure 7-9. </span>Hard prompts and soft prompts can be combined to change a model’s behaviors.</h6>\n</div></figure>",
      "image": "aien_0709.png",
      "alt": "A diagram of a model\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0709.png",
      "caption": {
        "label": "Figure 7-9.",
        "text": "Hard prompts and soft prompts can be combined to change a model’s behaviors."
      }
    },
    {
      "type": "paragraph",
      "content": "Soft prompt tuning as a subfield is characterized by a series of similar-sounding techniques that can be confusing, such as prefix-tuning (Li and Liang, 2021), P-Tuning (Liu et al., 2021), and prompt tuning (Lester et al., 2021).23 They differ mainly on the locations where the soft prompts are inserted. For example, prefix tuning prepends soft prompt tokens to the input at every transformer layer, whereas prompt tuning prepends soft prompt tokens to only the embedded input. If you want to use any of them, many PEFT frameworks will implement them out of the box for you.",
      "raw_html": "<p>Soft prompt tuning as a subfield is characterized by a series of similar-sounding techniques that can be confusing, such as prefix-tuning (<a href=\"https://arxiv.org/abs/2101.00190\">Li and Liang, 2021</a>), P-Tuning (<a href=\"https://arxiv.org/abs/2103.10385\">Liu et al., 2021</a>), and prompt tuning (<a href=\"https://arxiv.org/abs/2104.08691\">Lester et al., 2021</a>).<sup><a data-type=\"noteref\" href=\"ch07.html#id1431\" id=\"id1431-marker\">23</a></sup> They differ mainly on the locations where the soft prompts are inserted. For example, prefix tuning prepends soft prompt tokens to the input at every transformer layer, whereas prompt tuning prepends soft prompt tokens to only the embedded input. If you want to use any of them, many PEFT frameworks will implement them out of the box for you.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "To get a sense of what PEFT methods are being used, I analyzed over 1,000 open issues on the GitHub repository huggingface/peft in October 2024. The assumption is that if someone uses a technique, they are more likely to report issues or ask questions about it. Figure 7-10 shows the result. For “P-Tuning”, I searched for keywords “p_tuning” and “p tuning” to account for different spellings.",
      "raw_html": "<p>To get a sense of what PEFT methods are being used, I analyzed over 1,000 open issues on the <a href=\"https://github.com/huggingface/peft\">GitHub repository huggingface/peft</a> in October 2024. The assumption is that if someone uses a technique, they are more likely to report issues or ask questions about it. <a data-type=\"xref\" href=\"#ch07b_figure_7_1730159634220334\">Figure 7-10</a> shows the result. For “P-Tuning”, I searched for keywords “p_tuning” and “p tuning” to account for different spellings.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_7_1730159634220334",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_7_1730159634220334\">\n<img alt=\"A graph of a graph with different colored bars\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0710.png\"/>\n<h6><span class=\"label\">Figure 7-10. </span>The number of issues corresponding to different finetuning techniques from the GitHub repository huggingface/peft. This is a proxy to estimate the popularity of each technique.</h6>\n</div></figure>",
      "image": "aien_0710.png",
      "alt": "A graph of a graph with different colored bars\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0710.png",
      "caption": {
        "label": "Figure 7-10.",
        "text": "The number of issues corresponding to different finetuning techniques from the GitHub repository huggingface/peft. This is a proxy to estimate the popularity of each technique."
      }
    },
    {
      "type": "paragraph",
      "content": "From this analysis, it’s clear that LoRA dominates. Soft prompts are less common, but there seems to be growing interest from those who want more customization than what is afforded by prompt engineering but who don’t want to invest in finetuning.",
      "raw_html": "<p>From this analysis, it’s clear that LoRA dominates. Soft prompts are less common, but there seems to be growing interest from those who want more customization than what is afforded by prompt engineering but who don’t want to invest in <span class=\"keep-together\">finetuning.</span><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html34\" data-type=\"indexterm\" id=\"id1432\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because of LoRA’s popularity, the next section focuses on how LoRA works and how it solves the challenge posed by early adapter-based methods. Even if you don’t use LoRA, this deep dive should provide a framework for you to explore other finetuning methods.",
      "raw_html": "<p>Because of LoRA’s popularity, the next section focuses on how LoRA works and how it solves the challenge posed by early adapter-based methods. Even if you don’t use LoRA, this deep dive should provide a framework for you to explore other finetuning methods.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html33\" data-type=\"indexterm\" id=\"id1433\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html32a\" data-type=\"indexterm\" id=\"id1434\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html32\" data-type=\"indexterm\" id=\"id1435\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "LoRA",
      "id": "heading-7",
      "raw_html": "<h3>LoRA</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Unlike the original adapter method by Houlsby et al. (2019), LoRA (Low-Rank Adaptation) (Hu et al., 2021) incorporates additional parameters in a way that doesn’t incur extra inference latency. Instead of introducing additional layers to the base model, LoRA uses modules that can be merged back to the original layers.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"adapters\" data-secondary=\"LoRA\" data-type=\"indexterm\" id=\"ch07.html35a\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"techniques\" data-tertiary=\"LoRA\" data-type=\"indexterm\" id=\"ch07.html35\"></a><a contenteditable=\"false\" data-primary=\"LoRA (low-rank adaptation)\" data-type=\"indexterm\" id=\"ch07.html36\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-secondary=\"LoRA\" data-type=\"indexterm\" id=\"ch07.html37\"></a>Unlike the original adapter method by <a href=\"https://arxiv.org/abs/1902.00751\">Houlsby et al. (2019)</a>, LoRA (Low-Rank Adaptation) (<a href=\"https://arxiv.org/abs/2106.09685\">Hu et al., 2021</a>) incorporates additional parameters in a way that doesn’t incur extra inference latency. Instead of introducing additional layers to the base model, LoRA uses modules that can be merged back to the original layers. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "You can apply LoRA to individual weight matrices. Given a weight matrix, LoRA decomposes this matrix into the product of two smaller matrices, then updates these two smaller matrices before merging them back to the original matrix.",
      "raw_html": "<p class=\"pagebreak-before\">You can apply LoRA to individual weight matrices. Given a weight matrix, LoRA decomposes this matrix into the product of two smaller matrices, then updates these two smaller matrices before merging them back to the original matrix.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Consider the weight matrix W of the dimension n × m. LoRA works as follows:",
      "raw_html": "<p>Consider the weight matrix <em>W</em> of the dimension <em>n</em> × <em>m</em>. LoRA works as follows:</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "First, choose the dimension of the smaller matrices. Let r be the chosen value. Construct two matrices: A (dimension n × r) and B (dimension r × m). Their product is WAB, which is of the same dimension as W. r is the LoRA rank.",
        "Add WAB to the original weight matrix W to create a new weight matrix Wʹ. Use Wʹ in place of W as part of the model. You can use a hyperparameter ɑ to determine how much WAB should contribute to the new matrix: \n\nW\nâ\n\n\n=\nW\n+\nα r\nW AB",
        "During finetuning, update only the parameters in A and B. W is kept intact."
      ],
      "raw_html": "<ol>\n<li><p>First, choose the dimension of the smaller matrices. Let <em>r</em> be the chosen value. Construct two matrices: <em>A</em> (dimension <em>n</em> × <em>r</em>) and <em>B</em> (dimension <em>r</em> × <em>m</em>). Their product is <em>W</em><sub><em>AB</em></sub>, which is of the same dimension as <em>W</em>. <em>r</em> is the LoRA <em>rank</em>.</p></li>\n<li><p>Add <em>W</em><sub><em>AB</em></sub> to the original weight matrix <em>W</em> to create a new weight matrix <em>W</em>ʹ. Use <em>W</em>ʹ in place of <em>W</em> as part of the model. You can use a hyperparameter ɑ to determine how much <em>W</em><sub><em>AB</em></sub> should contribute to the new matrix: <math alttext=\"upper W prime equals upper W plus StartFraction alpha Over r EndFraction upper W Subscript upper A upper B\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mrow>\n<mi>W</mi>\n<mi>â</mi>\n<mi></mi>\n<mi></mi>\n<mo>=</mo>\n<mi>W</mi>\n<mo>+</mo>\n<mfrac><mi>α</mi> <mi>r</mi></mfrac>\n<msub><mi>W</mi> <mrow><mi>A</mi><mi>B</mi></mrow> </msub>\n</mrow>\n</math></p></li>\n<li><p>During finetuning, update only the parameters in <em>A</em> and <em>B</em>. <em>W</em> is kept intact.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "First, choose the dimension of the smaller matrices. Let r be the chosen value. Construct two matrices: A (dimension n × r) and B (dimension r × m). Their product is WAB, which is of the same dimension as W. r is the LoRA rank.",
      "raw_html": "<p>First, choose the dimension of the smaller matrices. Let <em>r</em> be the chosen value. Construct two matrices: <em>A</em> (dimension <em>n</em> × <em>r</em>) and <em>B</em> (dimension <em>r</em> × <em>m</em>). Their product is <em>W</em><sub><em>AB</em></sub>, which is of the same dimension as <em>W</em>. <em>r</em> is the LoRA <em>rank</em>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Add WAB to the original weight matrix W to create a new weight matrix Wʹ. Use Wʹ in place of W as part of the model. You can use a hyperparameter ɑ to determine how much WAB should contribute to the new matrix: \n\nW\nâ\n\n\n=\nW\n+\nα r\nW AB",
      "raw_html": "<p>Add <em>W</em><sub><em>AB</em></sub> to the original weight matrix <em>W</em> to create a new weight matrix <em>W</em>ʹ. Use <em>W</em>ʹ in place of <em>W</em> as part of the model. You can use a hyperparameter ɑ to determine how much <em>W</em><sub><em>AB</em></sub> should contribute to the new matrix: <math alttext=\"upper W prime equals upper W plus StartFraction alpha Over r EndFraction upper W Subscript upper A upper B\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mrow>\n<mi>W</mi>\n<mi>â</mi>\n<mi></mi>\n<mi></mi>\n<mo>=</mo>\n<mi>W</mi>\n<mo>+</mo>\n<mfrac><mi>α</mi> <mi>r</mi></mfrac>\n<msub><mi>W</mi> <mrow><mi>A</mi><mi>B</mi></mrow> </msub>\n</mrow>\n</math></p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "During finetuning, update only the parameters in A and B. W is kept intact.",
      "raw_html": "<p>During finetuning, update only the parameters in <em>A</em> and <em>B</em>. <em>W</em> is kept intact.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Figure 7-11 visualizes this process.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch07b_figure_8_1730159634220345\">Figure 7-11</a> visualizes this process. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_8_1730159634220345",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_8_1730159634220345\">\n<img alt=\"A diagram of a diagram\n\nDescription automatically generated\" src=\"assets/aien_0711.png\"/>\n<h6><span class=\"label\">Figure 7-11. </span>To apply LoRA to a weight matrix W, decompose it into the product of two matrices A and B. During finetuning, only A and B are updated. W is kept intact.</h6>\n</div></figure>",
      "image": "aien_0711.png",
      "alt": "A diagram of a diagram\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0711.png",
      "caption": {
        "label": "Figure 7-11.",
        "text": "To apply LoRA to a weight matrix W, decompose it into the product of two matrices A and B. During finetuning, only A and B are updated. W is kept intact."
      }
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "LoRA (Low-Rank Adaptation) is built on the concept of low-rank factorization, a long-standing dimensionality reduction technique. The key idea is that you can factorize a large matrix into a product of two smaller matrices to reduce the number of parameters, which, in turn, reduces both the computation and memory requirements. For example, a 9 × 9 matrix can be factorized into the product of two matrices of dimensions 9 × 1 and 1 × 9. The original matrix has 81 parameters, but the two product matrices have only 18 parameters combined.\n\nThe number of columns in the first factorized matrix and the number of columns in the second factorized matrix correspond to the rank of the factorization. The original matrix is full-rank, while the two smaller matrices represent a low-rank approximation.\n\nWhile factorization can significantly reduce the number of parameters, it’s lossy because it only approximates the original matrix. The higher the rank, the more information from the original matrix the factorization can preserve.",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p><a contenteditable=\"false\" data-primary=\"low-rank factorization\" data-type=\"indexterm\" id=\"id1436\"></a>LoRA (Low-Rank Adaptation) is built on the concept of <em>low-rank factorization</em>, a long-standing dimensionality reduction technique. The key idea is that you can factorize a large matrix into a product of two smaller matrices to reduce the number of parameters, which, in turn, reduces both the computation and memory requirements. For example, a <code>9 × 9</code> matrix can be factorized into the product of two matrices of dimensions <code>9 × 1</code> and <code>1 × 9</code>. The original matrix has 81 parameters, but the two product matrices have only 18 parameters combined.</p>\n<p>The number of columns in the first factorized matrix and the number of columns in the second factorized matrix correspond to the rank of the factorization. The original matrix is <em>full-rank</em>, while the two smaller matrices represent a low-rank approximation.</p>\n<p>While factorization can significantly reduce the number of parameters, it’s lossy because it only approximates the original matrix. The higher the rank, the more information from the original matrix the factorization can preserve.</p>\n</div>"
    },
    {
      "type": "paragraph",
      "content": "Like the original adapter method, LoRA is parameter-efficient and sample-efficient. The factorization enables LoRA to use even fewer trainable parameters. The LoRA paper showed that, for GPT-3, LoRA achieves comparable or better performance with full finetuning on several tasks while using only ~4.7M trainable parameters, 0.0027% of full finetuning.",
      "raw_html": "<p>Like the original adapter method, LoRA is parameter-efficient and sample-efficient. The factorization enables LoRA to use even fewer trainable parameters. The LoRA paper showed that, for GPT-3, LoRA achieves comparable or better performance with full finetuning on several tasks while using only ~4.7M trainable parameters, 0.0027% of full finetuning.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Why does LoRA work?",
      "id": "heading-7",
      "raw_html": "<h4>Why does LoRA work?</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Parameter-efficient methods like LoRA have become so popular that many people take them for granted. But why is parameter efficiency possible at all? If a model requires a lot of parameters to learn certain behaviors during pre-training, shouldn’t it also require a lot of parameters to change its behaviors during finetuning?",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"LoRA (low-rank adaptation)\" data-secondary=\"mechanism of operation\" data-type=\"indexterm\" id=\"id1437\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-secondary=\"LoRA\" data-tertiary=\"how it works\" data-type=\"indexterm\" id=\"id1438\"></a>Parameter-efficient methods like LoRA have become so popular that many people take them for granted. <em>But why is parameter efficiency possible at all?</em> If a model requires a lot of parameters to learn certain behaviors during pre-training, shouldn’t it also require a lot of parameters to change its behaviors during finetuning?</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The same question can be raised for data. If a model requires a lot of data to learn a behavior, shouldn’t it also require a lot of data to meaningfully change this behavior? How is it possible that you need millions or billions of examples to pre-train a model, but only a few hundreds or thousands of examples to finetune it?",
      "raw_html": "<p>The same question can be raised for data. If a model requires a lot of data to learn a behavior, shouldn’t it also require a lot of data to meaningfully change this behavior? How is it possible that you need millions or billions of examples to pre-train a model, but only a few hundreds or thousands of examples to finetune it?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Many papers have argued that while LLMs have many parameters, they have very low intrinsic dimensions; see Li et al. (2018); Aghajanyan et al. (2020); and Hu et al. (2021). They showed that pre-training implicitly minimizes the model’s intrinsic dimension. Surprisingly, larger models tend to have lower intrinsic dimensions after pre-training. This suggests that pre-training acts as a compression framework for downstream tasks. In other words, the better trained an LLM is, the easier it is to finetune the model using a small number of trainable parameters and a small amount of data.",
      "raw_html": "<p>Many papers have argued that while LLMs have many parameters, they have very low intrinsic dimensions; see <a href=\"https://arxiv.org/abs/1804.08838\">Li et al. (2018)</a>; <a href=\"https://arxiv.org/abs/2012.13255\">Aghajanyan et al. (2020)</a>; and <a href=\"https://arxiv.org/abs/2106.09685\">Hu et al. (2021)</a>. They showed that <em>pre-training implicitly minimizes the model’s intrinsic dimension</em>. Surprisingly, larger models tend to have lower intrinsic dimensions after pre-training. This suggests that pre-training acts as a compression framework for downstream tasks. In other words, the better trained an LLM is, the easier it is to finetune the model using a small number of trainable parameters and a small amount of data.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "You might wonder, if low-rank factorization works so well, why don’t we use LoRA for pre-training as well? Instead of pre-training a large model and applying low-rank factorization only during finetuning, could we factorize a model from the start for pre-training? Low-rank pre-training can significantly reduce the model’s number of parameters, significantly reducing the model’s pre-training time and cost.",
      "raw_html": "<p>You might wonder, if low-rank factorization works so well, <em>why don’t we use LoRA for pre-training as well?</em> Instead of pre-training a large model and applying low-rank factorization only during finetuning, could we factorize a model from the start for pre-training? Low-rank pre-training can significantly reduce the model’s number of parameters, significantly reducing the model’s pre-training time and cost.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Throughout the 2010s, many people tried training low-rank neural networks, exemplified in studies such as “Low-Rank Matrix Factorization for Deep Neural Network Training with High-Dimensional Output Targets” (Sainath et al., 2013), “Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks” (Povey et al., 2018), and “Speeding up Convolutional Neural Networks with Low Rank Expansions” (Jaderberg et al., 2014).",
      "raw_html": "<p>Throughout the 2010s, many people tried training low-rank neural networks, exemplified in studies such as “Low-Rank Matrix Factorization for Deep Neural Network Training with High-Dimensional Output Targets” (<a href=\"https://oreil.ly/xzdiG\">Sainath et al., 2013</a>), “Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks” (<a href=\"https://oreil.ly/LHLNz\">Povey et al., 2018</a>), and “Speeding up Convolutional Neural Networks with Low Rank Expansions” (<a href=\"https://oreil.ly/BR63I\">Jaderberg et al., 2014</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Low-rank factorization has proven to be effective at smaller scales. For example, by applying various factorization strategies, including replacing 3 × 3 convolution with 1 × 1 convolution, SqueezeNet (Iandola et al., 2016) achieves AlexNet-level accuracy on ImageNet using 50 times fewer parameters.",
      "raw_html": "<p>Low-rank factorization has proven to be effective at smaller scales. For example, by applying various factorization strategies, including replacing 3 × 3 convolution with 1 × 1 convolution, SqueezeNet (<a href=\"https://arxiv.org/abs/1602.07360\">Iandola et al., 2016</a>) achieves AlexNet-level accuracy on ImageNet using 50 times fewer parameters.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "More recent attempts to train low-rank LLMs include ReLoRA (Lialin et al., 2023) and GaLore (Zhao et al., 2024). ReLoRA works for transformer-based models of up to 1.3B parameters. GaLore achieves performance comparable to that of a full-rank model at 1B parameters and promising performance at 7B parameters.",
      "raw_html": "<p>More recent attempts to train low-rank LLMs include ReLoRA (<a href=\"https://arxiv.org/abs/2307.05695\">Lialin et al., 2023</a>) and GaLore (<a href=\"https://arxiv.org/abs/2403.03507\">Zhao et al., 2024</a>). ReLoRA works for transformer-based models of up to 1.3B parameters. GaLore achieves performance comparable to that of a full-rank model at 1B parameters and promising performance at 7B parameters.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "It’s possible that one day not too far in the future, researchers will develop a way to scale up low-rank pre-training to hundreds of billions of parameters. However, if Aghajanyan et al.’s argument is correct—that pre-training implicitly compresses a model’s intrinsic dimension—full-rank pre-training is still necessary to sufficiently reduce the model’s intrinsic dimension to a point where low-rank factorization can work. It would be interesting to study exactly how much full-rank training is necessary before it’s possible to switch to low-rank training.",
      "raw_html": "<p>It’s possible that one day not too far in the future, researchers will develop a way to scale up low-rank pre-training to hundreds of billions of parameters. However, if <a href=\"https://arxiv.org/abs/2012.13255\">Aghajanyan et al.’s argument</a> is correct—that pre-training implicitly compresses a model’s intrinsic dimension—full-rank pre-training is still necessary to sufficiently reduce the model’s intrinsic dimension to a point where low-rank factorization can work. It would be interesting to study exactly how much full-rank training is necessary before it’s possible to switch to low-rank <span class=\"keep-together\">training.</span></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "LoRA configurations",
      "id": "heading-7",
      "raw_html": "<h4>LoRA configurations</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "To apply LoRA, you need to decide what weight matrices to apply LoRA to and the rank of each factorization. This section will discuss the considerations for each of these decisions.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"LoRA (low-rank adaptation)\" data-secondary=\"configurations\" data-type=\"indexterm\" id=\"ch07.html38\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-secondary=\"LoRA\" data-tertiary=\"configurations\" data-type=\"indexterm\" id=\"ch07.html39\"></a>To apply LoRA, you need to decide what weight matrices to apply LoRA to and the rank of each factorization. This section will discuss the considerations for each of these decisions.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "LoRA can be applied to each individual weight matrix. The efficiency of LoRA, therefore, depends not only on what matrices LoRA is applied to but also on the model’s architecture, as different architectures have different weight matrices.",
      "raw_html": "<p>LoRA can be applied to each individual weight matrix. The efficiency of LoRA, therefore, depends not only on what matrices LoRA is applied to but also on the model’s architecture, as different architectures have different weight matrices.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While there have been examples of LoRA with other architectures, such as convolutional neural networks (Dutt et al., 2023; Zhong et al., 2024; Aleem et al., 2024), LoRA has been primarily used for transformer models.24 LoRA is most commonly applied to the four weight matrices in the attention modules: the query (Wq), key (Wk), value (Wv), and output projection (Wo) matrices.",
      "raw_html": "<p class=\"pagebreak-before\">While there have been examples of LoRA with other architectures, such as convolutional neural networks (<a href=\"https://arxiv.org/abs/2305.08252\">Dutt et al., 2023</a>; <a href=\"https://arxiv.org/abs/2401.17868\">Zhong et al., 2024</a>; <a href=\"https://arxiv.org/abs/2402.04964\">Aleem et al., 2024</a>), LoRA has been primarily used for transformer models.<sup><a data-type=\"noteref\" href=\"ch07.html#id1439\" id=\"id1439-marker\">24</a></sup> LoRA is most commonly applied to the four weight matrices in the attention modules: the query (<em>W</em><sub><em>q</em></sub>), key (<em>W</em><sub><em>k</em></sub>), value (<em>W</em><sub><em>v</em></sub>), and output projection (<em>W</em><sub><em>o</em></sub>) matrices.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Typically, LoRA is applied uniformly to all matrices of the same type within a model. For example, applying LoRA to the query matrix means applying LoRA to all query matrices in the model.",
      "raw_html": "<p>Typically, LoRA is applied uniformly to all matrices of the same type within a model. For example, applying LoRA to the query matrix means applying LoRA to all query matrices in the model.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Naively, you can apply LoRA to all these attention matrices. However, often, you’re constrained by your hardware’s memory and can accommodate only a fixed number of trainable parameters. Given a fixed budget of trainable parameters, what matrices should you apply LoRA to, to maximize performance?",
      "raw_html": "<p>Naively, you can apply LoRA to all these attention matrices. However, often, you’re constrained by your hardware’s memory and can accommodate only a fixed number of trainable parameters. Given a fixed budget of trainable parameters, what matrices should you apply LoRA to, to maximize performance?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "When finetuning GPT-3 175B, Hu et al. (2021) set their trainable parameter budget at 18M, which is 0.01% of the model’s total number of parameters. This budget allows them to apply LoRA to the following:",
      "raw_html": "<p>When finetuning GPT-3 175B, Hu et al. (2021) set their trainable parameter budget at 18M, which is 0.01% of the model’s total number of parameters. This budget allows them to apply LoRA to the following:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "One matrix with the rank of 8",
        "Two matrices with the rank of 4",
        "All four matrices with the rank of 2"
      ],
      "raw_html": "<ol>\n<li>\n<p>One matrix with the rank of 8</p>\n</li>\n<li>\n<p>Two matrices with the rank of 4</p>\n</li>\n<li>\n<p>All four matrices with the rank of 2</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "One matrix with the rank of 8",
      "raw_html": "<p>One matrix with the rank of 8</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Two matrices with the rank of 4",
      "raw_html": "<p>Two matrices with the rank of 4</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "All four matrices with the rank of 2",
      "raw_html": "<p>All four matrices with the rank of 2</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "GPT-3 175B has 96 transformer layers with a model dimension of 12,288. Applying LoRA with rank = 2 to all four matrices would yield (12,288 × 2 × 2) × 4 = 196,608 trainable parameters per layer, or 18,874,368 trainable parameters for the whole model.",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p>GPT-3 175B has 96 transformer layers with a model dimension of 12,288. Applying LoRA with rank = 2 to all four matrices would yield (12,288 × 2 × 2) × 4 = 196,608 trainable parameters per layer, or 18,874,368 trainable parameters for the whole model.</p>\n</div>"
    },
    {
      "type": "paragraph",
      "content": "They found that applying LoRA to all four matrices with rank = 2 yields the best performance on the WikiSQL (Zhong et al., 2017) and MultiNLI (Multi-Genre Natural Language Inference) benchmarks (Williams et al., 2017). Table 7-5 shows their results. However, the authors suggested that if you can choose only two attention matrices, the query and value matrices generally yield the best results.",
      "raw_html": "<p>They found that applying LoRA to all four matrices with rank = 2 yields the best performance on the WikiSQL (<a href=\"https://arxiv.org/abs/1709.00103\">Zhong et al., 2017</a>) and MultiNLI (Multi-Genre Natural Language Inference) benchmarks (<a href=\"https://oreil.ly/mqHMU\">Williams et al., 2017</a>). <a data-type=\"xref\" href=\"#ch07b_table_3_1730159634233616\">Table 7-5</a> shows their results. However, the authors suggested that if you can choose only two attention matrices, the query and value matrices generally yield the best results.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch07b_table_3_1730159634233616",
      "raw_html": "<table id=\"ch07b_table_3_1730159634233616\">\n<caption><span class=\"label\">Table 7-5. </span>LoRA performance with the budget of 18M trainable parameters. Results from LoRA (Hu et al., 2021).</caption>\n<thead>\n<tr>\n<th> </th>\n<th class=\"center\" colspan=\"7\">Number of trainable parameters = 18M</th>\n</tr>\n</thead>\n<tr>\n<td>Weight type</td>\n<td>W<sub>q</sub></td>\n<td>W<sub>k</sub></td>\n<td>W<sub>v</sub></td>\n<td>W<sub>o</sub></td>\n<td>W<sub>q</sub>, W<sub>k</sub></td>\n<td>W<sub>q</sub>, W<sub>v</sub></td>\n<td>W<sub>q</sub>, W<sub>k</sub>, W<sub>v</sub>, W<sub>o</sub></td>\n</tr>\n<tr>\n<td>Rank r</td>\n<td>8</td>\n<td>8</td>\n<td>8</td>\n<td>8</td>\n<td>4</td>\n<td>4</td>\n<td>2</td>\n</tr>\n<tr>\n<td>WikiSQL (± 0.5%)</td>\n<td>70.4</td>\n<td>70.0</td>\n<td>73.0</td>\n<td>73.2</td>\n<td>71.4</td>\n<td><strong>73.7</strong></td>\n<td><strong>73.7</strong></td>\n</tr>\n<tr>\n<td>MultiNLI (± 0.1%)</td>\n<td>91.0</td>\n<td>90.8</td>\n<td>91.0</td>\n<td>91.3</td>\n<td>91.3</td>\n<td>91.3</td>\n<td><strong>91.7</strong></td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 7-5.",
        "text": "LoRA performance with the budget of 18M trainable parameters. Results from LoRA (Hu et al., 2021)."
      },
      "headers": [
        "",
        "Number of trainable parameters = 18M"
      ],
      "rows": [
        [
          "Weight type",
          "Wq",
          "Wk",
          "Wv",
          "Wo",
          "Wq, Wk",
          "Wq, Wv",
          "Wq, Wk, Wv, Wo"
        ],
        [
          "Rank r",
          "8",
          "8",
          "8",
          "8",
          "4",
          "4",
          "2"
        ],
        [
          "WikiSQL (± 0.5%)",
          "70.4",
          "70.0",
          "73.0",
          "73.2",
          "71.4",
          "73.7",
          "73.7"
        ],
        [
          "MultiNLI (± 0.1%)",
          "91.0",
          "90.8",
          "91.0",
          "91.3",
          "91.3",
          "91.3",
          "91.7"
        ]
      ],
      "row_count": 4,
      "column_count": 2
    },
    {
      "type": "paragraph",
      "content": "Empirical observations suggest that applying LoRA to more weight matrices, including the feedforward matrices, yields better results. For example, Databricks showed that the biggest performance boost they got was from applying LoRA to all feedforward layers (Sooriyarachchi, 2023). Fomenko et al. (2024) noted that feedforward-based LoRA can be complementary to attention-based LoRA, though attention-based LoRA typically offers greater efficacy within memory constraints.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"feedforward layer\" data-type=\"indexterm\" id=\"id1440\"></a>Empirical observations suggest that applying LoRA to more weight matrices, including the feedforward matrices, yields better results. For example, Databricks showed that the biggest performance boost they got was from applying LoRA to all feedforward layers (<a href=\"https://oreil.ly/zzREV\">Sooriyarachchi, 2023</a>). <a href=\"https://arxiv.org/html/2404.05086v1\">Fomenko et al. (2024)</a> noted that feedforward-based LoRA can be complementary to attention-based LoRA, though attention-based LoRA typically offers greater efficacy within memory constraints.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The beauty of LoRA is that while its performance depends on its rank, studies have shown that a small r, such as between 4 and 64, is usually sufficient for many use cases. A smaller r means fewer LoRA parameters, which translates to a lower memory footprint.",
      "raw_html": "<p>The beauty of LoRA is that while its performance depends on its rank, studies have shown that <em>a small r, such as between 4 and 64, is usually sufficient for many use cases</em>. A smaller <em>r</em> means fewer LoRA parameters, which translates to a lower memory footprint.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The LoRA authors observed that, to their surprise, increasing the value of r doesn’t increase finetuning performance. This observation is consistent with Databricks’ report that “increasing r beyond a certain value may not yield any discernible increase in quality of model output” (Sooriyarachchi, 2023).25 Some argue that a higher r might even hurt as it can lead to overfitting. However, in some cases, a higher rank might be necessary. Raschka (2023) found that r = 256 achieved the best performance on his tasks.",
      "raw_html": "<p>The LoRA authors observed that, to their surprise, increasing the value of <em>r</em> doesn’t increase finetuning performance. This observation is consistent with Databricks’ report that “increasing <em>r</em> beyond a certain value may not yield any discernible increase in quality of model output” (Sooriyarachchi, 2023).<sup><a data-type=\"noteref\" href=\"ch07.html#id1441\" id=\"id1441-marker\">25</a></sup> Some argue that a higher <em>r</em> might even hurt as it can lead to overfitting. However, in some cases, a higher rank might be necessary. <a href=\"https://oreil.ly/A-d5f\">Raschka (2023)</a> found that <em>r</em> = 256 achieved the best performance on his tasks. </p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Another LoRA hyperparameter you can configure is the value \nα\n that determines how much the product WAB should contribute to the new matrix during merging: \n\nW\nâ\n\n\n=\nW\n+\nα r\nW AB \n\n. In practice, I’ve often seen ɑ chosen so that the ratio \n\nα\n\n:\n\nr\n\n is typically between 1:8 and 8:1, but the optimal ratio varies. For example, if r is small, you might want \nα\n to be larger, and if r is large, you might want \nα\n to be smaller. Experimentation is needed to determine the best \n\n(\nr\n,\nα\n)\n\n combination for your use case.",
      "raw_html": "<p>Another LoRA hyperparameter you can configure is the value <math alttext=\"alpha\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mi>α</mi>\n</math> that determines how much the product <em>W</em><sub><em>AB</em></sub> should contribute to the new matrix during merging: <math alttext=\"upper W prime equals upper W plus StartFraction alpha Over r EndFraction upper W Subscript upper A upper B\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mrow>\n<mi>W</mi>\n<mi>â</mi>\n<mi></mi>\n<mi></mi>\n<mo>=</mo>\n<mi>W</mi>\n<mo>+</mo>\n<mfrac><mi>α</mi> <mi>r</mi></mfrac>\n<msub><mi>W</mi> <mrow><mi>A</mi><mi>B</mi></mrow> </msub>\n</mrow>\n</math>. In practice, I’ve often seen ɑ chosen so that the ratio <math alttext=\"alpha colon r\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mrow>\n<mi>α</mi>\n<mspace width=\"-0.166667em\"></mspace>\n<mo>:</mo>\n<mspace width=\"-0.166667em\"></mspace>\n<mi>r</mi>\n</mrow>\n</math> is typically between 1:8 and 8:1, but the optimal ratio varies. For example, if <em>r</em> is small, you might want <math alttext=\"alpha\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mi>α</mi>\n</math> to be larger, and if <em>r</em> is large, you might want <math alttext=\"alpha\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mi>α</mi>\n</math> to be smaller. Experimentation is needed to determine the best <math alttext=\"left-parenthesis r comma alpha right-parenthesis\" xmlns=\"http://www.w3.org/1998/Math/MathML\">\n<mrow>\n<mo>(</mo>\n<mi>r</mi>\n<mo>,</mo>\n<mi>α</mi>\n<mo>)</mo>\n</mrow>\n</math> combination for your use case.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html39\" data-type=\"indexterm\" id=\"id1442\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html38\" data-type=\"indexterm\" id=\"id1443\"></a></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Serving LoRA adapters",
      "id": "heading-7",
      "raw_html": "<h4>Serving LoRA adapters</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "LoRA not only lets you finetune models using less memory and data, but it also simplifies serving multiple models due to its modularity. To understand this benefit, let’s examine how to serve a LoRA-finetuned model.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"LoRA (low-rank adaptation)\" data-secondary=\"LoRA adapters service\" data-type=\"indexterm\" id=\"ch07.html40\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-secondary=\"LoRA\" data-tertiary=\"LoRA adapters service\" data-type=\"indexterm\" id=\"ch07.html41\"></a>LoRA not only lets you finetune models using less memory and data, but it also simplifies serving multiple models due to its modularity. To understand this benefit, let’s examine how to serve a LoRA-finetuned model.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In general, there are two ways to serve a LoRA-finetuned model:",
      "raw_html": "<p>In general, there are two ways to serve a LoRA-finetuned model:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Merge the LoRA weights A and B into the original model to create the new matrix Wʹ prior to serving the finetuned model. Since no extra computation is done during inference, no extra latency is added.",
        "Keep W, A, and B separate during serving. The process of merging A and B back to W happens during inference, which adds extra latency."
      ],
      "raw_html": "<ol>\n<li><p>Merge the LoRA weights <em>A</em> and <em>B</em> into the original model to create the new matrix Wʹ prior to serving the finetuned model. Since no extra computation is done during inference, no extra latency is added.</p>\n</li>\n<li><p>Keep <em>W</em>, <em>A</em>, and <em>B</em> separate during serving. The process of merging <em>A</em> and <em>B</em> back to <em>W</em> happens during inference, which adds extra latency.</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Merge the LoRA weights A and B into the original model to create the new matrix Wʹ prior to serving the finetuned model. Since no extra computation is done during inference, no extra latency is added.",
      "raw_html": "<p>Merge the LoRA weights <em>A</em> and <em>B</em> into the original model to create the new matrix Wʹ prior to serving the finetuned model. Since no extra computation is done during inference, no extra latency is added.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Keep W, A, and B separate during serving. The process of merging A and B back to W happens during inference, which adds extra latency.",
      "raw_html": "<p>Keep <em>W</em>, <em>A</em>, and <em>B</em> separate during serving. The process of merging <em>A</em> and <em>B</em> back to <em>W</em> happens during inference, which adds extra latency.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The first option is generally better if you have only one LoRA model to serve, whereas the second is generally better for multi-LoRA serving—serving multiple LoRA models that share the same base model. Figure 7-12 visualizes multi-LoRA serving if you keep the LoRA adapters separate.",
      "raw_html": "<p>The first option is generally better if you have only one LoRA model to serve, whereas the second is generally better for <em>multi-LoRA serving—</em>serving multiple LoRA models that share the same base model. <a data-type=\"xref\" href=\"#ch07b_figure_9_1730159634220354\">Figure 7-12</a> visualizes multi-LoRA serving if you keep the LoRA adapters separate.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch07b_figure_9_1730159634220354",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_9_1730159634220354\">\n<img alt=\"A diagram of a flowchart\n\nDescription automatically generated\" src=\"assets/aien_0712.png\"/>\n<h6><span class=\"label\">Figure 7-12. </span>Keeping LoRA adapters separate allows reuse of the same full-rank matrix <em>W</em> in multi-LoRA serving.</h6>\n</div></figure>",
      "image": "aien_0712.png",
      "alt": "A diagram of a flowchart\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0712.png",
      "caption": {
        "label": "Figure 7-12.",
        "text": "Keeping LoRA adapters separate allows reuse of the same full-rank matrix W in multi-LoRA serving."
      }
    },
    {
      "type": "paragraph",
      "content": "For multi-LoRA serving, while option 2 adds latency overhead, it significantly reduces the storage needed. Consider the scenario in which you finetune a model for each of your customers using LoRA. With 100 customers, you end up with 100 finetuned models, all sharing the same base model. With option 1, you have to store 100 full-rank matrices Wʹ. With option 2, you only have to store one full-rank matrix W, and 100 sets of smaller matrices (A, B).",
      "raw_html": "<p>For multi-LoRA serving, while option 2 adds latency overhead, it significantly reduces the storage needed. Consider the scenario in which you finetune a model for each of your customers using LoRA. With 100 customers, you end up with 100 finetuned models, all sharing the same base model. With option 1, you have to store 100 full-rank matrices <em>W</em>ʹ. With option 2, you only have to store one full-rank matrix <em>W</em>, and 100 sets of smaller matrices (<em>A</em>, <em>B</em>).</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "To put this in perspective, let’s say that the original matrix W is of the dimension 4096 × 4096 (16.8M parameters). If the LoRA’s rank is 8, the number of parameters in A and B is 4096 × 8 × 2 = 65,536:",
      "raw_html": "<p>To put this in perspective, let’s say that the original matrix <em>W</em> is of the dimension <code>4096 × 4096 </code>(16.8M parameters). If the LoRA’s rank is 8, the number of parameters in <em>A</em> and <em>B</em> is <code>4096 × 8 × 2 = 65,536</code>:</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "In option 1, 100 full-rank matrices Wʹ totals 16.8M × 100 = 1.68B parameters.",
        "In option 2, one full-rank matrix W and 100 sets of small matrices (A, B) totals: 16.8M + 65,536 × 100 = 23.3M parameters."
      ],
      "raw_html": "<ul>\n<li><p>In option 1, 100 full-rank matrices <em>W</em>ʹ totals <code>16.8M × 100 = 1.68B</code> parameters.</p></li>\n<li><p>In option 2, one full-rank matrix <em>W</em> and 100 sets of small matrices (<em>A</em>, <em>B</em>) totals: <code>16.8M + 65,536 × 100 = 23.3M</code> parameters.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "In option 1, 100 full-rank matrices Wʹ totals 16.8M × 100 = 1.68B parameters.",
      "raw_html": "<p>In option 1, 100 full-rank matrices <em>W</em>ʹ totals <code>16.8M × 100 = 1.68B</code> parameters.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "In option 2, one full-rank matrix W and 100 sets of small matrices (A, B) totals: 16.8M + 65,536 × 100 = 23.3M parameters.",
      "raw_html": "<p>In option 2, one full-rank matrix <em>W</em> and 100 sets of small matrices (<em>A</em>, <em>B</em>) totals: <code>16.8M + 65,536 × 100 = 23.3M</code> parameters.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Option 2 also makes it faster to switch between tasks. Let’s say you’re currently serving customer X using this customer’s model. To switch to serving customer Y, instead of loading this customer’s full weight matrix, you only need to load Y’s LoRA adapter, which can significantly reduce the loading time. While keeping A and B separate incurs additional latency, there are optimization techniques to minimize the added latency. The book’s GitHub repository contains a walkthrough of how to do so.",
      "raw_html": "<p>Option 2 also makes it faster to switch between tasks. Let’s say you’re currently serving customer <em>X</em> using this customer’s model. To switch to serving customer <em>Y</em>, instead of loading this customer’s full weight matrix, you only need to load Y’s LoRA adapter, which can significantly reduce the loading time. While keeping <em>A</em> and <em>B</em> separate incurs additional latency, there are optimization techniques to minimize the added latency. The <a href=\"https://github.com/chiphuyen/aie-book\">book’s GitHub repository</a> contains a walkthrough of how to do so. </p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Multi-LoRA serving makes it easy to combine multiple specialized models. Instead of having one big powerful model for multiple tasks, you can have one LoRA adapter for each task. For example, Apple used multiple LoRA adapters to adapt the same 3B-parameter base model to different iPhone features (2024). They utilized quantization techniques to further reduce the memory footprint of this base model and adapters, allowing the serving of all of them on-device.",
      "raw_html": "<p>Multi-LoRA serving makes it easy to combine multiple specialized models. Instead of having one big powerful model for multiple tasks, you can have one LoRA adapter for each task. For example, Apple used multiple <a href=\"https://oreil.ly/vfXqE\">LoRA adapters</a> to adapt the same 3B-parameter base model to different iPhone features (2024). They utilized quantization techniques to further reduce the memory footprint of this base model and adapters, allowing the serving of all of them on-device.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The modularity of LoRA adapters means that LoRA adapters can be shared and reused. There are publicly available finetuned LoRA adapters that you can use the way you’d use pre-trained models. You can find them on Hugging Face26 or initiatives like AdapterHub.",
      "raw_html": "<p>The modularity of LoRA adapters means that LoRA adapters can be shared and reused. There are publicly available finetuned LoRA adapters that you can use the way you’d use pre-trained models. You can find them on <a href=\"https://oreil.ly/T08JJ\">Hugging Face</a><sup><a data-type=\"noteref\" href=\"ch07.html#id1444\" id=\"id1444-marker\">26</a></sup> or initiatives like <a href=\"https://adapterhub.ml\">AdapterHub</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "You might be wondering: “LoRA sounds great, but what’s the catch?” The main drawback of LoRA is that it doesn’t offer performance as strong as full finetuning. It’s also more challenging to do than full finetuning as it involves modifying the model’s implementation, which requires an understanding of the model’s architecture and coding skills. However, this is usually only an issue for less popular base models. PEFT frameworks—such as Hugging Face’s PEFT, Axolotl, unsloth, and LitGPT—likely support LoRA for popular base models right out of the box.",
      "raw_html": "<p>You might be wondering: “LoRA sounds great, but what’s the catch?” The main drawback of LoRA is that it doesn’t offer performance as strong as full finetuning. <span class=\"keep-together\">It’s also</span> more challenging to do than full finetuning as it involves modifying the model’s implementation, which requires an understanding of the model’s architecture and coding skills. However, this is usually only an issue for less popular base models. PEFT frameworks—such as <a href=\"https://github.com/huggingface/peft\">Hugging Face’s PEFT</a>, <a href=\"https://github.com/axolotl-ai-cloud/axolotl\">Axolotl</a>, <a href=\"https://github.com/unslothai/unsloth\">unsloth</a>, and <a href=\"https://github.com/Lightning-AI/litgpt\">LitGPT</a>—likely support LoRA for popular base models right out of the box.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html41\" data-type=\"indexterm\" id=\"id1445\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html40\" data-type=\"indexterm\" id=\"id1446\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Quantized LoRA",
      "id": "heading-7",
      "raw_html": "<h4>Quantized LoRA</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "The rapid rise of LoRA has led to the development of numerous LoRA variations. Some aim to reduce the number of trainable parameters even further. However, as illustrated in Table 7-6, the memory of a LoRA adapter is minimal compared to the memory of the model’s weights. Reducing the number of LoRA parameters decreases the overall memory footprint by only a small percentage.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"LoRA (low-rank adaptation)\" data-secondary=\"quantized LoRA (QLoRA)\" data-type=\"indexterm\" id=\"ch07.html42\"></a><a contenteditable=\"false\" data-primary=\"parameter-efficient finetuning\" data-secondary=\"LoRA\" data-tertiary=\"quantized LoRA\" data-type=\"indexterm\" id=\"ch07.html43\"></a><a contenteditable=\"false\" data-primary=\"QLoRA (quantized LoRA)\" data-type=\"indexterm\" id=\"ch07.html44\"></a><a contenteditable=\"false\" data-primary=\"quantized LoRA (QLoRA)\" data-type=\"indexterm\" id=\"ch07.html45\"></a>The rapid rise of LoRA has led to the development of numerous LoRA variations. Some aim to reduce the number of trainable parameters even further. However, as illustrated in <a data-type=\"xref\" href=\"#ch07b_table_4_1730159634233626\">Table 7-6</a>, the memory of a LoRA adapter is minimal compared to the memory of the model’s weights. Reducing the number of LoRA parameters decreases the overall memory footprint by only a small percentage.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch07b_table_4_1730159634233626",
      "raw_html": "<table id=\"ch07b_table_4_1730159634233626\">\n<caption><span class=\"label\">Table 7-6. </span>The memory needed by LoRA weights compared to that needed by the model’s weights.</caption>\n<thead>\n<tr>\n<th></th>\n<th>Model’s weights memory <br/>(16 bits)</th>\n<th>LoRA trainable params<br/>\n(r=2, query &amp; key matrices)</th>\n<th>LoRA adapter memory<br/> (16 bits)</th>\n</tr>\n</thead>\n<tr>\n<td>Llama 2 (13B)</td>\n<td>26 GB</td>\n<td>3.28M</td>\n<td>6.55 MB</td>\n</tr>\n<tr>\n<td>GPT-3 (175B)</td>\n<td>350 GB</td>\n<td>18.87M</td>\n<td>37.7 MB</td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 7-6.",
        "text": "The memory needed by LoRA weights compared to that needed by the model’s weights."
      },
      "headers": [
        "",
        "Model’s weights memory (16 bits)",
        "LoRA trainable params\n(r=2, query & key matrices)",
        "LoRA adapter memory (16 bits)"
      ],
      "rows": [
        [
          "Llama 2 (13B)",
          "26 GB",
          "3.28M",
          "6.55 MB"
        ],
        [
          "GPT-3 (175B)",
          "350 GB",
          "18.87M",
          "37.7 MB"
        ]
      ],
      "row_count": 2,
      "column_count": 4
    },
    {
      "type": "paragraph",
      "content": "Rather than trying to reduce LoRA’s number of parameters, you can reduce the memory usage more effectively by quantizing the model’s weights, activations, and/or gradients during finetuning. An early promising quantized version of LoRA is QLoRA (Dettmers et al., 2023).27 In the original LoRA paper, during finetuning, the model’s weights are stored using 16 bits. QLoRA stores the model’s weights in 4 bits but dequantizes (converts) them back into BF16 when computing the forward and backward pass.",
      "raw_html": "<p class=\"pagebreak-before\">Rather than trying to reduce LoRA’s number of parameters, you can reduce the memory usage more effectively by quantizing the model’s weights, activations, and/or gradients during finetuning. An early promising quantized version of LoRA is QLoRA (<a href=\"https://arxiv.org/abs/2305.14314\">Dettmers et al., 2023</a>).<sup><a data-type=\"noteref\" href=\"ch07.html#id1447\" id=\"id1447-marker\">27</a></sup> In the original LoRA paper, during finetuning, the model’s weights are stored using 16 bits. QLoRA stores the model’s weights in 4 bits but dequantizes (converts) them back into BF16 when computing the forward and backward pass.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The 4-bit format that QLoRA uses is NF4 (NormalFloat-4), which quantizes values based on the insight that pre-trained weights usually follow a normal distribution with a median of zero. On top of 4-bit quantization, QLoRA also uses paged optimizers to automatically transfer data between the CPU and GPU when the GPU runs out of memory, especially with long sequence lengths. These techniques allow a 65B-parameter model to be finetuned on a single 48 GB GPU.",
      "raw_html": "<p>The 4-bit format that QLoRA uses is NF4 (NormalFloat-4), which quantizes values based on the insight that pre-trained weights usually follow a normal distribution with a median of zero. On top of 4-bit quantization, QLoRA also uses paged optimizers to automatically transfer data between the CPU and GPU when the GPU runs out of memory, especially with long sequence lengths. These techniques allow a 65B-parameter model to be finetuned on a single 48 GB GPU.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The authors finetuned a variety of models, including Llama 7B to 65B, in the 4-bit mode. The resulting family of models, called Guanaco, showed competitive performance on both public benchmarks and comparative evaluation. Table 7-7 shows the Elo ratings of Guanaco models, GPT-4, and ChatGPT in May 2023, as judged by GPT-4. While Guanaco 65B didn’t outperform GPT-4, it was often preferred to ChatGPT.",
      "raw_html": "<p>The authors finetuned a variety of models, including Llama 7B to 65B, in the 4-bit mode. The resulting family of models, called Guanaco, showed competitive performance on both public benchmarks and comparative evaluation. <a contenteditable=\"false\" data-primary=\"Elo\" data-type=\"indexterm\" id=\"id1448\"></a><a data-type=\"xref\" href=\"#ch07b_table_5_1730159634233637\">Table 7-7</a> shows the Elo ratings of Guanaco models, GPT-4, and ChatGPT in May 2023, as judged by GPT-4. While Guanaco 65B didn’t outperform GPT-4, it was often preferred to ChatGPT.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch07b_table_5_1730159634233637",
      "raw_html": "<table id=\"ch07b_table_5_1730159634233637\">\n<caption><span class=\"label\">Table 7-7. </span>Elo ratings of Guanaco models compared to popular models in May 2023 using GPT-4 as a judge. The experiment is from QLoRA (Dettmers et al., 2023).</caption>\n<thead>\n<tr>\n<th>Model</th>\n<th>Size</th>\n<th>Elo</th>\n</tr>\n</thead>\n<tr>\n<td>GPT-4</td>\n<td>-</td>\n<td>1348 ± 1</td>\n</tr>\n<tr>\n<td>Guanaco 65B</td>\n<td>41 GB</td>\n<td>1022 ± 1</td>\n</tr>\n<tr>\n<td>Guanaco 33B</td>\n<td>21 GB</td>\n<td>992 ± 1</td>\n</tr>\n<tr>\n<td>Vicuna 13B</td>\n<td>26 GB</td>\n<td>974 ± 1</td>\n</tr>\n<tr>\n<td>ChatGPT</td>\n<td>-</td>\n<td>966 ± 1</td>\n</tr>\n<tr>\n<td>Guanaco 13B</td>\n<td>10 GB</td>\n<td>916 ± 1</td>\n</tr>\n<tr>\n<td>Bard</td>\n<td>-</td>\n<td>902 ± 1</td>\n</tr>\n<tr>\n<td>Guanaco 7B</td>\n<td>6 GB</td>\n<td>879 ± 1</td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 7-7.",
        "text": "Elo ratings of Guanaco models compared to popular models in May 2023 using GPT-4 as a judge. The experiment is from QLoRA (Dettmers et al., 2023)."
      },
      "headers": [
        "Model",
        "Size",
        "Elo"
      ],
      "rows": [
        [
          "GPT-4",
          "-",
          "1348 ± 1"
        ],
        [
          "Guanaco 65B",
          "41 GB",
          "1022 ± 1"
        ],
        [
          "Guanaco 33B",
          "21 GB",
          "992 ± 1"
        ],
        [
          "Vicuna 13B",
          "26 GB",
          "974 ± 1"
        ],
        [
          "ChatGPT",
          "-",
          "966 ± 1"
        ],
        [
          "Guanaco 13B",
          "10 GB",
          "916 ± 1"
        ],
        [
          "Bard",
          "-",
          "902 ± 1"
        ],
        [
          "Guanaco 7B",
          "6 GB",
          "879 ± 1"
        ]
      ],
      "row_count": 8,
      "column_count": 3
    },
    {
      "type": "paragraph",
      "content": "The main limitation of QLoRA is that NF4 quantization is expensive. While QLoRA can reduce the memory footprint, it might increase training time due to the extra time required by quantization and dequantization steps.",
      "raw_html": "<p>The main limitation of QLoRA is that NF4 quantization is expensive. While QLoRA can reduce the memory footprint, it might increase training time due to the extra time required by quantization and dequantization steps.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Due to its memory-saving promise, quantized LoRA is an active area of research. Other than QLoRA, quantized LoRA works include QA-LoRA (Xu et al., 2023), ModuLoRA (Yin et al., 2023), and IR-QLoRA (Qin et al., 2024).",
      "raw_html": "<p class=\"pagebreak-before\">Due to its memory-saving promise, quantized LoRA is an active area of research. Other than QLoRA, quantized LoRA works include QA-LoRA (<a href=\"https://arxiv.org/abs/2309.14717\">Xu et al., 2023</a>), ModuLoRA (<a href=\"https://arxiv.org/abs/2309.16119\">Yin et al., 2023</a>), and IR-QLoRA (<a href=\"https://arxiv.org/abs/2402.05445\">Qin et al., 2024</a>)<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html45\" data-type=\"indexterm\" id=\"id1449\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html44\" data-type=\"indexterm\" id=\"id1450\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html43\" data-type=\"indexterm\" id=\"id1451\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html42\" data-type=\"indexterm\" id=\"id1452\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html37\" data-type=\"indexterm\" id=\"id1453\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html36\" data-type=\"indexterm\" id=\"id1454\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html35a\" data-type=\"indexterm\" id=\"id1455\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html35\" data-type=\"indexterm\" id=\"id1456\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html31\" data-type=\"indexterm\" id=\"id1457\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html30\" data-type=\"indexterm\" id=\"id1458\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html29\" data-type=\"indexterm\" id=\"id1459\"></a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Model Merging and Multi-Task Finetuning",
      "id": "heading-7",
      "raw_html": "<h2>Model Merging and Multi-Task Finetuning</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "If finetuning allows you to create a custom model by altering a single model, model merging allows you to create a custom model by combining multiple models. Model merging offers you greater flexibility than finetuning alone. You can take two available models and merge them together to create a new, hopefully more useful, model. You can also finetune any or all of the constituent models before merging them together.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"techniques\" data-tertiary=\"model merging and multi-task finetuning\" data-type=\"indexterm\" id=\"ch07.html46\"></a><a contenteditable=\"false\" data-primary=\"model merging\" data-type=\"indexterm\" id=\"ch07.html47\"></a>If finetuning allows you to create a custom model by altering a single model, model merging allows you to create a custom model by combining multiple models. Model merging offers you greater flexibility than finetuning alone. You can take two available models and merge them together to create a new, hopefully more useful, model. You can also finetune any or all of the constituent models before merging them together.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While you don’t have to further finetune the merged model, its performance can often be improved by finetuning. Without finetuning, model merging can be done without GPUs, making merging particularly attractive to indie model developers that don’t have access to a lot of compute.",
      "raw_html": "<p>While you don’t have to further finetune the merged model, its performance can often be improved by finetuning. Without finetuning, model merging can be done without GPUs, making merging particularly attractive to indie model developers that don’t have access to a lot of compute.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The goal of model merging is to create a single model that provides more value than using all the constituent models separately. The added value can come from improved performance. For example, if you have two models that are good at different things on the same task, you can merge them into a single model that is better than both of them on that task. Imagine one model that can answer the first 60% of the questions and another model that can answer the last 60% of the questions. Combined, perhaps they can answer 80% of the questions.",
      "raw_html": "<p>The goal of model merging is to create a single model that provides more value than using all the constituent models separately. The added value can come from improved performance. For example, if you have two models that are good at different things on the same task, you can merge them into a single model that is better than both of them on that task. Imagine one model that can answer the first 60% of the questions and another model that can answer the last 60% of the questions. Combined, perhaps they can answer 80% of the questions.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The added value can also come from a reduced memory footprint, which leads to reduced costs. For example, if you have two models that can do different tasks, they can be merged into one model that can do both tasks but with fewer parameters. This is particularly attractive for adapter-based models. Given two models that were finetuned on top of the same base model, you can combine their adapters into a single adapter.",
      "raw_html": "<p>The added value can also come from a reduced memory footprint, which leads to reduced costs. For example, if you have two models that can do different tasks, they can be merged into one model that can do both tasks but with fewer parameters. This is particularly attractive for adapter-based models. Given two models that were finetuned on top of the same base model, you can combine their adapters into a single adapter.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One important use case of model merging is multi-task finetuning. Without model merging, if you want to a finetune a model for multiple tasks, you generally have to follow one of these approaches:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"multi-task finetuning\" data-type=\"indexterm\" id=\"id1460\"></a>One important use case of model merging is multi-task finetuning. Without model merging, if you want to a finetune a model for multiple tasks, you generally have to follow one of these approaches:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Simultaneous finetuning",
          "definition": "You create a dataset with examples for all the tasks and finetune the model on this dataset to make the model learn all the tasks simultaneously. However, because it’s generally harder to learn multiple skills at the same time, this approach typically requires more data and more training."
        }
      ],
      "raw_html": "<dl>\n<dt>Simultaneous finetuning</dt>\n<dd>\n<p><a contenteditable=\"false\" data-primary=\"simultaneous finetuning\" data-type=\"indexterm\" id=\"id1461\"></a>You create a dataset with examples for all the tasks and finetune the model on this dataset to make the model learn all the tasks simultaneously. However, because it’s generally harder to learn multiple skills at the same time, this approach typically requires more data and more training.</p>\n</dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "You create a dataset with examples for all the tasks and finetune the model on this dataset to make the model learn all the tasks simultaneously. However, because it’s generally harder to learn multiple skills at the same time, this approach typically requires more data and more training.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"simultaneous finetuning\" data-type=\"indexterm\" id=\"id1461\"></a>You create a dataset with examples for all the tasks and finetune the model on this dataset to make the model learn all the tasks simultaneously. However, because it’s generally harder to learn multiple skills at the same time, this approach typically requires more data and more training.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Sequential finetuning",
          "definition": "You can finetune the model on each task separately but sequentially. After training a model on task A, train it on task B, and so on. The assumption is that it’s easier for models to learn one task at a time. Unfortunately, neural networks are prone to catastrophic forgetting (Kirkpatrick et al., 2016). A model can forget how to do an old task when it’s trained on a new task, leading to a significant performance drop on earlier tasks."
        }
      ],
      "raw_html": "<dl class=\"pagebreak-before\">\n<dt class=\"less_space\">Sequential finetuning</dt>\n<dd>\n<p><a contenteditable=\"false\" data-primary=\"sequential finetuning\" data-type=\"indexterm\" id=\"id1462\"></a>You can finetune the model on each task separately but sequentially. After training a model on task A, train it on task B, and so on. The assumption is that it’s easier for models to learn one task at a time. Unfortunately, neural networks are prone to catastrophic forgetting (<a href=\"https://arxiv.org/abs/1612.00796\">Kirkpatrick et al., 2016</a>). A model can forget how to do an old task when it’s trained on a new task, leading to a significant performance drop on earlier tasks.</p>\n</dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "You can finetune the model on each task separately but sequentially. After training a model on task A, train it on task B, and so on. The assumption is that it’s easier for models to learn one task at a time. Unfortunately, neural networks are prone to catastrophic forgetting (Kirkpatrick et al., 2016). A model can forget how to do an old task when it’s trained on a new task, leading to a significant performance drop on earlier tasks.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"sequential finetuning\" data-type=\"indexterm\" id=\"id1462\"></a>You can finetune the model on each task separately but sequentially. After training a model on task A, train it on task B, and so on. The assumption is that it’s easier for models to learn one task at a time. Unfortunately, neural networks are prone to catastrophic forgetting (<a href=\"https://arxiv.org/abs/1612.00796\">Kirkpatrick et al., 2016</a>). A model can forget how to do an old task when it’s trained on a new task, leading to a significant performance drop on earlier tasks.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model merging offers another method for multi-task finetuning. You can finetune the model on different tasks separately but in parallel. Once done, these different models are merged together. Finetuning on each task separately allows the model to learn that task better. Because there’s no sequential learning, there’s less risk of catastrophic forgetting.",
      "raw_html": "<p>Model merging offers another method for multi-task finetuning. You can finetune the model on different tasks separately but in parallel. Once done, these different models are merged together. Finetuning on each task separately allows the model to learn that task better. Because there’s no sequential learning, there’s less risk of catastrophic forgetting.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model merging is also appealing when you have to deploy models to devices such as phones, laptops, cars, smartwatches, and warehouse robots. On-device deployment is often challenging because of limited on-device memory capacity. Instead of squeezing multiple models for different tasks onto a device, you can merge these models together into one model that can perform multiple tasks while requiring much less memory.",
      "raw_html": "<p>Model merging is also appealing when you have to deploy models to devices such as phones, laptops, cars, smartwatches, and warehouse robots. On-device deployment is often challenging because of limited on-device memory capacity. Instead of squeezing multiple models for different tasks onto a device, you can merge these models together into one model that can perform multiple tasks while requiring much less memory.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "On-device deployment is necessary for use cases where data can’t leave the device (often due to privacy), or where there’s limited or unreliable internet access. On-device deployment can also significantly reduce inference costs. The more computation you can offload to user devices, the less you have to pay to data centers.28",
      "raw_html": "<p>On-device deployment is necessary for use cases where data can’t leave the device (often due to privacy), or where there’s limited or unreliable internet access. On-device deployment can also significantly reduce inference costs. The more computation you can offload to user devices, the less you have to pay to data centers.<sup><a data-type=\"noteref\" href=\"ch07.html#id1463\" id=\"id1463-marker\">28</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model merging is one way to do federated learning (McMahan et al., 2016), in which multiple devices train the same model using separate data. For example, if you deploy model X to multiple devices, each copy of X can continue learning separately from the on-device data. After a while, you have multiple copies of X, all trained on different data. You can merge these copies together into one new base model that contains the learning of all constituent models.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"federated learning\" data-type=\"indexterm\" id=\"id1464\"></a>Model merging is one way to do <em>federated learning</em> (<a href=\"https://arxiv.org/abs/1602.05629\">McMahan et al., 2016</a>), in which multiple devices train the same model using separate data. For example, if you deploy model X to multiple devices, each copy of X can continue learning separately from the on-device data. After a while, you have multiple copies of X, all trained on different data. You can merge these copies together into one new base model that contains the learning of all constituent <span class=\"keep-together\">models.</span></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The idea of combining models together to obtain better performance started with model ensemble methods. According to Wikipedia, ensembling combines “multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.” If model merging typically involves mixing parameters of constituent models together, ensembling typically combines only model outputs while keeping each constituent model intact.",
      "raw_html": "<p>The idea of combining models together to obtain better performance started with <em>model ensemble methods</em>. According to <a href=\"https://en.wikipedia.org/wiki/Ensemble_learning\">Wikipedia</a>, ensembling combines “multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.” If model merging typically involves mixing parameters of constituent models together, ensembling typically combines only model outputs while keeping each constituent model intact.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "For example, in ensembling, given a query, you might use three models to generate three different answers. Then, a final answer is generated based on these three answers, using a simple majority vote or another trainable ML module.29 While ensembling can generally improve performance, it has a higher inference cost since it requires multiple inference calls per request.",
      "raw_html": "<p class=\"pagebreak-before\">For example, in ensembling, given a query, you might use three models to generate three different answers. Then, a final answer is generated based on these three answers, using a simple majority vote or another trainable ML module.<sup><a data-type=\"noteref\" href=\"ch07.html#id1465\" id=\"id1465-marker\">29</a></sup> While ensembling can generally improve performance, it has a higher inference cost since it requires multiple inference calls per request.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Figure 7-13 compares ensembling and model merging. Just like model ensembles used to dominate leaderboards, many models on top of the Hugging Face’s Open LLM Leaderboard are merged models.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch07b_figure_10_1730159634220361\">Figure 7-13</a> compares ensembling and model merging. Just like model ensembles used to dominate leaderboards, many models on top of the <a href=\"https://oreil.ly/hRV9P\">Hugging Face’s Open LLM Leaderboard</a> are merged models.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_10_1730159634220361",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_10_1730159634220361\">\n<img alt=\"A diagram of a model\n\nDescription automatically generated\" src=\"assets/aien_0713.png\"/>\n<h6><span class=\"label\">Figure 7-13. </span>How ensembling and model merging work.</h6>\n</div></figure>",
      "image": "aien_0713.png",
      "alt": "A diagram of a model\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0713.png",
      "caption": {
        "label": "Figure 7-13.",
        "text": "How ensembling and model merging work."
      }
    },
    {
      "type": "paragraph",
      "content": "Many model-merging techniques are experimental and might become outdated as the community gains a better understanding of the underlying theory. For this reason, I’ll focus on the high-level merging approaches instead of any individual technique.",
      "raw_html": "<p>Many model-merging techniques are experimental and might become outdated as the community gains a better understanding of the underlying theory. For this reason, I’ll focus on the high-level merging approaches instead of any individual <span class=\"keep-together\">technique.</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model merging approaches differ in how the constituent parameters are combined. Three approaches covered here are summing, layer stacking, and concatenation. Figure 7-14 shows their high-level differences.",
      "raw_html": "<p>Model merging approaches differ in how the constituent parameters are combined. Three approaches covered here are summing, layer stacking, and concatenation. <a data-type=\"xref\" href=\"#ch07b_figure_11_1730159634220368\">Figure 7-14</a> shows their high-level differences.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_11_1730159634220368",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_11_1730159634220368\">\n<img alt=\"A diagram of different colored bricks\n\nDescription automatically generated\" src=\"assets/aien_0714.png\"/>\n<h6><span class=\"label\">Figure 7-14. </span>Three main approaches to model merging: summing, layer stacking, and concatenation.</h6>\n</div></figure>",
      "image": "aien_0714.png",
      "alt": "A diagram of different colored bricks\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0714.png",
      "caption": {
        "label": "Figure 7-14.",
        "text": "Three main approaches to model merging: summing, layer stacking, and concatenation."
      }
    },
    {
      "type": "paragraph",
      "content": "You can mix these approaches when merging models, e.g., summing some layers and stacking others. Let’s explore each of these approaches.",
      "raw_html": "<p>You can mix these approaches when merging models, e.g., summing some layers and stacking others. Let’s explore each of these approaches.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Summing",
      "id": "heading-7",
      "raw_html": "<h3>Summing</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "This approach involves adding the weight values of constituent models together. I’ll discuss two summing methods: linear combination and spherical linear interpolation. If the parameters in two models are in different scales, e.g., one model’s parameter values are much larger than the other’s, you can rescale the models before summing so that their parameter values are in the same range.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"model merging\" data-secondary=\"summing\" data-type=\"indexterm\" id=\"ch07.html48\"></a><a contenteditable=\"false\" data-primary=\"summing\" data-type=\"indexterm\" id=\"ch07.html49\"></a>This approach involves adding the weight values of constituent models together. I’ll discuss two summing methods: linear combination and spherical linear interpolation. If the parameters in two models are in different scales, e.g., one model’s parameter values are much larger than the other’s, you can rescale the models before summing so that their parameter values are in the same range.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Linear combination",
      "id": "heading-7",
      "raw_html": "<h4>Linear combination</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Linear combination includes both an average and a weighted average. Given two models, A and B, their weighted average is:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"linear combination summing\" data-type=\"indexterm\" id=\"ch07.html50\"></a><a contenteditable=\"false\" data-primary=\"summing\" data-secondary=\"linear combination\" data-type=\"indexterm\" id=\"ch07.html51\"></a>Linear combination includes both an average and a weighted average. Given two models, A and B, their weighted average is:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Figure 7-15 shows how to linearly combine two layers when wA = wB = 1.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch07b_figure_12_1730159634220376\">Figure 7-15</a> shows how to linearly combine two layers when <em>w</em><sub><em>A</em></sub> = <em>w</em><sub><em>B</em></sub> = 1.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch07b_figure_12_1730159634220376",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_12_1730159634220376\">\n<img alt=\"A diagram of yellow circles with numbers\n\nDescription automatically generated\" src=\"assets/aien_0715.png\"/>\n<h6><span class=\"label\">Figure 7-15. </span>Merging parameters by averaging them.</h6>\n</div></figure>",
      "image": "aien_0715.png",
      "alt": "A diagram of yellow circles with numbers\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0715.png",
      "caption": {
        "label": "Figure 7-15.",
        "text": "Merging parameters by averaging them."
      }
    },
    {
      "type": "paragraph",
      "content": "Linear combination works surprisingly well, given how simple it is.30 The idea that multiple models can be linearly combined to create a better one was studied as early as the early 1990s (Perrone, 1993). Linear combination is often used in federated learning (Wang et al., 2020).",
      "raw_html": "<p>Linear combination works surprisingly well, given how simple it is.<sup><a data-type=\"noteref\" href=\"ch07.html#id1466\" id=\"id1466-marker\">30</a></sup> The idea that multiple models can be linearly combined to create a better one was studied as early as the early 1990s (<a href=\"https://oreil.ly/eXC02\">Perrone, 1993</a>). Linear combination is often used in federated learning (<a href=\"https://oreil.ly/ZKRPR\">Wang et al., 2020</a>). </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "You can linearly combine entire models or parts of models. Model soups (Wortsman et al., 2022) showed how averaging the entire weights of multiple finetuned models can improve accuracy without increasing inference time. However, it’s more common to merge models by linearly combining specific components, such as their adapters.",
      "raw_html": "<p>You can linearly combine entire models or parts of models. Model soups (<a href=\"https://arxiv.org/abs/2203.05482\">Wortsman et al., 2022</a>) showed how averaging the entire weights of multiple finetuned models can improve accuracy without increasing inference time. However, it’s more common to merge models by linearly combining specific components, such as their adapters.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While you can linearly combine any set of models, linear combination is the most effective for models finetuned on top of the same base model. In this case, linear combination can be viewed through the concept of task vectors. The idea is that once you’ve finetuned a model for a specific task, subtracting the base model from it should give you a vector that captures the essence of the task. Task vectors are also called delta parameters. If you finetune using LoRA, you can construct the task vector from the LoRA weights.",
      "raw_html": "<p>While you can linearly combine any set of models, <em>linear combination is the most effective for models finetuned on top of the same base model. </em>In this case, linear combination can be viewed through the concept of <em>task vectors</em>. The idea is that once you’ve finetuned a model for a specific task, subtracting the base model from it should give you a vector that captures the essence of the task. Task vectors are also called <em>delta parameters</em>. If you finetune using LoRA, you can construct the task vector from the LoRA weights.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Task vectors allow us to do task arithmetic (Ilharco et al., 2022), such as adding two task vectors to combine task capabilities or subtracting a task vector to reduce specific capabilities. Task subtraction can be useful for removing undesirable model behaviors, such as invasive capabilities like facial recognition or biases obtained during pre-training.",
      "raw_html": "<p>Task vectors allow us to do <em>task arithmetic</em> (<a href=\"https://arxiv.org/abs/2212.04089\">Ilharco et al., 2022</a>), such as adding two task vectors to combine task capabilities or subtracting a task vector to reduce specific capabilities. Task subtraction can be useful for removing undesirable model behaviors, such as invasive capabilities like facial recognition or biases obtained during pre-training.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Linear combination is straightforward when the components to be merged are of the same architecture and of the same size. However, it can also work for models that don’t share the same architecture or the same size. For example, if one model’s layer is larger than that of the other model, you can project one or both layers into the same dimension.",
      "raw_html": "<p>Linear combination is straightforward when the components to be merged are of the same architecture and of the same size. However, it can also work for models that don’t share the same architecture or the same size. For example, if one model’s layer is larger than that of the other model, you can project one or both layers into the same dimension.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Some people proposed aligning models before averaging to ensure that functionally related parameters are averaged together, such as in “Model Fusion via Optimal Transport” (Singh and Jaggi, 2020), “Git Re-Basin: Merging Models Modulo Permutation Symmetries” (Ainsworth et al., 2022), and “Merging by Matching Models in Task Parameter Subspaces” (Tam et al., 2023). While it makes sense to combine aligned parameters, aligning parameters can be challenging to do, and, therefore, this approach is less common on naive linear combinations.",
      "raw_html": "<p>Some people proposed aligning models before averaging to ensure that functionally related parameters are averaged together, such as in “Model Fusion via Optimal Transport” (<a href=\"https://arxiv.org/abs/1910.05653\">Singh and Jaggi, 2020</a>), “Git Re-Basin: Merging Models Modulo Permutation Symmetries” (<a href=\"https://arxiv.org/abs/2209.04836\">Ainsworth et al., 2022</a>), and “Merging by Matching Models in Task Parameter Subspaces” (<a href=\"https://arxiv.org/abs/2312.04339\">Tam et al., 2023</a>). While it makes sense to combine aligned parameters, aligning parameters can be challenging to do, and, therefore, this approach is less common on naive linear combinations.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html51\" data-type=\"indexterm\" id=\"id1467\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html50\" data-type=\"indexterm\" id=\"id1468\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Spherical linear interpolation (SLERP)",
      "id": "heading-7",
      "raw_html": "<h4>Spherical linear interpolation (SLERP)</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Another common model summing method is SLERP, which is based on the mathematical operator of the same name, Spherical LinEar inteRPolation.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"SLERP (spherical linear interpolation)\" data-type=\"indexterm\" id=\"id1469\"></a><a contenteditable=\"false\" data-primary=\"spherical linear interpolation (SLERP)\" data-type=\"indexterm\" id=\"id1470\"></a><a contenteditable=\"false\" data-primary=\"summing\" data-secondary=\"spherical linear interpolation (SLERP)\" data-type=\"indexterm\" id=\"id1471\"></a>Another common model summing method is SLERP, which is based on the mathematical operator of the same name, Spherical LinEar inteRPolation.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "Interpolation means estimating unknown values based on known values. In the case of model merging, the unknown value is the merged model, and the known values are the constituent models. Linear combination is one interpolation technique. SLERP is another.",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p>Interpolation means estimating unknown values based on known values. In the case of model merging, the unknown value is the merged model, and the known values are the constituent models. Linear combination is one interpolation technique. SLERP is another.</p>\n</div>"
    },
    {
      "type": "paragraph",
      "content": "Because the formula for SLERP is mathy, and model-merging tools typically implement it for you, I won’t go into the details here. Intuitively, you can think of each component (vector) to be merged as a point on a sphere. To merge two vectors, you first draw the shortest path between these two points along the sphere’s surface. This is similar to drawing the shortest path between two cities along the Earth’s surface. The merged vector of these two vectors is a point along their shortest path. Where exactly the point falls along the path depends on the interpolation factor, which you can set to be between 0 and 1. Factor values less than 0.5 bring the merged vector closer to the first vector, which means that the first task vector will contribute more to the result. A factor of 0.5 means that you pick a point exactly halfway. This middle point is the blue point in Figure 7-16.",
      "raw_html": "<p>Because the formula for SLERP is mathy, and model-merging tools typically implement it for you, I won’t go into the details here. Intuitively, you can think of each component (vector) to be merged as a point on a sphere. To merge two vectors, you first draw the shortest path between these two points along the sphere’s surface. This is similar to drawing the shortest path between two cities along the Earth’s surface. The merged vector of these two vectors is a point along their shortest path. Where exactly the point falls along the path depends on the interpolation factor, which you can set to be between 0 and 1. Factor values less than 0.5 bring the merged vector closer to the first vector, which means that the first task vector will contribute more to the result. A factor of 0.5 means that you pick a point exactly halfway. This middle point is the blue point in <a data-type=\"xref\" href=\"#ch07b_figure_13_1730159634220389\">Figure 7-16</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "SLERP, as a mathematical operation, is defined with only two vectors, which means that you can merge only two vectors at a time. If you want to merge more than two vectors, you can potentially do SLERP sequentially, i.e., merging A with B, and then merging that result with C.",
      "raw_html": "<p>SLERP, as a mathematical operation, is defined with only two vectors, which means that you can merge only two vectors at a time. If you want to merge more than two vectors, you can potentially do SLERP sequentially, i.e., merging A with B, and then merging that result with C.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_13_1730159634220389",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_13_1730159634220389\">\n<img alt=\"A circle with arrows and a red circle\n\nDescription automatically generated\" src=\"assets/aien_0716.png\"/>\n<h6><span class=\"label\">Figure 7-16. </span>How SLERP works for two vectors t1 and t2. The red line is their shortest path on the spherical surface. Depending on the interpolation, the merged vector can be any point along this path. The blue vector is the resulting merged vector when the interpolation factor is 0.5.</h6>\n</div></figure>",
      "image": "aien_0716.png",
      "alt": "A circle with arrows and a red circle\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0716.png",
      "caption": {
        "label": "Figure 7-16.",
        "text": "How SLERP works for two vectors t1 and t2. The red line is their shortest path on the spherical surface. Depending on the interpolation, the merged vector can be any point along this path. The blue vector is the resulting merged vector when the interpolation factor is 0.5."
      }
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Pruning redundant task-specific parameters",
      "id": "heading-7",
      "raw_html": "<h4>Pruning redundant task-specific parameters</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "During finetuning, many models’ parameters are adjusted. However, most of these adjustments are minor and don’t significantly contribute to the model’s performance on the task.31 Adjustments that don’t contribute to the model’s performance are considered redundant.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"summing\" data-secondary=\"pruning redundant task-specific parameters\" data-type=\"indexterm\" id=\"id1472\"></a>During finetuning, many models’ parameters are adjusted. However, most of these adjustments are minor and don’t significantly contribute to the model’s performance on the task.<sup><a data-type=\"noteref\" href=\"ch07.html#id1473\" id=\"id1473-marker\">31</a></sup> Adjustments that don’t contribute to the model’s performance are considered <em>redundant</em>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "In the paper “TIES-Merging: Resolving Interference When Merging Models”, Yadav et al. (2023) showed that you can reset a large portion of task vector parameters with minimal performance degradation, as shown in Figure 7-17. Resetting means changing the finetuned parameter to its original value in the base model, effectively setting the corresponding task vector parameter to zero. (Recall that the task vector can be obtained by subtracting the base model from the finetuned model.)",
      "raw_html": "<p>In the paper “TIES-Merging: Resolving Interference When Merging Models”, <a href=\"https://arxiv.org/abs/2306.01708\">Yadav et al. (2023)</a> showed that you can reset a large portion of task vector parameters with minimal performance degradation, as shown in <a data-type=\"xref\" href=\"#ch07b_figure_14_1730159634220402\">Figure 7-17</a>. Resetting means changing the finetuned parameter to its original value in the base model, effectively setting the corresponding task vector parameter to zero. (Recall that the task vector can be obtained by subtracting the base model from the finetuned model.) </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_14_1730159634220402",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_14_1730159634220402\">\n<img alt=\"A graph with a line and a dotted line\n\nDescription automatically generated\" src=\"assets/aien_0717.png\"/>\n<h6><span class=\"label\">Figure 7-17. </span>In Yadav et al.’s experiments, keeping the top 20% of the task vector parameters gives comparable performance to keeping 100% of the parameters.</h6>\n</div></figure>",
      "image": "aien_0717.png",
      "alt": "A graph with a line and a dotted line\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0717.png",
      "caption": {
        "label": "Figure 7-17.",
        "text": "In Yadav et al.’s experiments, keeping the top 20% of the task vector parameters gives comparable performance to keeping 100% of the parameters."
      }
    },
    {
      "type": "paragraph",
      "content": "These redundant parameters, while not harmful to one model, might be harmful to the merged model. Merging techniques such as TIES (Yadav et al., 2023) and DARE (Yu et al., 2023) first prune the redundant parameters from task vectors before merging them.32 Both papers showed that this practice can significantly improve the quality of the final merged models. The more models there are to merge, the more important pruning is because there are more opportunities for redundant parameters in one task to interfere with other tasks.33",
      "raw_html": "<p>These redundant parameters, while not harmful to one model, might be harmful to the merged model. Merging techniques such as TIES (Yadav et al., 2023) and DARE (<a href=\"https://arxiv.org/abs/2311.03099\">Yu et al., 2023</a>) first prune the redundant parameters from task vectors before merging them.<sup><a data-type=\"noteref\" href=\"ch07.html#id1474\" id=\"id1474-marker\">32</a></sup> Both papers showed that this practice can significantly improve the quality of the final merged models. The more models there are to merge, the more important pruning is because there are more opportunities for redundant parameters in one task to interfere with other tasks.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html49\" data-type=\"indexterm\" id=\"id1475\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html48\" data-type=\"indexterm\" id=\"id1476\"></a><sup><a data-type=\"noteref\" href=\"ch07.html#id1477\" id=\"id1477-marker\">33</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Layer stacking",
      "id": "heading-7",
      "raw_html": "<h3>Layer stacking</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "In this approach, you take different layers from one or more models and stack them on top of each other. For example, you might take the first layer from model 1 and the second layer from model 2. This approach is also called passthrough or frankenmerging. It can create models with unique architectures and numbers of parameters. Unlike the merging by summing approach, the merged models resulting from layer stacking typically require further finetuning to achieve good performance.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"layer stacking\" data-type=\"indexterm\" id=\"ch07.html52\"></a><a contenteditable=\"false\" data-primary=\"model merging\" data-secondary=\"layer stacking\" data-type=\"indexterm\" id=\"ch07.html53\"></a>In this approach, you take different layers from one or more models and stack them on top of each other. For example, you might take the first layer from model 1 and the second layer from model 2. This approach is also called <em>passthrough</em> or <em>frankenmerging</em>. It can create models with unique architectures and numbers of parameters. Unlike the merging by summing approach, the merged models resulting from layer stacking typically require further finetuning to achieve good performance.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "One early success of frankenmerging is Goliath-120B (alpindale, 2023), which was merged from two finetuned Llama 2-70B models, Xwin and Euryale. It took 72 out of 80 layers from each model and merged them together.",
      "raw_html": "<p>One early success of frankenmerging is <a href=\"https://oreil.ly/IM0Jc\">Goliath-120B</a> (alpindale, 2023), which was merged from two finetuned Llama 2-70B models, <a href=\"https://oreil.ly/URfbk\">Xwin</a> and <a href=\"https://oreil.ly/Ftnxd\">Euryale</a>. It took 72 out of 80 layers from each model and merged them together.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Layer stacking can be used to train mixture-of-experts (MoE) models, as introduced in “Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints” (Komatsuzaki et al., 2022). Rather than training an MOE from scratch, you take a pre-trained model and make multiple copies of certain layers or modules. A router is then added to send each input to the most suitable copy. You then further train the merged model along with the router to refine their performance. Figure 7-18 illustrates this process.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"mixture-of-experts (MoE) models\" data-type=\"indexterm\" id=\"id1478\"></a><a contenteditable=\"false\" data-primary=\"MoE (mixture-of-experts) models\" data-type=\"indexterm\" id=\"id1479\"></a>Layer stacking can be used to train mixture-of-experts (MoE) models, as introduced in “Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints” (<a href=\"https://arxiv.org/abs/2212.05055\">Komatsuzaki et al., 2022</a>). Rather than training an MOE from scratch, you take a pre-trained model and make multiple copies of certain layers or modules. A router is then added to send each input to the most suitable copy. You then further train the merged model along with the router to refine their performance. <a data-type=\"xref\" href=\"#ch07b_figure_15_1730159634220410\">Figure 7-18</a> illustrates this process. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Komatsuzaki et al. showed that layer stacking can produce models that outperform MoE models trained from scratch. Using this approach, Together AI mixed six weaker open source models together to create Mixture-of-Agents, which achieved comparable performance to OpenAI’s GPT-4o in some benchmarks (Wang et al., 2024).",
      "raw_html": "<p>Komatsuzaki et al. showed that layer stacking can produce models that outperform MoE models trained from scratch. Using this approach, Together AI mixed six weaker open source models together to create Mixture-of-Agents, which achieved comparable performance to OpenAI’s GPT-4o in some benchmarks (<a href=\"https://arxiv.org/abs/2406.04692\">Wang et al., 2024</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_15_1730159634220410",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_15_1730159634220410\">\n<img alt=\"A diagram of a machine\n\nDescription automatically generated\" src=\"assets/aien_0718.png\"/>\n<h6><span class=\"label\">Figure 7-18. </span>You can create an MoE model from a pre-trained model. Image adapted from Komatsuzaki et al. (2022).</h6>\n</div></figure>",
      "image": "aien_0718.png",
      "alt": "A diagram of a machine\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0718.png",
      "caption": {
        "label": "Figure 7-18.",
        "text": "You can create an MoE model from a pre-trained model. Image adapted from Komatsuzaki et al. (2022)."
      }
    },
    {
      "type": "paragraph",
      "content": "An interesting use case of layer stacking is model upscaling. Model upscaling is the study of how to create larger models using fewer resources. Sometimes, you might want a bigger model than what you already have, presumably because bigger models give better performance. For example, your team might have originally trained a model to fit on your 40 GB GPU. However, you obtained a new machine with 80 GB, which allows you to serve a bigger model. Instead of training a new model from scratch, you can use layer stacking to create a larger model from the existing model.",
      "raw_html": "<p>An interesting use case of layer stacking is <em>model upscaling</em>. Model upscaling is the study of how to create larger models using fewer resources. Sometimes, you might want a bigger model than what you already have, presumably because bigger models give better performance. For example, your team might have originally trained a model to fit on your 40 GB GPU. However, you obtained a new machine with 80 GB, which allows you to serve a bigger model. Instead of training a new model from scratch, you can use layer stacking to create a larger model from the existing model.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "One approach to layer upscaling is depthwise scaling. Kim et al. (2023) used this technique to create SOLAR 10.7B from one 7B-parameter model with 32 layers. The procedure works as follows:",
      "raw_html": "<p>One approach to layer upscaling is <em>depthwise scaling</em>. <a href=\"https://arxiv.org/abs/2312.15166\">Kim et al. (2023)</a> used this technique to create SOLAR 10.7B from one 7B-parameter model with 32 layers. The procedure works as follows:</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Make a copy of the original pre-trained model.",
        "Merge these two copies by summing certain layers (summing two layers and turning them into one layer) and stacking the rest. The layers to be summed are carefully selected to match the target model size. For SOLAR 10.7B, 16 layers are summed, leaving the final model with 32 × 2 - 16 = 48 layers.",
        "Further train this upscaled model toward the target performance."
      ],
      "raw_html": "<ol>\n<li><p>Make a copy of the original pre-trained model.</p></li>\n<li><p>Merge these two copies by summing certain layers (summing two layers and turning them into one layer) and stacking the rest. The layers to be summed are carefully selected to match the target model size. For SOLAR 10.7B, 16 layers are summed, leaving the final model with 32 × 2 - 16 = 48 layers.</p></li>\n<li><p>Further train this upscaled model toward the target performance.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Make a copy of the original pre-trained model.",
      "raw_html": "<p>Make a copy of the original pre-trained model.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Merge these two copies by summing certain layers (summing two layers and turning them into one layer) and stacking the rest. The layers to be summed are carefully selected to match the target model size. For SOLAR 10.7B, 16 layers are summed, leaving the final model with 32 × 2 - 16 = 48 layers.",
      "raw_html": "<p>Merge these two copies by summing certain layers (summing two layers and turning them into one layer) and stacking the rest. The layers to be summed are carefully selected to match the target model size. For SOLAR 10.7B, 16 layers are summed, leaving the final model with 32 × 2 - 16 = 48 layers.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Further train this upscaled model toward the target performance.",
      "raw_html": "<p>Further train this upscaled model toward the target performance.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Figure 7-19 illustrates this process.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch07b_figure_16_1730159634220419\">Figure 7-19</a> illustrates this process.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html53\" data-type=\"indexterm\" id=\"id1480\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html52\" data-type=\"indexterm\" id=\"id1481\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch07b_figure_16_1730159634220419",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_16_1730159634220419\">\n<img alt=\"A screenshot of a computer program\n\nDescription automatically generated\" src=\"assets/aien_0719.png\"/>\n<h6><span class=\"label\">Figure 7-19. </span>Use depthwise scaling to create a 48-layer model from a 32-layer model. The image is licensed under CC BY 4.0 and was slightly modified for readability. </h6>\n</div></figure>",
      "image": "aien_0719.png",
      "alt": "A screenshot of a computer program\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0719.png",
      "caption": {
        "label": "Figure 7-19.",
        "text": "Use depthwise scaling to create a 48-layer model from a 32-layer model. The image is licensed under CC BY 4.0 and was slightly modified for readability."
      }
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Concatenation",
      "id": "heading-7",
      "raw_html": "<h3>Concatenation</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Instead of adding the parameters of the constituent models together in different manners, you can also concatenate them. The merged component’s number of parameters will be the sum of the number of parameters from all constituent components. If you merge two LoRA adapters of ranks r1 and r2, the merged adapter’s rank will be r1 + r2, as shown in Figure 7-20.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"concatenation\" data-type=\"indexterm\" id=\"id1482\"></a><a contenteditable=\"false\" data-primary=\"model merging\" data-secondary=\"concatenation\" data-type=\"indexterm\" id=\"id1483\"></a>Instead of adding the parameters of the constituent models together in different manners, you can also concatenate them. The merged component’s number of parameters will be the sum of the number of parameters from all constituent components. <a contenteditable=\"false\" data-primary=\"adapters\" data-secondary=\"merging with concatenation\" data-type=\"indexterm\" id=\"id1484\"></a>If you merge two LoRA adapters of ranks <em>r</em><sub>1</sub> and <em>r</em><sub>2</sub>, the merged adapter’s rank will be <em>r</em><sub>1</sub> + <em>r</em><sub>2</sub>, as shown in <a data-type=\"xref\" href=\"#ch07b_figure_17_1730159634220429\">Figure 7-20</a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch07b_figure_17_1730159634220429",
      "raw_html": "<figure><div class=\"figure\" id=\"ch07b_figure_17_1730159634220429\">\n<img alt=\"A diagram of a algorithm\n\nDescription automatically generated\" src=\"assets/aien_0720.png\"/>\n<h6><span class=\"label\">Figure 7-20. </span>If you merge two LoRA adapters using concatenation, the rank of the merged adapter will be the sum of both adapters’ ranks.</h6>\n</div></figure>",
      "image": "aien_0720.png",
      "alt": "A diagram of a algorithm\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0720.png",
      "caption": {
        "label": "Figure 7-20.",
        "text": "If you merge two LoRA adapters using concatenation, the rank of the merged adapter will be the sum of both adapters’ ranks."
      }
    },
    {
      "type": "paragraph",
      "content": "Concatenation isn’t recommended because it doesn’t reduce the memory footprint compared to serving different models separately. Concatenation might give better performance, but the incremental performance might not be worth the number of extra parameters.34",
      "raw_html": "<p>Concatenation isn’t recommended because it doesn’t reduce the memory footprint compared to serving different models separately. Concatenation might give better performance, but the incremental performance might not be worth the number of extra parameters.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html47\" data-type=\"indexterm\" id=\"id1485\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html46\" data-type=\"indexterm\" id=\"id1486\"></a><sup><a data-type=\"noteref\" href=\"ch07.html#id1487\" id=\"id1487-marker\">34</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Finetuning Tactics",
      "id": "heading-7",
      "raw_html": "<h2>Finetuning Tactics</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "This chapter has discussed multiple finetuning approaches, what problems they solve, and how they work. In this last section, I’ll focus on more practical finetuning tactics.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"tactics\" data-type=\"indexterm\" id=\"ch07.html54\"></a>This chapter has discussed multiple finetuning approaches, what problems they solve, and how they work. In this last section, I’ll focus on more practical finetuning tactics.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Finetuning frameworks and base models",
      "id": "heading-7",
      "raw_html": "<h3>Finetuning frameworks and base models</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "While many things around finetuning—deciding whether to finetune, acquiring data, and maintaining finetuned models—are hard, the actual process of finetuning is more straightforward. There are three things you need to choose: a base model, a finetuning method, and a framework for finetuning.",
      "raw_html": "<p>While many things around finetuning—deciding whether to finetune, acquiring data, and maintaining finetuned models—are hard, the actual process of finetuning is more straightforward. There are three things you need to choose: a base model, a finetuning method, and a framework for finetuning.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Base models",
      "id": "heading-7",
      "raw_html": "<h4>Base models</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Chapter 4 already covered the criteria for model selection that can be applied to both prompt-based methods and finetuning. Some of the criteria discussed include model size, licenses, and benchmark performance. At the beginning of an AI project, when you’re still exploring the feasibility of your task, it’s useful to start with the most powerful model you can afford. If this model struggles to produce good results, weaker models are likely to perform even worse. If the strongest model meets your needs, you can then explore weaker models, using the initial model as a benchmark for comparison.",
      "raw_html": "<p><a data-type=\"xref\" href=\"ch04.html#ch04_evaluate_ai_systems_1730130866187863\">Chapter 4</a> already covered the criteria for model selection that can be applied to both prompt-based methods and finetuning. Some of the criteria discussed include model size, licenses, and benchmark performance. At the beginning of an AI project, when you’re still exploring the feasibility of your task, it’s useful to start with the most powerful model you can afford. If this model struggles to produce good results, weaker models are likely to perform even worse. If the strongest model meets your needs, you can then explore weaker models, using the initial model as a benchmark for comparison.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "For finetuning, the starting models vary for different projects. OpenAI’s finetuning best practices document gives examples of two development paths: the progression path and the distillation path.",
      "raw_html": "<p>For finetuning, the starting models vary for different projects. <a href=\"https://oreil.ly/7I6Ch\">OpenAI’s finetuning best practices document</a> gives examples of two development paths: the progression path and the distillation path.<a contenteditable=\"false\" data-primary=\"OpenAI\" data-secondary=\"progression/distillation paths\" data-type=\"indexterm\" id=\"id1488\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The progression path looks like this:",
      "raw_html": "<p>The progression path looks like this:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Test your finetuning code using the cheapest and fastest model to make sure the code works as expected.35",
        "Test your data by finetuning a middling model. If the training loss doesn’t go down with more data, something might be wrong.",
        "Run a few more experiments with the best model to see how far you can push performance.",
        "Once you have good results, do a training run with all models to map out the price/performance frontier and select the model that makes the most sense for your use case."
      ],
      "raw_html": "<ol>\n<li><p>Test your finetuning code using the cheapest and fastest model to make sure the code works as expected.<sup><a data-type=\"noteref\" href=\"ch07.html#id1489\" id=\"id1489-marker\">35</a></sup></p>\n</li>\n<li><p>Test your data by finetuning a middling model. If the training loss doesn’t go down with more data, something might be wrong.</p>\n</li>\n<li><p>Run a few more experiments with the best model to see how far you can push performance.</p>\n</li>\n<li><p>Once you have good results, do a training run with all models to map out the price/performance frontier and select the model that makes the most sense for your use case.</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Test your finetuning code using the cheapest and fastest model to make sure the code works as expected.35",
      "raw_html": "<p>Test your finetuning code using the cheapest and fastest model to make sure the code works as expected.<sup><a data-type=\"noteref\" href=\"ch07.html#id1489\" id=\"id1489-marker\">35</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Test your data by finetuning a middling model. If the training loss doesn’t go down with more data, something might be wrong.",
      "raw_html": "<p>Test your data by finetuning a middling model. If the training loss doesn’t go down with more data, something might be wrong.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Run a few more experiments with the best model to see how far you can push performance.",
      "raw_html": "<p>Run a few more experiments with the best model to see how far you can push performance.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Once you have good results, do a training run with all models to map out the price/performance frontier and select the model that makes the most sense for your use case.",
      "raw_html": "<p>Once you have good results, do a training run with all models to map out the price/performance frontier and select the model that makes the most sense for your use case.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The distillation path might look as follows:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"distillation\" data-secondary=\"base\" data-type=\"indexterm\" id=\"id1490\"></a>The distillation path might look as follows:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Start with a small dataset and the strongest model you can afford. Train the best possible model with this small dataset. Because the base model is already strong, it requires less data to achieve good performance.",
        "Use this finetuned model to generate more training data.",
        "Use this new dataset to train a cheaper model."
      ],
      "raw_html": "<ol>\n<li><p>Start with a small dataset and the strongest model you can afford. Train the best possible model with this small dataset. Because the base model is already strong, it requires less data to achieve good performance.</p>\n</li>\n<li><p>Use this finetuned model to generate more training data.</p>\n</li>\n<li><p>Use this new dataset to train a cheaper model.</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Start with a small dataset and the strongest model you can afford. Train the best possible model with this small dataset. Because the base model is already strong, it requires less data to achieve good performance.",
      "raw_html": "<p>Start with a small dataset and the strongest model you can afford. Train the best possible model with this small dataset. Because the base model is already strong, it requires less data to achieve good performance.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Use this finetuned model to generate more training data.",
      "raw_html": "<p>Use this finetuned model to generate more training data.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Use this new dataset to train a cheaper model.",
      "raw_html": "<p>Use this new dataset to train a cheaper model.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because finetuning usually comes after experiments with prompt engineering, by the time you start to finetune, ideally, you should have a pretty good understanding of different models’ behaviors. You should plan your finetuning development path based on this understanding.",
      "raw_html": "<p>Because finetuning usually comes after experiments with prompt engineering, by the time you start to finetune, ideally, you should have a pretty good understanding of different models’ behaviors. You should plan your finetuning development path based on this understanding.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Finetuning methods",
      "id": "heading-7",
      "raw_html": "<h4>Finetuning methods</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Recall that adapter techniques like LoRA are cost-effective but typically don’t deliver the same level of performance as full finetuning. If you’re just starting with finetuning, try something like LoRA, and attempt full finetuning later.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"adapters\" data-secondary=\"finetuning\" data-type=\"indexterm\" id=\"id1491\"></a>Recall that adapter techniques like LoRA are cost-effective but typically don’t deliver the same level of performance as full finetuning. If you’re just starting with finetuning, try something like LoRA, and attempt full finetuning later.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The finetuning methods to use also depend on your data volume. Depending on the base model and the task, full finetuning typically requires at least thousands of examples and often many more. PEFT methods, however, can show good performance with a much smaller dataset. If you have a small dataset, such as a few hundred examples, full finetuning might not outperform LoRA.",
      "raw_html": "<p>The finetuning methods to use also depend on your data volume. Depending on the base model and the task, full finetuning typically requires at least thousands of examples and often many more. PEFT methods, however, can show good performance with a much smaller dataset. If you have a small dataset, such as a few hundred examples, full finetuning might not outperform LoRA.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Take into account how many finetuned models you need and how you want to serve them when deciding on a finetuning method. Adapter-based methods like LoRA allow you to more efficiently serve multiple models that share the same base model. With LoRA, you only need to serve a single full model, whereas full finetuning requires serving multiple full models.",
      "raw_html": "<p>Take into account how many finetuned models you need and how you want to serve them when deciding on a finetuning method. Adapter-based methods like LoRA allow you to more efficiently serve multiple models that share the same base model. With LoRA, you only need to serve a single full model, whereas full finetuning requires serving multiple full models.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Finetuning frameworks",
      "id": "heading-7",
      "raw_html": "<h4>Finetuning frameworks</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "The easiest way to finetune is to use a finetuning API where you can upload data, select a base model, and get back a finetuned model. Like model inference APIs, finetuning APIs can be provided by model providers, cloud service providers, and third-party providers. A limitation of this approach is that you’re limited to the base models that the API supports. Another limitation is that the API might not expose all the knobs you can use for optimal finetuning performance. Finetuning APIs are suitable for those who want something quick and easy, but they might be frustrating for those who want more customization.",
      "raw_html": "<p>The easiest way to finetune is to use a finetuning API where you can upload data, select a base model, and get back a finetuned model. Like model inference APIs, finetuning APIs can be provided by model providers, cloud service providers, and third-party providers. A limitation of this approach is that you’re limited to the base models that the API supports. Another limitation is that the API might not expose all the knobs you can use for optimal finetuning performance. Finetuning APIs are suitable for those who want something quick and easy, but they might be frustrating for those who want more customization.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "You can also finetune using one of many great finetuning frameworks available, such as LLaMA-Factory, unsloth, PEFT, Axolotl, and LitGPT. They support a wide range of finetuning methods, especially adapter-based techniques. If you want to do full finetuning, many base models provide their open source training code on GitHub that you can clone and run with your own data. Llama Police has a more comprehensive and up-to-date list of finetuning frameworks and model repositories.",
      "raw_html": "<p>You can also finetune using one of many great finetuning frameworks available, such as <a href=\"https://github.com/hiyouga/LLaMA-Factory\">LLaMA-Factory</a>, <a href=\"https://github.com/unslothai/unsloth\">unsloth</a>, <a href=\"https://github.com/huggingface/peft\">PEFT</a>, <a href=\"https://github.com/axolotl-ai-cloud/axolotl\">Axolotl</a>, and <a href=\"https://github.com/Lightning-AI/litgpt\">LitGPT</a>. They support a wide range of finetuning methods, especially adapter-based techniques. If you want to do full finetuning, many base models provide their open source training code on GitHub that you can clone and run with your own data. <a href=\"https://huyenchip.com/llama-police\">Llama Police</a> has a more comprehensive and up-to-date list of finetuning frameworks and model repositories.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Doing your own finetuning gives you more flexibility, but you’ll have to provision the necessary compute. If you do only adapter-based techniques, a mid-tier GPU might suffice for most models. If you need more compute, you can choose a framework that integrates seamlessly with your cloud provider.",
      "raw_html": "<p>Doing your own finetuning gives you more flexibility, but you’ll have to provision the necessary compute. If you do only adapter-based techniques, a mid-tier GPU might suffice for most models. If you need more compute, you can choose a framework that integrates seamlessly with your cloud provider.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "To finetune a model using more than one machine, you’ll need a framework that helps you do distributed training, such as DeepSpeed, PyTorch Distributed, and ColossalAI.",
      "raw_html": "<p>To finetune a model using more than one machine, you’ll need a framework that helps you do distributed training, such as <a href=\"https://github.com/microsoft/DeepSpeed\">DeepSpeed</a>, <a href=\"https://oreil.ly/hxUAk\">PyTorch Distributed</a>, and <a href=\"https://github.com/microsoft/DeepSpeed\">ColossalAI</a>. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Finetuning hyperparameters",
      "id": "heading-7",
      "raw_html": "<h3>Finetuning hyperparameters</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Depending on the base model and the finetuning method, there are many hyperparameters you can tune to improve finetuning efficiency. For specific hyperparameters for your use case, check out the documentation of the base model or the finetuning framework you use. Here, I’ll cover a few important hyperparameters that frequently appear.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"hyperparameters\" data-type=\"indexterm\" id=\"ch07.html55\"></a><a contenteditable=\"false\" data-primary=\"hyperparameters\" data-type=\"indexterm\" id=\"ch07.html56\"></a>Depending on the base model and the finetuning method, there are many hyperparameters you can tune to improve finetuning efficiency. For specific hyperparameters for your use case, check out the documentation of the base model or the finetuning framework you use. Here, I’ll cover a few important hyperparameters that frequently appear.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Learning rate",
      "id": "heading-7",
      "raw_html": "<h4>Learning rate</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "The learning rate determines how fast the model’s parameters should change with each learning step. If you think of learning as finding a path toward a goal, the learning rate is the step size. If the step size is too small, it might take too long to get to the goal. If the step size is too big, you might overstep the goal, and, hence, the model might never converge.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"hyperparameters\" data-tertiary=\"learning rate\" data-type=\"indexterm\" id=\"id1492\"></a><a contenteditable=\"false\" data-primary=\"learning rate\" data-type=\"indexterm\" id=\"id1493\"></a>The learning rate determines how fast the model’s parameters should change with each learning step. If you think of learning as finding a path toward a goal, the learning rate is the step size. If the step size is too small, it might take too long to get to the goal. If the step size is too big, you might overstep the goal, and, hence, the model might never converge.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A universal optimal learning rate doesn’t exist. You’ll have to experiment with different learning rates, typically between the range of 1e-7 to 1e-3, to see which one works best. A common practice is to take the learning rate at the end of the pre-training phase and multiply it with a constant between 0.1 and 1.",
      "raw_html": "<p>A universal optimal learning rate doesn’t exist. You’ll have to experiment with different learning rates, typically between the range of 1e-7 to 1e-3, to see which one works best. A common practice is to take the learning rate at the end of the pre-training phase and multiply it with a constant between 0.1 and 1.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The loss curve can give you hints about the learning rate. If the loss curve fluctuates a lot, it’s likely that the learning rate is too big. If the loss curve is stable but takes a long time to decrease, the learning is likely too small. Increase the learning rate as high as the loss curve remains stable.",
      "raw_html": "<p>The loss curve can give you hints about the learning rate. If the loss curve fluctuates a lot, it’s likely that the learning rate is too big. If the loss curve is stable but takes a long time to decrease, the learning is likely too small. Increase the learning rate as high as the loss curve remains stable.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "You can vary learning rates during the training process. You can use larger learning rates in the beginning and smaller learning rates near the end. Algorithms that determine how learning rates should change throughout the training process are called learning rate schedules.",
      "raw_html": "<p>You can vary learning rates during the training process. You can use larger learning rates in the beginning and smaller learning rates near the end. Algorithms that <span class=\"keep-together\">determine</span> how learning rates should change throughout the training process are called learning rate schedules.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Batch size",
      "id": "heading-7",
      "raw_html": "<h4>Batch size</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "The batch size determines how many examples a model learns from in each step to update its weights. A batch size that is too small, such as fewer than eight, can lead to unstable training.36 A larger batch size helps aggregate the signals from different examples, resulting in more stable and reliable updates.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"batching\" data-secondary=\"batch size\" data-type=\"indexterm\" id=\"id1494\"></a><a contenteditable=\"false\" data-primary=\"batch size\" data-type=\"indexterm\" id=\"id1495\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"hyperparameters\" data-tertiary=\"batch size\" data-type=\"indexterm\" id=\"id1496\"></a>The batch size determines how many examples a model learns from in each step to update its weights. A batch size that is too small, such as fewer than eight, can lead to unstable training.<sup><a data-type=\"noteref\" href=\"ch07.html#id1497\" id=\"id1497-marker\">36</a></sup> A larger batch size helps aggregate the signals from different examples, resulting in more stable and reliable updates.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In general, the larger the batch size, the faster the model can go through training examples. However, the larger the batch size, the more memory is needed to run your model. Thus, batch size is limited by the hardware you use.",
      "raw_html": "<p>In general, the larger the batch size, the faster the model can go through training examples. However, the larger the batch size, the more memory is needed to run your model. Thus, batch size is limited by the hardware you use.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This is where you see the cost versus efficiency trade-off. More expensive compute allows faster finetuning.",
      "raw_html": "<p>This is where you see the cost versus efficiency trade-off. More expensive compute allows faster finetuning.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "As of this writing, compute is still a bottleneck for finetuning. Often, models are so large, and memory is so constrained, that only small batch sizes can be used. This can lead to unstable model weight updates. To address this, instead of updating the model weights after each batch, you can accumulate gradients across several batches and update the model weights once enough reliable gradients are accumulated. This technique is called gradient accumulation.37",
      "raw_html": "<p>As of this writing, compute is still a bottleneck for finetuning. Often, models are so large, and memory is so constrained, that only small batch sizes can be used. This can lead to unstable model weight updates. To address this, instead of updating the model weights after each batch, you can accumulate gradients across several batches and update the model weights once enough reliable gradients are accumulated. This technique is called <em>gradient accumulation</em>.<sup><a data-type=\"noteref\" href=\"ch07.html#id1498\" id=\"id1498-marker\">37</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "When compute cost isn’t the most important factor, you can experiment with different batch sizes to see which gives the best model performance.",
      "raw_html": "<p>When compute cost isn’t the most important factor, you can experiment with different batch sizes to see which gives the best model performance.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Number of epochs",
      "id": "heading-7",
      "raw_html": "<h4>Number of epochs</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "An epoch is a pass over the training data. The number of epochs determines how many times each training example is trained on.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"epochs\" data-type=\"indexterm\" id=\"id1499\"></a><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"hyperparameters\" data-tertiary=\"number of epochs\" data-type=\"indexterm\" id=\"id1500\"></a>An epoch is a pass over the training data. The number of epochs determines how many times each training example is trained on.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Small datasets may need more epochs than large datasets. For a dataset with millions of examples, 1–2 epochs might be sufficient. A dataset with thousands of examples might still see performance improvement after 4–10 epochs.",
      "raw_html": "<p>Small datasets may need more epochs than large datasets. For a dataset with millions of examples, 1–2 epochs might be sufficient. A dataset with thousands of examples might still see performance improvement after 4–10 epochs.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The difference between the training loss and the validation loss can give you hints about epochs. If both the training loss and the validation loss still steadily decrease, the model can benefit from more epochs (and more data). If the training loss still decreases but the validation loss increases, the model is overfitting to the training data, and you might try lowering the number of epochs.",
      "raw_html": "<p>The difference between the training loss and the validation loss can give you hints about epochs. If both the training loss and the validation loss still steadily decrease, the model can benefit from more epochs (and more data). If the training loss still decreases but the validation loss increases, the model is overfitting to the training data, and you might try lowering the number of epochs.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Prompt loss weight",
      "id": "heading-7",
      "raw_html": "<h4 class=\"less_space\">Prompt loss weight</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "For instruction finetuning, each example consists of a prompt and a response, both of which can contribute to the model’s loss during training. During inference, however, prompts are usually provided by users, and the model only needs to generate responses. Therefore, response tokens should contribute more to the model’s loss during training than prompt tokens.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"finetuning\" data-secondary=\"hyperparameters\" data-tertiary=\"prompt loss rate\" data-type=\"indexterm\" id=\"id1501\"></a><a contenteditable=\"false\" data-primary=\"prompt loss rate\" data-type=\"indexterm\" id=\"id1502\"></a>For instruction finetuning, each example consists of a prompt and a response, both of which can contribute to the model’s loss during training. During inference, however, prompts are usually provided by users, and the model only needs to generate responses. Therefore, response tokens should contribute more to the model’s loss during training than prompt tokens.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The prompt model weight determines how much prompts should contribute to this loss compared to responses. If this weight is 100%, prompts contribute to the loss as much as responses, meaning that the model learns equally from both. If this weight is 0%, the model learns only from responses. Typically, this weight is set to 10% by default, meaning that the model should learn some from prompts but mostly from responses.",
      "raw_html": "<p>The prompt model weight determines how much prompts should contribute to this loss compared to responses. If this weight is 100%, prompts contribute to the loss as much as responses, meaning that the model learns equally from both. If this weight is 0%, the model learns only from responses. Typically, this weight is set to 10% by default, meaning that the model should learn some from prompts but mostly from <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html56\" data-type=\"indexterm\" id=\"id1503\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html55\" data-type=\"indexterm\" id=\"id1504\"></a>responses<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html54\" data-type=\"indexterm\" id=\"id1505\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html28\" data-type=\"indexterm\" id=\"id1506\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Summary",
      "id": "heading-7",
      "raw_html": "<h1>Summary</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "Outside of the evaluation chapters, finetuning has been the most challenging chapter to write. It touched on a wide range of concepts, both old (transfer learning) and new (PEFT), fundamental (low-rank factorization) and experimental (model merging), mathematical (memory calculation) and tactical (hyperparameter tuning). Arranging all these different aspects into a coherent structure while keeping them accessible was difficult.",
      "raw_html": "<p>Outside of the evaluation chapters, finetuning has been the most challenging chapter to write. It touched on a wide range of concepts, both old (transfer learning) and new (PEFT), fundamental (low-rank factorization) and experimental (model merging), mathematical (memory calculation) and tactical (hyperparameter tuning). Arranging all these different aspects into a coherent structure while keeping them accessible was difficult.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The process of finetuning itself isn’t hard. Many finetuning frameworks handle the training process for you. These frameworks can even suggest common finetuning methods with sensible default hyperparameters.",
      "raw_html": "<p>The process of finetuning itself isn’t hard. Many finetuning frameworks handle the training process for you. These frameworks can even suggest common finetuning methods with sensible default hyperparameters.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "However, the context surrounding finetuning is complex. It starts with whether you should even finetune a model. This chapter started with the reasons for finetuning and the reasons for not finetuning. It also discussed one question that I have been asked many times: when to finetune and when to do RAG.",
      "raw_html": "<p>However, the context surrounding finetuning is complex. It starts with whether you should even finetune a model. This chapter started with the reasons for finetuning and the reasons for not finetuning. It also discussed one question that I have been asked many times: when to finetune and when to do RAG.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In its early days, finetuning was similar to pre-training—both involved updating the model’s entire weights. However, as models increased in size, full finetuning became impractical for most practitioners. The more parameters to update during finetuning, the more memory finetuning needs. Most practitioners don’t have access to sufficient resources (hardware, time, and data) to do full finetuning with foundation models.",
      "raw_html": "<p>In its early days, finetuning was similar to pre-training—both involved updating the model’s entire weights. However, as models increased in size, full finetuning became impractical for most practitioners. The more parameters to update during finetuning, the more memory finetuning needs. Most practitioners don’t have access to sufficient resources (hardware, time, and data) to do full finetuning with foundation models.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Many finetuning techniques have been developed with the same motivation: to achieve strong performance on a minimal memory footprint. For example, PEFT reduces finetuning’s memory requirements by reducing the number of trainable parameters. Quantized training, on the other hand, mitigates this memory bottleneck by reducing the number of bits needed to represent each value.",
      "raw_html": "<p>Many finetuning techniques have been developed with the same motivation: to achieve strong performance on a minimal memory footprint. For example, PEFT reduces finetuning’s memory requirements by reducing the number of trainable parameters. Quantized training, on the other hand, mitigates this memory bottleneck by reducing the number of bits needed to represent each value.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "After giving an overview of PEFT, the chapter zoomed into LoRA—why and how it works. LoRA has many properties that make it popular among practitioners. On top of being parameter-efficient and data-efficient, it’s also modular, making it much easier to serve and combine multiple LoRA models.",
      "raw_html": "<p>After giving an overview of PEFT, the chapter zoomed into LoRA—why and how it works. LoRA has many properties that make it popular among practitioners. On top of being parameter-efficient and data-efficient, it’s also modular, making it much easier to serve and combine multiple LoRA models.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The idea of combining finetuned models brought the chapter to model merging; its goal is to combine multiple models into one model that works better than these models separately. This chapter discussed the many use cases of model merging, from on-device deployment to model upscaling, and general approaches to model merging.",
      "raw_html": "<p>The idea of combining finetuned models brought the chapter to model merging; its goal is to combine multiple models into one model that works better than these models separately. This chapter discussed the many use cases of model merging, from on-device deployment to model upscaling, and general approaches to model merging.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A comment I often hear from practitioners is that finetuning is easy, but getting data for finetuning is hard. Obtaining high-quality annotated data, especially instruction data, is challenging. The next chapter will dive into these challenges.",
      "raw_html": "<p>A comment I often hear from practitioners is that finetuning is easy, but getting data for finetuning is hard. Obtaining high-quality annotated data, especially instruction data, is challenging. The next chapter will dive into these challenges.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch07.html0\" data-type=\"indexterm\" id=\"id1507\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "1 Some people call this phenomenon an alignment tax (Bai et al., 2020), but this term can be confused with penalties against human preference alignment.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1369\"><sup><a href=\"ch07.html#id1369-marker\">1</a></sup> Some people call this phenomenon an alignment tax (<a href=\"https://arxiv.org/abs/2204.05862\">Bai et al., 2020</a>), but this term can be confused with penalties against human preference alignment.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "2 Many businesses resist changing technologies they consider “good enough.” If all companies were quick to adopt more optimal solutions, fax machines would have become obsolete by now.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1370\"><sup><a href=\"ch07.html#id1370-marker\">2</a></sup> Many businesses resist changing technologies they consider “good enough.” If all companies were quick to adopt more optimal solutions, fax machines would have become obsolete by now.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "3 I’ve also noticed a few cases when engineers know that finetuning isn’t strictly necessary but still insist on doing it because they want to learn how to finetune. As an engineer who likes learning new skills, I appreciate this mindset. However, if you’re in a leadership position, it can be hard to differentiate whether finetuning is needed or wanted.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1371\"><sup><a href=\"ch07.html#id1371-marker\">3</a></sup> I’ve also noticed a few cases when engineers know that finetuning isn’t strictly necessary but still insist on doing it because they want to learn how to finetune. As an engineer who likes learning new skills, I appreciate this mindset. However, if you’re in a leadership position, it can be hard to differentiate whether finetuning is needed or wanted.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "4 0314 denotes the date this GPT-4 version came out, March 14, 2024. The specific date stamp matters because different versions vary significantly in performance.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1374\"><sup><a href=\"ch07.html#id1374-marker\">4</a></sup> 0314 denotes the date this GPT-4 version came out, March 14, 2024. The specific date stamp matters because different versions vary significantly in performance.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "5 Some people, such as the authors of the Llama 3.1 paper (Dubey et al., 2024), adhere to “the principle that post-training should align the model to ‘know what it knows’ rather than add knowledge.”",
      "raw_html": "<p data-type=\"footnote\" id=\"id1377\"><sup><a href=\"ch07.html#id1377-marker\">5</a></sup> Some people, such as the authors of the Llama 3.1 paper (<a href=\"https://arxiv.org/abs/2407.21783\">Dubey et al., 2024</a>), adhere to “the principle that post-training should align the model to ‘know what it knows’ rather than add knowledge.”</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "6 Other than backpropagation, a promising approach to training neural networks is evolutionary strategy. One example, described by Maheswaranathan et al., combines random search with surrogate gradients, instead of using real gradients, to update model weights. Another interesting approach is direct feedback alignment (Arild Nøkland, 2016).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1382\"><sup><a href=\"ch07.html#id1382-marker\">6</a></sup> Other than backpropagation, a promising approach to training neural networks is evolutionary strategy. One example, described by <a href=\"https://oreil.ly/B59ci\">Maheswaranathan et al.</a>, combines random search with surrogate gradients, instead of using real gradients, to update model weights. Another interesting approach is direct feedback alignment (<a href=\"https://arxiv.org/abs/1609.01596\">Arild Nøkland, 2016</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "7 If a parameter is not trainable, it doesn’t need to be updated and, therefore, there’s no need to compute its gradient.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1383\"><sup><a href=\"ch07.html#id1383-marker\">7</a></sup> If a parameter is not trainable, it doesn’t need to be updated and, therefore, there’s no need to compute its gradient.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "8 Some might say that you’re not doing AI until you’ve seen a “RuntimeError: CUDA out of memory” error.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1389\"><sup><a href=\"ch07.html#id1389-marker\">8</a></sup> Some might say that you’re not doing AI until you’ve seen a “RuntimeError: CUDA out of memory” error.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "9 To learn more about inference memory calculation, check out Carol Chen’s “Transformer Inference Arithmetic”, kipply’s blog (March 2022).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1390\"><sup><a href=\"ch07.html#id1390-marker\">9</a></sup> To learn more about inference memory calculation, check out Carol Chen’s <a href=\"https://oreil.ly/u7wYx\">“Transformer Inference Arithmetic”</a>, kipply’s blog (March 2022).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "10 To learn more about training memory calculation, check out EleutherAI’s “Transformer Math 101” (Anthony et al., April 2023).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1395\"><sup><a href=\"ch07.html#id1395-marker\">10</a></sup> To learn more about training memory calculation, check out EleutherAI’s <a href=\"https://oreil.ly/Xe7h6\">“Transformer Math 101”</a> (Anthony et al., April 2023).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "11 Google introduced BFloat16 as “the secret to high performance on Cloud TPUs”.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1396\"><sup><a href=\"ch07.html#id1396-marker\">11</a></sup> Google introduced BFloat16 as <a href=\"https://oreil.ly/atIgi\">“the secret to high performance on Cloud TPUs”</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "12 Integer formats are also called fixed point formats.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1397\"><sup><a href=\"ch07.html#id1397-marker\">12</a></sup> Integer formats are also called <em>fixed point</em> formats.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "13 Range bits are called exponents. Precision bits are called significands.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1398\"><sup><a href=\"ch07.html#id1398-marker\">13</a></sup> Range bits are called <em>exponents</em>. Precision bits are called significand<em>s</em>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "14 Note that usually the number at the end of a format’s name signifies how many bits it occupies, but TF32 actually has 19 bits, not 32 bits. I believe it was named so to suggest its functional compatibility with FP32. But honestly, why it’s called TF32 and not TF19 keeps me up at night. An ex-coworker at NVIDIA volunteered his conjecture that people might be skeptical of weird formats (19-bit), so naming this format TF32 makes it look more friendly.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1401\"><sup><a href=\"ch07.html#id1401-marker\">14</a></sup> Note that usually the number at the end of a format’s name signifies how many bits it occupies, but TF32 actually has 19 bits, not 32 bits. I believe it was named so to suggest its functional compatibility with FP32. But honestly, why it’s called TF32 and not TF19 keeps me up at night. An ex-coworker at NVIDIA volunteered his conjecture that people might be skeptical of weird formats (19-bit), so naming this format TF32 makes it look more friendly.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "15 The FP16 and BF16 confusion continued with Llama 3.1. See X and Threads discussions: 1; 2, 3, 4; and llama.cpp’s benchmark between BF16 and FP16, Bloke’s writeup, and Raschka’s writeup.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1404\"><sup><a href=\"ch07.html#id1404-marker\">15</a></sup> The FP16 and BF16 confusion continued with Llama 3.1. See X and Threads discussions: <a href=\"https://en.wikipedia.org/wiki/IEEE_754\">1</a>; <a href=\"https://x.com/abacaj/status/1695334296792264792?s=20\">2</a>, <a href=\"https://oreil.ly/U8L4d\">3</a>, <a href=\"https://oreil.ly/8ush1\">4</a>; and llama.cpp’s <a href=\"https://github.com/ggerganov/llama.cpp/pull/7150\">benchmark between BF16 and FP16</a>, <a href=\"https://oreil.ly/0vuze\">Bloke’s writeup</a>, and <a href=\"https://oreil.ly/WK_zT\">Raschka’s writeup</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "16 Designing numerical formats is a fascinating discipline. Being able to create a lower-precision format that doesn’t compromise a system’s quality can make that system much cheaper and faster, enabling new use cases.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1405\"><sup><a href=\"ch07.html#id1405-marker\">16</a></sup> Designing numerical formats is a fascinating discipline. Being able to create a lower-precision format that doesn’t compromise a system’s quality can make that system much cheaper and faster, enabling new use cases. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "17 Another major contributor to the memory footprint of transformer-based models is the KV cache, which is discussed in Chapter 9.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1406\"><sup><a href=\"ch07.html#id1406-marker\">17</a></sup> Another major contributor to the memory footprint of transformer-based models is the KV cache, which is discussed in <a data-type=\"xref\" href=\"ch09.html#ch09_inference_optimization_1730130963006301\">Chapter 9</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "18 The smallest possible float size that follows all IEEE principles is 4-bit.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1407\"><sup><a href=\"ch07.html#id1407-marker\">18</a></sup> The smallest possible float size that follows all IEEE principles is 4-bit.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "19 The authors of the Xnor-Net paper spun off Xnor.ai, a startup that focused on model compression. In early 2020, it was acquired by Apple for a reported $200M.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1408\"><sup><a href=\"ch07.html#id1408-marker\">19</a></sup> The authors of the Xnor-Net paper spun off Xnor.ai, a startup that focused on model compression. <a href=\"https://oreil.ly/V4pma\">In early 2020, it was acquired by Apple for a reported $200M</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "20 During training, the model’s weights are updated via multiple steps. Small rounding changes can compound during the training process, making it difficult for the model to achieve the desirable performance. On top of that, loss values require precise computation. Small changes in the loss value can point parameter updates in the wrong direction.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1415\"><sup><a href=\"ch07.html#id1415-marker\">20</a></sup> During training, the model’s weights are updated via multiple steps. Small rounding changes can compound during the training process, making it difficult for the model to achieve the desirable performance. On top of that, loss values require precise computation. Small changes in the loss value can point parameter updates in the wrong direction.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "21 Personal anecdote: much of my team’s work at NVIDIA was on mixed precision training. See “Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq” (Huyen et al., NVIDIA Developer Technical Blog, October 2018).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1416\"><sup><a href=\"ch07.html#id1416-marker\">21</a></sup> Personal anecdote: much of my team’s work at NVIDIA was on mixed precision training. See <a href=\"https://oreil.ly/QL2gL\">“Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq”</a> (Huyen et al., NVIDIA Developer Technical Blog, October 2018). </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "22 In partial finetuning, it’s common to finetune the layers closest to the output layer because those layers are usually more task-specific, whereas earlier layers tend to capture more general features.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1429\"><sup><a href=\"ch07.html#id1429-marker\">22</a></sup> In partial finetuning, it’s common to finetune the layers closest to the output layer because those layers are usually more task-specific, whereas earlier layers tend to capture more general features.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "23 I’ve never met a single person who could explain to me, on the spot, the differences between these techniques.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1431\"><sup><a href=\"ch07.html#id1431-marker\">23</a></sup> I’ve never met a single person who could explain to me, on the spot, the differences between these techniques.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "24 To effectively use LoRA for a model, it’s necessary to understand that model’s architecture. Chapter 2 already covered the weight composition of some transformer-based models. For the exact weight composition of a model, refer to its paper.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1439\"><sup><a href=\"ch07.html#id1439-marker\">24</a></sup> To effectively use LoRA for a model, it’s necessary to understand that model’s architecture. <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a> already covered the weight composition of some transformer-based models. For the exact weight composition of a model, refer to its paper.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "25 As of this writing, some finetuning frameworks like Fireworks only allow a maximum LoRA rank of 32. However, this constraint is unlikely due to performance and more likely due to their hardware’s memory constraint.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1441\"><sup><a href=\"ch07.html#id1441-marker\">25</a></sup> As of this writing, some finetuning frameworks like <a href=\"https://oreil.ly/82-jJ\">Fireworks</a> only allow a maximum LoRA rank of 32. However, this constraint is unlikely due to performance and more likely due to their hardware’s memory constraint.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "26 Search for these adapters by tags “adapter”, “peft”, or “LoRA”.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1444\"><sup><a href=\"ch07.html#id1444-marker\">26</a></sup> Search for these adapters by tags “adapter”, “peft”, or “LoRA”.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "27 QLoRA isn’t the only quantized LoRA work. Many research labs have been working on quantized LoRA without publicly discussing it.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1447\"><sup><a href=\"ch07.html#id1447-marker\">27</a></sup> QLoRA isn’t the only quantized LoRA work. Many research labs have been working on quantized LoRA without publicly discussing it.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "28 My book, Designing Machine Learning Systems has a section on “ML on the Cloud and on the Edge.”",
      "raw_html": "<p data-type=\"footnote\" id=\"id1463\"><sup><a href=\"ch07.html#id1463-marker\">28</a></sup> My book, <a class=\"orm:hideurl\" href=\"https://oreil.ly/u_cVP\"><em>Designing Machine Learning Systems</em></a> has a section on “ML on the Cloud and on the Edge.”</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "29 You can read more about ensemble methods in my book Designing Machine Learning Systems.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1465\"><sup><a href=\"ch07.html#id1465-marker\">29</a></sup> You can read more about ensemble methods in my book <a class=\"orm:hideurl\" href=\"https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/\"><em>Designing Machine Learning Systems</em></a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "30 Averaging works not just with weights but also with embeddings. For example, given a sentence, you can use a word embedding algorithm to generate an embedding vector for each word in the sentence, then average all these word embeddings into a sentence embedding. When I started out in ML, I couldn’t believe that averaging seems to just work. It’s magical when simple components, when used correctly, can create something so wonderfully perplexing, like AI.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1466\"><sup><a href=\"ch07.html#id1466-marker\">30</a></sup> Averaging works not just with weights but also with embeddings. For example, given a sentence, you can use a word embedding algorithm to generate an embedding vector for each word in the sentence, then average all these word embeddings into a sentence embedding. When I started out in ML, I couldn’t believe that averaging seems to just work. It’s magical when simple components, when used correctly, can create something so wonderfully perplexing, like AI.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "31 The assumption is that the parameters that undergo the most substantial changes during finetuning are the ones most crucial for the target task.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1473\"><sup><a href=\"ch07.html#id1473-marker\">31</a></sup> The assumption is that the parameters that undergo the most substantial changes during finetuning are the ones most crucial for the target task.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "32 TIES is abbreviated from “TrIm, Elect Sign, and merge,” while DARE is from “Drop And REscale.” I know, these abbreviations pain me too.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1474\"><sup><a href=\"ch07.html#id1474-marker\">32</a></sup> TIES is abbreviated from “TrIm, Elect Sign, and merge,” while DARE is from “Drop And REscale.” I know, these abbreviations pain me too.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "33 When task vectors are pruned, they become more sparse, but the finetuned model doesn’t. Pruning, in this case, isn’t to reduce the memory footprint or inference latency, but to improve performance.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1477\"><sup><a href=\"ch07.html#id1477-marker\">33</a></sup> When task vectors are pruned, they become more sparse, but the finetuned model doesn’t. Pruning, in this case, isn’t to reduce the memory footprint or inference latency, but to improve performance.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "34 I debated for a long time whether to include the concatenation technique in this book, and decided to include it for completeness.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1487\"><sup><a href=\"ch07.html#id1487-marker\">34</a></sup> I debated for a long time whether to include the concatenation technique in this book, and decided to include it for completeness.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "35 In college, I made the painful mistake of letting my model train overnight, only to have it crash after eight hours because I tried to save the checkpoint in a nonexistent folder. All that progress was lost.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1489\"><sup><a href=\"ch07.html#id1489-marker\">35</a></sup> In college, I made the painful mistake of letting my model train overnight, only to have it crash after eight hours because I tried to save the checkpoint in a nonexistent folder. All that progress was lost.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "36 While it’s commonly acknowledged that small batch sizes lead to unstable training, I wasn’t able to find good explanations for why that’s the case. If you have references about this, please feel free to send them my way.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1497\"><sup><a href=\"ch07.html#id1497-marker\">36</a></sup> While it’s commonly acknowledged that small batch sizes lead to unstable training, I wasn’t able to find good explanations for why that’s the case. If you have references about this, please feel free to send them my way.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "37 I tried to find the first paper where gradient accumulation was introduced but couldn’t. Its use in deep learning was mentioned as early as 2016 in “Ako: Decentralised Deep Learning with Partial Gradient Exchange” (Watcharapichat et al., Proceedings of the Seventh ACM Symposium on Cloud Computing, 2016). The concept seems to come from distributed training, where gradients computed on different machines need to be accumulated and used to update the model’s weights.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1498\"><sup><a href=\"ch07.html#id1498-marker\">37</a></sup> I tried to find the first paper where gradient accumulation was introduced but couldn’t. Its use in deep learning was mentioned as early as 2016 in <a href=\"https://oreil.ly/GFeC7\">“Ako: Decentralised Deep Learning with Partial Gradient Exchange”</a> (Watcharapichat et al., <em>Proceedings of the Seventh ACM Symposium on Cloud Computing</em>, 2016). The concept seems to come from distributed training, where gradients computed on different machines need to be accumulated and used to update the model’s weights.</p>",
      "contains_links": true,
      "contains_emphasis": true
    }
  ],
  "metadata": {
    "word_count": 18014,
    "reading_time_minutes": 90,
    "extraction_date": "2025-06-29T12:20:44.056100"
  },
  "content_summary": {
    "figures": [],
    "tables": [],
    "code_blocks": [],
    "headings": [],
    "images": []
  },
  "cross_references": {
    "internal_links": [
      {
        "target": "ch07a_figure_1_1730160615799658",
        "text": "Figure 7-1",
        "context": "Figure 7-1 shows the making of different Code Llama models ("
      },
      {
        "target": "ch07a_table_1_1730160615803945",
        "text": "Table 7-1",
        "context": "BloombergGPT across various financial benchmarks. Table 7-1 provides details of two such benchmarks."
      },
      {
        "target": "ch07a_figure_2_1730160615799676",
        "text": "Figure 7-2",
        "context": "ter prompts with the finetuned model, as shown in Figure 7-2."
      },
      {
        "target": "ch07a_table_2_1730160615803962",
        "text": "Table 7-2",
        "context": "tperformed RAG with finetuned models, as shown in Table 7-2. This finding indicates that while finetuning can"
      },
      {
        "target": "ch07a_figure_3_1730160615799691",
        "text": "Figure 7-3",
        "context": "ere’s no universal workflow for all applications. Figure 7-3 shows some paths an application development proce"
      },
      {
        "target": "ch07b_figure_1_1730159634220258",
        "text": "Figure 7-4",
        "context": "ne nonlinear activation function is visualized in Figure 7-4. I use this dummy neural network to simplify the"
      },
      {
        "target": "ch07b_figure_2_1730159634220278",
        "text": "Figure 7-5",
        "context": "dwarf the memory needed for the model’s weights. Figure 7-5 shows the memory needed for activations compared"
      },
      {
        "target": "ch07b_figure_3_1730159634220288",
        "text": "Figure 7-6",
        "context": "Figure 7-6 shows different floating point formats along with"
      },
      {
        "target": "ch07b_table_1_1730159634233580",
        "text": "Table 7-3",
        "context": "can cause a value to change or result in errors. Table 7-3 shows how FP32 values can be converted into FP16,"
      },
      {
        "target": "ch07b_table_1_1730159634233580",
        "text": "Table 7-3",
        "context": "Note in Table 7-3 that even though BF16 and FP16 have the same numb"
      },
      {
        "target": "ch07b_memory_math_1730159634259402",
        "text": "“Memory Math”",
        "context": "out hurting performance too much. As discussed in “Memory Math”, major contributors to a model’s memory footprint"
      },
      {
        "target": "ch07b_table_2_1730159634233604",
        "text": "Table 7-4",
        "context": "et al., 2023) up to 3.9B parameters, as shown in Table 7-4."
      },
      {
        "target": "ch07b_memory_math_1730159634259402",
        "text": "“Memory Math”",
        "context": "As discussed in “Memory Math”, the more trainable parameters there are, the mor"
      },
      {
        "target": "ch07b_figure_4_1730159634220299",
        "text": "Figure 7-7",
        "context": "tuning on the GLUE benchmark (Wang et al., 2018). Figure 7-7 shows the performance curve of partial finetuning"
      },
      {
        "target": "ch07b_figure_5_1730159634220312",
        "text": "Figure 7-8",
        "context": "ch transformer block of a BERT model, as shown in Figure 7-8."
      },
      {
        "target": "ch07b_figure_4_1730159634220299",
        "text": "Figure 7-7",
        "context": "umber of trainable parameters. The orange line in Figure 7-7 shows the performance delta between full finetuni"
      },
      {
        "target": "ch07b_figure_6_1730159634220324",
        "text": "Figure 7-9",
        "context": "ssover between prompt engineering and finetuning. Figure 7-9 visualizes how you can use soft prompts together"
      },
      {
        "target": "ch07b_figure_7_1730159634220334",
        "text": "Figure 7-10",
        "context": "ikely to report issues or ask questions about it. Figure 7-10 shows the result. For “P-Tuning”, I searched for"
      },
      {
        "target": "ch07b_figure_8_1730159634220345",
        "text": "Figure 7-11",
        "context": "Figure 7-11 visualizes this process."
      },
      {
        "target": "ch07b_table_3_1730159634233616",
        "text": "Table 7-5",
        "context": "ge Inference) benchmarks (Williams et al., 2017). Table 7-5 shows their results. However, the authors suggest"
      },
      {
        "target": "ch07b_figure_9_1730159634220354",
        "text": "Figure 7-12",
        "context": "tiple LoRA models that share the same base model. Figure 7-12 visualizes multi-LoRA serving if you keep the LoR"
      },
      {
        "target": "ch07b_table_4_1730159634233626",
        "text": "Table 7-6",
        "context": "rameters even further. However, as illustrated in Table 7-6, the memory of a LoRA adapter is minimal compared"
      },
      {
        "target": "ch07b_table_5_1730159634233637",
        "text": "Table 7-7",
        "context": "oth public benchmarks and comparative evaluation. Table 7-7 shows the Elo ratings of Guanaco models, GPT-4, a"
      },
      {
        "target": "ch07b_figure_10_1730159634220361",
        "text": "Figure 7-13",
        "context": "Figure 7-13 compares ensembling and model merging. Just like"
      },
      {
        "target": "ch07b_figure_11_1730159634220368",
        "text": "Figure 7-14",
        "context": "e are summing, layer stacking, and concatenation. Figure 7-14 shows their high-level differences."
      },
      {
        "target": "ch07b_figure_12_1730159634220376",
        "text": "Figure 7-15",
        "context": "Figure 7-15 shows how to linearly combine two layers when wA"
      },
      {
        "target": "ch07b_figure_13_1730159634220389",
        "text": "Figure 7-16",
        "context": "y halfway. This middle point is the blue point in Figure 7-16."
      },
      {
        "target": "ch07b_figure_14_1730159634220402",
        "text": "Figure 7-17",
        "context": "with minimal performance degradation, as shown in Figure 7-17. Resetting means changing the finetuned parameter"
      },
      {
        "target": "ch07b_figure_15_1730159634220410",
        "text": "Figure 7-18",
        "context": "long with the router to refine their performance. Figure 7-18 illustrates this process."
      },
      {
        "target": "ch07b_figure_16_1730159634220419",
        "text": "Figure 7-19",
        "context": "Figure 7-19 illustrates this process."
      },
      {
        "target": "ch07b_figure_17_1730159634220429",
        "text": "Figure 7-20",
        "context": "erged adapter’s rank will be r1 + r2, as shown in Figure 7-20."
      }
    ],
    "figure_references": [
      {
        "reference": "Figure 7-1",
        "figure_id": "7-1",
        "position": 9079
      },
      {
        "reference": "Figure 7-1",
        "figure_id": "7-1",
        "position": 9825
      },
      {
        "reference": "Figure 7-2",
        "figure_id": "7-2",
        "position": 20809
      },
      {
        "reference": "Figure 7-2",
        "figure_id": "7-2",
        "position": 21162
      },
      {
        "reference": "Figure 7-3",
        "figure_id": "7-3",
        "position": 26171
      },
      {
        "reference": "Figure 7-3",
        "figure_id": "7-3",
        "position": 26381
      },
      {
        "reference": "Figure 7-4",
        "figure_id": "7-4",
        "position": 33254
      },
      {
        "reference": "Figure 7-4",
        "figure_id": "7-4",
        "position": 33331
      },
      {
        "reference": "Figure 7-5",
        "figure_id": "7-5",
        "position": 37382
      },
      {
        "reference": "Figure 7-5",
        "figure_id": "7-5",
        "position": 38007
      },
      {
        "reference": "Figure 7-6",
        "figure_id": "7-6",
        "position": 40466
      },
      {
        "reference": "Figure 7-6",
        "figure_id": "7-6",
        "position": 40563
      },
      {
        "reference": "Figure 7-7",
        "figure_id": "7-7",
        "position": 54877
      },
      {
        "reference": "Figure 7-7",
        "figure_id": "7-7",
        "position": 54988
      },
      {
        "reference": "Figure 7-8",
        "figure_id": "7-8",
        "position": 56055
      },
      {
        "reference": "Figure 7-8",
        "figure_id": "7-8",
        "position": 56069
      },
      {
        "reference": "Figure 7-7",
        "figure_id": "7-7",
        "position": 56647
      },
      {
        "reference": "Figure 7-9",
        "figure_id": "7-9",
        "position": 59563
      },
      {
        "reference": "Figure 7-9",
        "figure_id": "7-9",
        "position": 59673
      },
      {
        "reference": "Figure 7-10",
        "figure_id": "7-10",
        "position": 60601
      },
      {
        "reference": "Figure 7-10",
        "figure_id": "7-10",
        "position": 60735
      },
      {
        "reference": "Figure 7-11",
        "figure_id": "7-11",
        "position": 62613
      },
      {
        "reference": "Figure 7-11",
        "figure_id": "7-11",
        "position": 62653
      },
      {
        "reference": "Figure 7-12",
        "figure_id": "7-12",
        "position": 72717
      },
      {
        "reference": "Figure 7-12",
        "figure_id": "7-12",
        "position": 72801
      },
      {
        "reference": "Figure 7-13",
        "figure_id": "7-13",
        "position": 83503
      },
      {
        "reference": "Figure 7-13",
        "figure_id": "7-13",
        "position": 83694
      },
      {
        "reference": "Figure 7-14",
        "figure_id": "7-14",
        "position": 84152
      },
      {
        "reference": "Figure 7-14",
        "figure_id": "7-14",
        "position": 84202
      },
      {
        "reference": "Figure 7-15",
        "figure_id": "7-15",
        "position": 85029
      },
      {
        "reference": "Figure 7-15",
        "figure_id": "7-15",
        "position": 85102
      },
      {
        "reference": "Figure 7-16",
        "figure_id": "7-16",
        "position": 88855
      },
      {
        "reference": "Figure 7-16",
        "figure_id": "7-16",
        "position": 89147
      },
      {
        "reference": "Figure 7-17",
        "figure_id": "7-17",
        "position": 89957
      },
      {
        "reference": "Figure 7-17",
        "figure_id": "7-17",
        "position": 90234
      },
      {
        "reference": "Figure 7-18",
        "figure_id": "7-18",
        "position": 92115
      },
      {
        "reference": "Figure 7-18",
        "figure_id": "7-18",
        "position": 92474
      },
      {
        "reference": "Figure 7-19",
        "figure_id": "7-19",
        "position": 93761
      },
      {
        "reference": "Figure 7-19",
        "figure_id": "7-19",
        "position": 93801
      },
      {
        "reference": "Figure 7-20",
        "figure_id": "7-20",
        "position": 94333
      },
      {
        "reference": "Figure 7-20",
        "figure_id": "7-20",
        "position": 94348
      },
      {
        "reference": "as shown in Figure 7-2",
        "figure_id": "7-2",
        "position": 20797
      },
      {
        "reference": "as shown in Figure 7-8",
        "figure_id": "7-8",
        "position": 56043
      },
      {
        "reference": "as shown in Figure 7-17",
        "figure_id": "7-17",
        "position": 89945
      },
      {
        "reference": "as shown in Figure 7-20",
        "figure_id": "7-20",
        "position": 94321
      }
    ],
    "table_references": [
      {
        "reference": "Table 7-1",
        "table_id": "7-1",
        "position": 19312
      },
      {
        "reference": "Table 7-1",
        "table_id": "7-1",
        "position": 19364
      },
      {
        "reference": "Table 7-2",
        "table_id": "7-2",
        "position": 22582
      },
      {
        "reference": "Table 7-2",
        "table_id": "7-2",
        "position": 22754
      },
      {
        "reference": "Table 7-3",
        "table_id": "7-3",
        "position": 40893
      },
      {
        "reference": "Table 7-3",
        "table_id": "7-3",
        "position": 40970
      },
      {
        "reference": "Table 7-3",
        "table_id": "7-3",
        "position": 41541
      },
      {
        "reference": "Table 7-4",
        "table_id": "7-4",
        "position": 46125
      },
      {
        "reference": "Table 7-4",
        "table_id": "7-4",
        "position": 46137
      },
      {
        "reference": "Table 7-5",
        "table_id": "7-5",
        "position": 69576
      },
      {
        "reference": "Table 7-5",
        "table_id": "7-5",
        "position": 69754
      },
      {
        "reference": "Table 7-6",
        "table_id": "7-6",
        "position": 75739
      },
      {
        "reference": "Table 7-6",
        "table_id": "7-6",
        "position": 75944
      },
      {
        "reference": "Table 7-7",
        "table_id": "7-7",
        "position": 77412
      },
      {
        "reference": "Table 7-7",
        "table_id": "7-7",
        "position": 77594
      }
    ],
    "section_references": [],
    "external_links": [
      {
        "url": "https://github.com/chiphuyen/aie-book",
        "text": "GitHub repository",
        "context": "his book. If you want a quick refresh, the book’s GitHub repository has pointers to helpful resources. In this chapte"
      },
      {
        "url": "https://oreil.ly/Udw0Z",
        "text": "Bozinovski and Fulgosi",
        "context": "transfer learning, a concept first introduced by Bozinovski and Fulgosi in 1976. Transfer learning focuses on how to tran"
      },
      {
        "url": "https://arxiv.org/abs/1611.04558",
        "text": "Johnson et. al, 2016",
        "context": "ing was Google’s multilingual translation system (Johnson et. al, 2016). The model transferred its knowledge of Portugue"
      },
      {
        "url": "https://oreil.ly/5-5lw",
        "text": "InstructGPT paper",
        "context": "uning just refines the model’s behavior. OpenAI’s InstructGPT paper (2022) suggested viewing finetuning as unlocking"
      },
      {
        "url": "https://arxiv.org/abs/2308.12950",
        "text": "Rozière et al., 2024",
        "context": "shows the making of different Code Llama models (Rozière et al., 2024), from the base model Llama 2, using different fi"
      },
      {
        "url": "https://oreil.ly/iPwB_",
        "text": "Wang and Russakovsky, 2023",
        "context": "ta during finetuning can counteract these biases (Wang and Russakovsky, 2023). For example, if a model consistently assigns CE"
      },
      {
        "url": "https://oreil.ly/RoPL4",
        "text": "Garimella et al. (2022)",
        "context": "set with many female CEOs can mitigate this bias. Garimella et al. (2022) found that finetuning BERT-like language models o"
      },
      {
        "url": "https://arxiv.org/abs/2210.11416",
        "text": "Chung et al., 2022",
        "context": "mmarly found that their finetuned Flan-T5 models (Chung et al., 2022) outperformed a GPT-3 variant specialized in text"
      },
      {
        "url": "https://arxiv.org/abs/2303.17564",
        "text": "Wu et al., 2023",
        "context": "3 million and $2.6 million, excluding data costs (Wu et al., 2023)."
      },
      {
        "url": "https://arxiv.org/abs/2305.05862",
        "text": "Li et al. (2023)",
        "context": "e month, OpenAI released GPT-4-0314.4 Research by Li et al. (2023) demonstrated that GPT-4-0314 significantly outper"
      },
      {
        "url": "https://oreil.ly/J-soV",
        "text": "Claude 3.5 Sonnet",
        "context": "comparable to GPT-4 have been released, including Claude 3.5 Sonnet (70B parameters), Llama 3-70B-Instruct, and Qwen2-"
      },
      {
        "url": "https://oreil.ly/6lt6-",
        "text": "Llama 3-70B-Instruct",
        "context": "ed, including Claude 3.5 Sonnet (70B parameters), Llama 3-70B-Instruct, and Qwen2-72B-Instruct. The latter two are open"
      },
      {
        "url": "https://oreil.ly/HZnfa",
        "text": "Qwen2-72B-Instruct",
        "context": "onnet (70B parameters), Llama 3-70B-Instruct, and Qwen2-72B-Instruct. The latter two are open weight and can be self-h"
      },
      {
        "url": "https://oreil.ly/t9HTH",
        "text": "“Fine-Tuning or Retrieval?”",
        "context": "The paper “Fine-Tuning or Retrieval?” by Ovadia et al. (2024) demonstrated that for tas"
      },
      {
        "url": "https://arxiv.org/abs/2009.03300",
        "text": "MMLU benchmark",
        "context": "ed that for almost all question categories in the MMLU benchmark, RAG outperforms finetuning for three different m"
      },
      {
        "url": "https://oreil.ly/t9HTH",
        "text": "Ovadia et al. (2024)",
        "context": "pplication’s performance. In the same experiment, Ovadia et al. (2024) showed that incorporating RAG on top of a finetun"
      },
      {
        "url": "https://oreil.ly/Ny1WI",
        "text": "OpenAI",
        "context": "igure is inspired by an example workflow shown by OpenAI (2023)."
      },
      {
        "url": "https://arxiv.org/abs/2205.05198",
        "text": "“Reducing Activation Recomputation in Large Transformer Models”",
        "context": "odels at different scales, according to the paper “Reducing Activation Recomputation in Large Transformer Models”, by Korthikanti et al. (2022)."
      },
      {
        "url": "https://en.wikipedia.org/wiki/Floating-point_arithmetic",
        "text": "float numbers",
        "context": "neural networks are traditionally represented as float numbers. The most common family of floating point formats"
      },
      {
        "url": "https://en.wikipedia.org/wiki/IEEE_754",
        "text": "IEEE 754",
        "context": "rs (IEEE) standard for Floating-Point Arithmetic (IEEE 754):"
      },
      {
        "url": "https://oreil.ly/BGXtn",
        "text": "TPUs",
        "context": "designed by Google to optimize AI performance on TPUs and TF32 was designed by NVIDIA for GPUs.11"
      },
      {
        "url": "https://oreil.ly/0pZgw",
        "text": "GPUs",
        "context": "mance on TPUs and TF32 was designed by NVIDIA for GPUs.11"
      },
      {
        "url": "https://arxiv.org/abs/2208.07339",
        "text": "Dettmers et al. (2022)",
        "context": "16 bits and in even lower precision. For example, Dettmers et al. (2022) have done excellent work quantizing LLMs into 8 b"
      },
      {
        "url": "https://arxiv.org/abs/2305.14314",
        "text": "Dettmers et al., 2023",
        "context": "nto 8 bits with LLM.int8() and 4 bits with QLoRA (Dettmers et al., 2023)."
      },
      {
        "url": "https://oreil.ly/lqLfv",
        "text": "Apple",
        "context": "n when necessary. To serve models on the devices, Apple (2024) leveraged a quantization scheme that uses"
      },
      {
        "url": "https://oreil.ly/FIP9V",
        "text": "Blackwell",
        "context": "rks, NVIDIA announced their new GPU architecture, Blackwell, that supports model inference in 4-bit float."
      },
      {
        "url": "https://en.wikipedia.org/wiki/Minifloat",
        "text": "minifloat",
        "context": "keep parameter values as floats using one of the minifloat formats, such as FP8 (8 bits) and FP4 (4 bits).18"
      },
      {
        "url": "https://arxiv.org/abs/1511.00363",
        "text": "Courbariaux et al., 2015",
        "context": "ed the 1-bit representation, e.g., BinaryConnect (Courbariaux et al., 2015), Xnor-Net (Rastegari et al., 2016), and BitNet ("
      },
      {
        "url": "https://arxiv.org/abs/1603.05279",
        "text": "Rastegari et al., 2016",
        "context": "naryConnect (Courbariaux et al., 2015), Xnor-Net (Rastegari et al., 2016), and BitNet (Wang et al., 2023).19"
      },
      {
        "url": "https://arxiv.org/abs/2310.11453",
        "text": "Wang et al., 2023",
        "context": "), Xnor-Net (Rastegari et al., 2016), and BitNet (Wang et al., 2023).19"
      },
      {
        "url": "https://arxiv.org/abs/2402.17764",
        "text": "Ma et al.",
        "context": "In 2024, Microsoft researchers (Ma et al.) declared that we’re entering the era of 1-bit LL"
      },
      {
        "url": "https://arxiv.org/abs/2307.09288",
        "text": "Touvron et al., 2023",
        "context": "hose performance is comparable to 16-bit Llama 2 (Touvron et al., 2023) up to 3.9B parameters, as shown in Table 7-4."
      },
      {
        "url": "https://oreil.ly/D-wIG",
        "text": "Hubara et al. (2016)",
        "context": "models in reduced precision as early as 2016; see Hubara et al. (2016) and Jacob et al. (2017). Character.AI (2024) shar"
      },
      {
        "url": "https://arxiv.org/abs/1712.05877",
        "text": "Jacob et al. (2017)",
        "context": "on as early as 2016; see Hubara et al. (2016) and Jacob et al. (2017). Character.AI (2024) shared that they were able t"
      },
      {
        "url": "https://oreil.ly/J7kVB",
        "text": "Character.AI (2024)",
        "context": "see Hubara et al. (2016) and Jacob et al. (2017). Character.AI (2024) shared that they were able to train their models"
      },
      {
        "url": "https://oreil.ly/pBaQM",
        "text": "mixed precision",
        "context": "Lower-precision training is often done in mixed precision, where a copy of the weights is kept in higher pr"
      },
      {
        "url": "https://arxiv.org/abs/2305.17888",
        "text": "Liu et al., 2023",
        "context": "mputed in higher precision. For example, LLM-QAT (Liu et al., 2023) quantizes weights and activations into 4 bits bu"
      },
      {
        "url": "https://oreil.ly/JZRsd",
        "text": "automatic mixed precision",
        "context": "ower precision can be set automatically using the automatic mixed precision (AMP) functionality offered by many ML frameworks"
      },
      {
        "url": "https://oreil.ly/Np1Hn",
        "text": "Rasley et al., 2020",
        "context": "s memory onto CPUs, as demonstrated by DeepSpeed (Rasley et al., 2020)."
      },
      {
        "url": "https://arxiv.org/abs/1902.00751",
        "text": "Houlsby et al. (2019)",
        "context": "ance close to that of full finetuning. A study by Houlsby et al. (2019) shows that with BERT large (Devlin et al., 2018),"
      },
      {
        "url": "https://arxiv.org/abs/1810.04805",
        "text": "Devlin et al., 2018",
        "context": "Houlsby et al. (2019) shows that with BERT large (Devlin et al., 2018), you’d need to update approximately 25% of the p"
      },
      {
        "url": "https://arxiv.org/abs/1804.07461",
        "text": "Wang et al., 2018",
        "context": "to that of full finetuning on the GLUE benchmark (Wang et al., 2018). Figure 7-7 shows the performance curve of parti"
      },
      {
        "url": "https://arxiv.org/abs/1902.00751",
        "text": "Houlsby et al. (2019)",
        "context": "o the model weights, such as the one developed by Houlsby et al. (2019). Because adapter-based methods involve adding par"
      },
      {
        "url": "https://arxiv.org/abs/2106.09685",
        "text": "Hu et al., 2021",
        "context": "As of this writing, LoRA (Hu et al., 2021) is by far the most popular adapter-based method,"
      },
      {
        "url": "https://arxiv.org/abs/2106.10199",
        "text": "Zaken et al., 2021",
        "context": "tion. Other adapter-based methods include BitFit (Zaken et al., 2021), which came out around the same time LoRA did. N"
      },
      {
        "url": "https://oreil.ly/avDPk",
        "text": "Liu et al., 2022",
        "context": "time LoRA did. Newer adapter methods include IA3 (Liu et al., 2022), whose efficient mixed-task batching strategy ma"
      },
      {
        "url": "https://arxiv.org/abs/2309.12307",
        "text": "Chen et al., 2023",
        "context": "and even full finetuning in some cases. LongLoRA (Chen et al., 2023) is a LoRA variant that incorporates attention-mo"
      },
      {
        "url": "https://arxiv.org/abs/2101.00190",
        "text": "Li and Liang, 2021",
        "context": "ues that can be confusing, such as prefix-tuning (Li and Liang, 2021), P-Tuning (Liu et al., 2021), and prompt tuning"
      },
      {
        "url": "https://arxiv.org/abs/2103.10385",
        "text": "Liu et al., 2021",
        "context": "as prefix-tuning (Li and Liang, 2021), P-Tuning (Liu et al., 2021), and prompt tuning (Lester et al., 2021).23 They"
      },
      {
        "url": "https://arxiv.org/abs/2104.08691",
        "text": "Lester et al., 2021",
        "context": ", P-Tuning (Liu et al., 2021), and prompt tuning (Lester et al., 2021).23 They differ mainly on the locations where the"
      },
      {
        "url": "https://github.com/huggingface/peft",
        "text": "GitHub repository huggingface/peft",
        "context": "ng used, I analyzed over 1,000 open issues on the GitHub repository huggingface/peft in October 2024. The assumption is that if someon"
      },
      {
        "url": "https://arxiv.org/abs/1902.00751",
        "text": "Houlsby et al. (2019)",
        "context": "Unlike the original adapter method by Houlsby et al. (2019), LoRA (Low-Rank Adaptation) (Hu et al., 2021) inc"
      },
      {
        "url": "https://arxiv.org/abs/2106.09685",
        "text": "Hu et al., 2021",
        "context": "oulsby et al. (2019), LoRA (Low-Rank Adaptation) (Hu et al., 2021) incorporates additional parameters in a way that"
      },
      {
        "url": "https://arxiv.org/abs/1804.08838",
        "text": "Li et al. (2018)",
        "context": "ers, they have very low intrinsic dimensions; see Li et al. (2018); Aghajanyan et al. (2020); and Hu et al. (2021)."
      },
      {
        "url": "https://arxiv.org/abs/2012.13255",
        "text": "Aghajanyan et al. (2020)",
        "context": "y low intrinsic dimensions; see Li et al. (2018); Aghajanyan et al. (2020); and Hu et al. (2021). They showed that pre-train"
      },
      {
        "url": "https://arxiv.org/abs/2106.09685",
        "text": "Hu et al. (2021)",
        "context": "e Li et al. (2018); Aghajanyan et al. (2020); and Hu et al. (2021). They showed that pre-training implicitly minimiz"
      },
      {
        "url": "https://oreil.ly/xzdiG",
        "text": "Sainath et al., 2013",
        "context": "k Training with High-Dimensional Output Targets” (Sainath et al., 2013), “Semi-Orthogonal Low-Rank Matrix Factorization"
      },
      {
        "url": "https://oreil.ly/LHLNz",
        "text": "Povey et al., 2018",
        "context": "k Matrix Factorization for Deep Neural Networks” (Povey et al., 2018), and “Speeding up Convolutional Neural Networks"
      },
      {
        "url": "https://oreil.ly/BR63I",
        "text": "Jaderberg et al., 2014",
        "context": "tional Neural Networks with Low Rank Expansions” (Jaderberg et al., 2014)."
      },
      {
        "url": "https://arxiv.org/abs/1602.07360",
        "text": "Iandola et al., 2016",
        "context": "3 convolution with 1 × 1 convolution, SqueezeNet (Iandola et al., 2016) achieves AlexNet-level accuracy on ImageNet usin"
      },
      {
        "url": "https://arxiv.org/abs/2307.05695",
        "text": "Lialin et al., 2023",
        "context": "t attempts to train low-rank LLMs include ReLoRA (Lialin et al., 2023) and GaLore (Zhao et al., 2024). ReLoRA works for"
      },
      {
        "url": "https://arxiv.org/abs/2403.03507",
        "text": "Zhao et al., 2024",
        "context": "include ReLoRA (Lialin et al., 2023) and GaLore (Zhao et al., 2024). ReLoRA works for transformer-based models of up"
      },
      {
        "url": "https://arxiv.org/abs/2012.13255",
        "text": "Aghajanyan et al.’s argument",
        "context": "o hundreds of billions of parameters. However, if Aghajanyan et al.’s argument is correct—that pre-training implicitly compresse"
      },
      {
        "url": "https://arxiv.org/abs/2305.08252",
        "text": "Dutt et al., 2023",
        "context": "itectures, such as convolutional neural networks (Dutt et al., 2023; Zhong et al., 2024; Aleem et al., 2024), LoRA ha"
      },
      {
        "url": "https://arxiv.org/abs/2401.17868",
        "text": "Zhong et al., 2024",
        "context": "convolutional neural networks (Dutt et al., 2023; Zhong et al., 2024; Aleem et al., 2024), LoRA has been primarily use"
      },
      {
        "url": "https://arxiv.org/abs/2402.04964",
        "text": "Aleem et al., 2024",
        "context": "networks (Dutt et al., 2023; Zhong et al., 2024; Aleem et al., 2024), LoRA has been primarily used for transformer mo"
      },
      {
        "url": "https://arxiv.org/abs/1709.00103",
        "text": "Zhong et al., 2017",
        "context": "k = 2 yields the best performance on the WikiSQL (Zhong et al., 2017) and MultiNLI (Multi-Genre Natural Language Infer"
      },
      {
        "url": "https://oreil.ly/mqHMU",
        "text": "Williams et al., 2017",
        "context": "lti-Genre Natural Language Inference) benchmarks (Williams et al., 2017). Table 7-5 shows their results. However, the aut"
      },
      {
        "url": "https://oreil.ly/zzREV",
        "text": "Sooriyarachchi, 2023",
        "context": "was from applying LoRA to all feedforward layers (Sooriyarachchi, 2023). Fomenko et al. (2024) noted that feedforward-ba"
      },
      {
        "url": "https://arxiv.org/html/2404.05086v1",
        "text": "Fomenko et al. (2024)",
        "context": "to all feedforward layers (Sooriyarachchi, 2023). Fomenko et al. (2024) noted that feedforward-based LoRA can be compleme"
      },
      {
        "url": "https://oreil.ly/A-d5f",
        "text": "Raschka (2023)",
        "context": "in some cases, a higher rank might be necessary. Raschka (2023) found that r = 256 achieved the best performance"
      },
      {
        "url": "https://github.com/chiphuyen/aie-book",
        "text": "book’s GitHub repository",
        "context": "ion techniques to minimize the added latency. The book’s GitHub repository contains a walkthrough of how to do so."
      },
      {
        "url": "https://oreil.ly/vfXqE",
        "text": "LoRA adapters",
        "context": "r for each task. For example, Apple used multiple LoRA adapters to adapt the same 3B-parameter base model to diff"
      },
      {
        "url": "https://oreil.ly/T08JJ",
        "text": "Hugging Face",
        "context": "ou’d use pre-trained models. You can find them on Hugging Face26 or initiatives like AdapterHub."
      },
      {
        "url": "https://adapterhub.ml",
        "text": "AdapterHub",
        "context": "n find them on Hugging Face26 or initiatives like AdapterHub."
      },
      {
        "url": "https://github.com/huggingface/peft",
        "text": "Hugging Face’s PEFT",
        "context": "less popular base models. PEFT frameworks—such as Hugging Face’s PEFT, Axolotl, unsloth, and LitGPT—likely support LoRA"
      },
      {
        "url": "https://github.com/axolotl-ai-cloud/axolotl",
        "text": "Axolotl",
        "context": "els. PEFT frameworks—such as Hugging Face’s PEFT, Axolotl, unsloth, and LitGPT—likely support LoRA for popu"
      },
      {
        "url": "https://github.com/unslothai/unsloth",
        "text": "unsloth",
        "context": "frameworks—such as Hugging Face’s PEFT, Axolotl, unsloth, and LitGPT—likely support LoRA for popular base"
      },
      {
        "url": "https://github.com/Lightning-AI/litgpt",
        "text": "LitGPT",
        "context": "uch as Hugging Face’s PEFT, Axolotl, unsloth, and LitGPT—likely support LoRA for popular base models right"
      },
      {
        "url": "https://arxiv.org/abs/2305.14314",
        "text": "Dettmers et al., 2023",
        "context": "rly promising quantized version of LoRA is QLoRA (Dettmers et al., 2023).27 In the original LoRA paper, during finetuning"
      },
      {
        "url": "https://arxiv.org/abs/2309.14717",
        "text": "Xu et al., 2023",
        "context": "than QLoRA, quantized LoRA works include QA-LoRA (Xu et al., 2023), ModuLoRA (Yin et al., 2023), and IR-QLoRA (Qin"
      },
      {
        "url": "https://arxiv.org/abs/2309.16119",
        "text": "Yin et al., 2023",
        "context": "orks include QA-LoRA (Xu et al., 2023), ModuLoRA (Yin et al., 2023), and IR-QLoRA (Qin et al., 2024)."
      },
      {
        "url": "https://arxiv.org/abs/2402.05445",
        "text": "Qin et al., 2024",
        "context": "2023), ModuLoRA (Yin et al., 2023), and IR-QLoRA (Qin et al., 2024)."
      },
      {
        "url": "https://arxiv.org/abs/1612.00796",
        "text": "Kirkpatrick et al., 2016",
        "context": "al networks are prone to catastrophic forgetting (Kirkpatrick et al., 2016). A model can forget how to do an old task when i"
      },
      {
        "url": "https://arxiv.org/abs/1602.05629",
        "text": "McMahan et al., 2016",
        "context": "odel merging is one way to do federated learning (McMahan et al., 2016), in which multiple devices train the same model"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Ensemble_learning",
        "text": "Wikipedia",
        "context": "started with model ensemble methods. According to Wikipedia, ensembling combines “multiple learning algorithm"
      },
      {
        "url": "https://oreil.ly/hRV9P",
        "text": "Hugging Face’s Open LLM Leaderboard",
        "context": "dominate leaderboards, many models on top of the Hugging Face’s Open LLM Leaderboard are merged models."
      },
      {
        "url": "https://oreil.ly/eXC02",
        "text": "Perrone, 1993",
        "context": "tter one was studied as early as the early 1990s (Perrone, 1993). Linear combination is often used in federated l"
      },
      {
        "url": "https://oreil.ly/ZKRPR",
        "text": "Wang et al., 2020",
        "context": "combination is often used in federated learning (Wang et al., 2020)."
      },
      {
        "url": "https://arxiv.org/abs/2203.05482",
        "text": "Wortsman et al., 2022",
        "context": "ne entire models or parts of models. Model soups (Wortsman et al., 2022) showed how averaging the entire weights of multi"
      },
      {
        "url": "https://arxiv.org/abs/2212.04089",
        "text": "Ilharco et al., 2022",
        "context": "Task vectors allow us to do task arithmetic (Ilharco et al., 2022), such as adding two task vectors to combine task"
      },
      {
        "url": "https://arxiv.org/abs/1910.05653",
        "text": "Singh and Jaggi, 2020",
        "context": "such as in “Model Fusion via Optimal Transport” (Singh and Jaggi, 2020), “Git Re-Basin: Merging Models Modulo Permutatio"
      },
      {
        "url": "https://arxiv.org/abs/2209.04836",
        "text": "Ainsworth et al., 2022",
        "context": "n: Merging Models Modulo Permutation Symmetries” (Ainsworth et al., 2022), and “Merging by Matching Models in Task Paramet"
      },
      {
        "url": "https://arxiv.org/abs/2312.04339",
        "text": "Tam et al., 2023",
        "context": "by Matching Models in Task Parameter Subspaces” (Tam et al., 2023). While it makes sense to combine aligned paramet"
      },
      {
        "url": "https://arxiv.org/abs/2306.01708",
        "text": "Yadav et al. (2023)",
        "context": "ing: Resolving Interference When Merging Models”, Yadav et al. (2023) showed that you can reset a large portion of task"
      },
      {
        "url": "https://arxiv.org/abs/2311.03099",
        "text": "Yu et al., 2023",
        "context": "iques such as TIES (Yadav et al., 2023) and DARE (Yu et al., 2023) first prune the redundant parameters from task v"
      },
      {
        "url": "https://oreil.ly/IM0Jc",
        "text": "Goliath-120B",
        "context": "One early success of frankenmerging is Goliath-120B (alpindale, 2023), which was merged from two fine"
      },
      {
        "url": "https://oreil.ly/URfbk",
        "text": "Xwin",
        "context": "was merged from two finetuned Llama 2-70B models, Xwin and Euryale. It took 72 out of 80 layers from eac"
      },
      {
        "url": "https://oreil.ly/Ftnxd",
        "text": "Euryale",
        "context": "d from two finetuned Llama 2-70B models, Xwin and Euryale. It took 72 out of 80 layers from each model and"
      },
      {
        "url": "https://arxiv.org/abs/2212.05055",
        "text": "Komatsuzaki et al., 2022",
        "context": "ining Mixture-of-Experts from Dense Checkpoints” (Komatsuzaki et al., 2022). Rather than training an MOE from scratch, you t"
      },
      {
        "url": "https://arxiv.org/abs/2406.04692",
        "text": "Wang et al., 2024",
        "context": "erformance to OpenAI’s GPT-4o in some benchmarks (Wang et al., 2024)."
      },
      {
        "url": "https://arxiv.org/abs/2312.15166",
        "text": "Kim et al. (2023)",
        "context": "approach to layer upscaling is depthwise scaling. Kim et al. (2023) used this technique to create SOLAR 10.7B from on"
      },
      {
        "url": "https://oreil.ly/7I6Ch",
        "text": "OpenAI’s finetuning best practices document",
        "context": "the starting models vary for different projects. OpenAI’s finetuning best practices document gives examples of two development paths: the prog"
      },
      {
        "url": "https://github.com/hiyouga/LLaMA-Factory",
        "text": "LLaMA-Factory",
        "context": "ny great finetuning frameworks available, such as LLaMA-Factory, unsloth, PEFT, Axolotl, and LitGPT. They support"
      },
      {
        "url": "https://github.com/unslothai/unsloth",
        "text": "unsloth",
        "context": "ning frameworks available, such as LLaMA-Factory, unsloth, PEFT, Axolotl, and LitGPT. They support a wide r"
      },
      {
        "url": "https://github.com/huggingface/peft",
        "text": "PEFT",
        "context": "eworks available, such as LLaMA-Factory, unsloth, PEFT, Axolotl, and LitGPT. They support a wide range o"
      },
      {
        "url": "https://github.com/axolotl-ai-cloud/axolotl",
        "text": "Axolotl",
        "context": "available, such as LLaMA-Factory, unsloth, PEFT, Axolotl, and LitGPT. They support a wide range of finetun"
      },
      {
        "url": "https://github.com/Lightning-AI/litgpt",
        "text": "LitGPT",
        "context": "uch as LLaMA-Factory, unsloth, PEFT, Axolotl, and LitGPT. They support a wide range of finetuning methods,"
      },
      {
        "url": "https://huyenchip.com/llama-police",
        "text": "Llama Police",
        "context": "ub that you can clone and run with your own data. Llama Police has a more comprehensive and up-to-date list of f"
      },
      {
        "url": "https://github.com/microsoft/DeepSpeed",
        "text": "DeepSpeed",
        "context": "k that helps you do distributed training, such as DeepSpeed, PyTorch Distributed, and ColossalAI."
      },
      {
        "url": "https://oreil.ly/hxUAk",
        "text": "PyTorch Distributed",
        "context": "s you do distributed training, such as DeepSpeed, PyTorch Distributed, and ColossalAI."
      },
      {
        "url": "https://github.com/microsoft/DeepSpeed",
        "text": "ColossalAI",
        "context": "ning, such as DeepSpeed, PyTorch Distributed, and ColossalAI."
      },
      {
        "url": "https://arxiv.org/abs/2204.05862",
        "text": "Bai et al., 2020",
        "context": "ome people call this phenomenon an alignment tax (Bai et al., 2020), but this term can be confused with penalties ag"
      },
      {
        "url": "https://arxiv.org/abs/2407.21783",
        "text": "Dubey et al., 2024",
        "context": "ople, such as the authors of the Llama 3.1 paper (Dubey et al., 2024), adhere to “the principle that post-training sho"
      },
      {
        "url": "https://oreil.ly/B59ci",
        "text": "Maheswaranathan et al.",
        "context": "evolutionary strategy. One example, described by Maheswaranathan et al., combines random search with surrogate gradients,"
      },
      {
        "url": "https://arxiv.org/abs/1609.01596",
        "text": "Arild Nøkland, 2016",
        "context": "nteresting approach is direct feedback alignment (Arild Nøkland, 2016)."
      },
      {
        "url": "https://oreil.ly/u7wYx",
        "text": "“Transformer Inference Arithmetic”",
        "context": "erence memory calculation, check out Carol Chen’s “Transformer Inference Arithmetic”, kipply’s blog (March 2022)."
      },
      {
        "url": "https://oreil.ly/Xe7h6",
        "text": "“Transformer Math 101”",
        "context": "aining memory calculation, check out EleutherAI’s “Transformer Math 101” (Anthony et al., April 2023)."
      },
      {
        "url": "https://oreil.ly/atIgi",
        "text": "“the secret to high performance on Cloud TPUs”",
        "context": "11 Google introduced BFloat16 as “the secret to high performance on Cloud TPUs”."
      },
      {
        "url": "https://en.wikipedia.org/wiki/IEEE_754",
        "text": "1",
        "context": "15 The FP16 and BF16 confusion continued with Llama"
      },
      {
        "url": "https://x.com/abacaj/status/1695334296792264792?s=20",
        "text": "2",
        "context": "with Llama 3.1. See X and Threads discussions: 1; 2, 3, 4; and llama.cpp’s benchmark between BF16 and"
      },
      {
        "url": "https://oreil.ly/U8L4d",
        "text": "3",
        "context": "The FP16 and BF16 confusion continued with Llama 3.1. See X and Threads discussions: 1; 2, 3, 4; and"
      },
      {
        "url": "https://oreil.ly/8ush1",
        "text": "4",
        "context": "lama 3.1. See X and Threads discussions: 1; 2, 3, 4; and llama.cpp’s benchmark between BF16 and FP16,"
      },
      {
        "url": "https://github.com/ggerganov/llama.cpp/pull/7150",
        "text": "benchmark between BF16 and FP16",
        "context": "Threads discussions: 1; 2, 3, 4; and llama.cpp’s benchmark between BF16 and FP16, Bloke’s writeup, and Raschka’s writeup."
      },
      {
        "url": "https://oreil.ly/0vuze",
        "text": "Bloke’s writeup",
        "context": "and llama.cpp’s benchmark between BF16 and FP16, Bloke’s writeup, and Raschka’s writeup."
      },
      {
        "url": "https://oreil.ly/WK_zT",
        "text": "Raschka’s writeup",
        "context": "hmark between BF16 and FP16, Bloke’s writeup, and Raschka’s writeup."
      },
      {
        "url": "https://oreil.ly/V4pma",
        "text": "In early 2020, it was acquired by Apple for a reported $200M",
        "context": ".ai, a startup that focused on model compression. In early 2020, it was acquired by Apple for a reported $200M."
      },
      {
        "url": "https://oreil.ly/QL2gL",
        "text": "“Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq”",
        "context": "rk at NVIDIA was on mixed precision training. See “Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq” (Huyen et al., NVIDIA Developer Technical Blog, O"
      },
      {
        "url": "https://oreil.ly/82-jJ",
        "text": "Fireworks",
        "context": "of this writing, some finetuning frameworks like Fireworks only allow a maximum LoRA rank of 32. However, th"
      },
      {
        "url": "https://oreil.ly/u_cVP",
        "text": "Designing Machine Learning Systems",
        "context": "28 My book, Designing Machine Learning Systems has a section on “ML on the Cloud and on the Edge"
      },
      {
        "url": "https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/",
        "text": "Designing Machine Learning Systems",
        "context": "u can read more about ensemble methods in my book Designing Machine Learning Systems."
      },
      {
        "url": "https://oreil.ly/GFeC7",
        "text": "“Ako: Decentralised Deep Learning with Partial Gradient Exchange”",
        "context": "n deep learning was mentioned as early as 2016 in “Ako: Decentralised Deep Learning with Partial Gradient Exchange” (Watcharapichat et al., Proceedings of the Sevent"
      }
    ],
    "bidirectional_refs": {}
  }
}