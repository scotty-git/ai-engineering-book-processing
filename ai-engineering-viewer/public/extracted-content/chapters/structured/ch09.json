{
  "id": "ch09",
  "title": "Chapter 9. Inference Optimization",
  "sections": [
    {
      "type": "chapter_title",
      "content": "Chapter 9. Inference Optimization",
      "id": "chapter-title",
      "raw_html": "<h1><span class=\"label\">Chapter 9. </span>Inference Optimization</h1>"
    },
    {
      "type": "paragraph",
      "content": "New models come and go, but one thing will always remain relevant: making them better, cheaper, and faster. Up until now, the book has discussed various techniques for making models better. This chapter focuses on making them faster and cheaper.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-type=\"indexterm\" id=\"ch09.html0\"></a>New models come and go, but one thing will always remain relevant: making them better, cheaper, and faster. Up until now, the book has discussed various techniques for making models better. This chapter focuses on making them faster and cheaper.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "No matter how good your model is, if it’s too slow, your users might lose patience, or worse, its predictions might become useless—imagine a next-day stock price prediction model that takes two days to compute each outcome. If your model is too expensive, its return on investment won’t be worth it.",
      "raw_html": "<p>No matter how good your model is, if it’s too slow, your users might lose patience, or worse, its predictions might become useless—imagine a next-day stock price prediction model that takes two days to compute each outcome. If your model is too expensive, its return on investment won’t be worth it.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Inference optimization can be done at the model, hardware, and service levels. At the model level, you can reduce a trained model’s size or develop more efficient architectures, such as one without the computation bottlenecks in the attention mechanism often used in transformer models. At the hardware level, you can design more powerful hardware.",
      "raw_html": "<p>Inference optimization can be done at the model, hardware, and service levels. At the model level, you can reduce a trained model’s size or develop more efficient architectures, such as one without the computation bottlenecks in the attention mechanism often used in transformer models. At the hardware level, you can design more powerful hardware.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The inference service runs the model on the given hardware to accommodate user requests. It can incorporate techniques that optimize models for specific hardware. It also needs to consider usage and traffic patterns to efficiently allocate resources to reduce latency and cost.",
      "raw_html": "<p>The inference service runs the model on the given hardware to accommodate user requests. It can incorporate techniques that optimize models for specific hardware. It also needs to consider usage and traffic patterns to efficiently allocate resources to reduce latency and cost.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because of this, inference optimization is an interdisciplinary field that often sees collaboration among model researchers, application developers, system engineers, compiler designers, hardware architects, and even data center operators.",
      "raw_html": "<p>Because of this, inference optimization is an interdisciplinary field that often sees collaboration among model researchers, application developers, system engineers, compiler designers, hardware architects, and even data center operators.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This chapter discusses bottlenecks for AI inference and techniques to overcome them. It’ll focus mostly on optimization at the model and service levels, with an overview of AI accelerators.",
      "raw_html": "<p>This chapter discusses bottlenecks for AI inference and techniques to overcome them. It’ll focus mostly on optimization at the model and service levels, with an overview of AI accelerators.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This chapter also covers performance metrics and trade-offs. Sometimes, a technique that speeds up a model can also reduce its cost. For example, reducing a model’s precision makes it smaller and faster. But often, optimization requires trade-offs. For example, the best hardware might make your model run faster but at a higher cost.",
      "raw_html": "<p class=\"pagebreak-before\">This chapter also covers performance metrics and trade-offs. Sometimes, a technique that speeds up a model can also reduce its cost. For example, reducing a model’s precision makes it smaller and faster. But often, optimization requires trade-offs. For example, the best hardware might make your model run faster but at a higher cost.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Given the growing availability of open source models, more teams are building their own inference services. However, even if you don’t implement these inference optimization techniques, understanding these techniques will help you evaluate inference services and frameworks. If your application’s latency and cost are hurting you, read on. This chapter might help you diagnose the causes and potential solutions.",
      "raw_html": "<p>Given the growing availability of open source models, more teams are building their own inference services. However, even if you don’t implement these inference optimization techniques, understanding these techniques will help you evaluate inference services and frameworks. If your application’s latency and cost are hurting you, read on. This chapter might help you diagnose the causes and potential solutions.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Understanding Inference Optimization",
      "id": "heading-7",
      "raw_html": "<h1>Understanding Inference Optimization</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "There are two distinct phases in an AI model’s lifecycle: training and inference. Training refers to the process of building a model. Inference refers to the process of using a model to compute an output for a given input.1 Unless you train or finetune a model, you’ll mostly need to care about inference.2",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"understanding\" data-type=\"indexterm\" id=\"ch09.html1\"></a>There are two distinct phases in an AI model’s lifecycle: training and inference. Training refers to the process of building a model. Inference refers to the process of using a model to compute an output for a given input.<sup><a data-type=\"noteref\" href=\"ch09.html#id1597\" id=\"id1597-marker\">1</a></sup> Unless you train or finetune a model, you’ll mostly need to care about inference.<sup><a data-type=\"noteref\" href=\"ch09.html#id1598\" id=\"id1598-marker\">2</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This section starts with an overview of inference that introduces a shared vocabulary to discuss the rest of the chapter. If you’re already familiar with these concepts, feel free to skip to the section of interest.",
      "raw_html": "<p>This section starts with an overview of inference that introduces a shared vocabulary to discuss the rest of the chapter. If you’re already familiar with these concepts, feel free to skip to the section of interest.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Inference Overview",
      "id": "heading-7",
      "raw_html": "<h2>Inference Overview</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "In production, the component that runs model inference is called an inference server. It hosts the available models and has access to the necessary hardware. Based on requests from applications (e.g., user prompts), it allocates resources to execute the appropriate models and returns the responses to users. An inference server is part of a broader inference service, which is also responsible for receiving, routing, and possibly preprocessing requests before they reach the inference server. A visualization of a simple inference service is shown in Figure 9-1.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"understanding\" data-tertiary=\"inference overview\" data-type=\"indexterm\" id=\"ch09.html2\"></a>In production, the component that runs model inference is called an inference server. It hosts the available models and has access to the necessary hardware. Based on requests from applications (e.g., user prompts), it allocates resources to execute the appropriate models and returns the responses to users. <a contenteditable=\"false\" data-primary=\"inference service\" data-secondary=\"and inference optimization\" data-secondary-sortas=\"inference optimization\" data-type=\"indexterm\" id=\"id1599\"></a>An inference server is part of a broader inference service, which is also responsible for receiving, routing, and possibly preprocessing requests before they reach the inference server. A visualization of a simple inference service is shown in <a data-type=\"xref\" href=\"#ch09_figure_1_1730130962952524\">Figure 9-1</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_1_1730130962952524",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_1_1730130962952524\">\n<p>. </p>\n<img alt=\"A diagram of a computer hardware system\n\nDescription automatically generated\" src=\"assets/aien_0901.png\"/>\n<h6><span class=\"label\">Figure 9-1. </span>A simple inference service.</h6>\n</div></figure>",
      "image": "aien_0901.png",
      "alt": "A diagram of a computer hardware system\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0901.png",
      "caption": {
        "label": "Figure 9-1.",
        "text": "A simple inference service."
      }
    },
    {
      "type": "paragraph",
      "content": ".",
      "raw_html": "<p>. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model APIs like those provided by OpenAI and Google are inference services. If you use one of these services, you won’t be implementing most of the techniques discussed in this chapter. However, if you host a model yourself, you’ll be responsible for building, optimizing, and maintaining its inference service.",
      "raw_html": "<p>Model APIs like those provided by OpenAI and Google are inference services. If you use one of these services, you won’t be implementing most of the techniques discussed in this chapter. However, if you host a model yourself, you’ll be responsible for building, optimizing, and maintaining its inference service.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Computational bottlenecks",
      "id": "heading-7",
      "raw_html": "<h3>Computational bottlenecks</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Optimization is about identifying bottlenecks and addressing them. For example, to optimize traffic, city planners might identify congestion points and take measures to alleviate congestion. Similarly, an inference server should be designed to address the computational bottlenecks of the inference workloads it serves. There are two main computational bottlenecks, compute-bound and memory bandwidth-bound:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"computational\" data-type=\"indexterm\" id=\"ch09.html3\"></a><a contenteditable=\"false\" data-primary=\"computational bottlenecks\" data-type=\"indexterm\" id=\"ch09.html4\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference overview\" data-tertiary=\"computational bottlenecks\" data-type=\"indexterm\" id=\"ch09.html5\"></a>Optimization is about identifying bottlenecks and addressing them. For example, to optimize traffic, city planners might identify congestion points and take measures to alleviate congestion. Similarly, an inference server should be designed to address the computational bottlenecks of the inference workloads it serves. There are two main computational bottlenecks, <em>compute-bound</em> and <em>memory bandwidth-bound</em>:</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Compute-bound",
          "definition": "This refers to tasks whose time-to-complete is determined by the computation needed for the tasks. For example, password decryption is typically compute-bound due to the intensive mathematical calculations required to break encryption algorithms."
        },
        {
          "term": "Memory bandwidth-bound",
          "definition": "These tasks are constrained by the data transfer rate within the system, such as the speed of data movement between memory and processors. For example, if you store your data in the CPU memory and train a model on GPUs, you have to move data from the CPU to the GPU, which can take a long time. This can be shortened as bandwidth-bound. In literature, memory bandwidth-bound is often referred to as memory-bound."
        }
      ],
      "raw_html": "<dl>\n<dt>Compute-bound </dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"compute-bound\" data-type=\"indexterm\" id=\"id1600\"></a><a contenteditable=\"false\" data-primary=\"compute-bound bottlenecks\" data-type=\"indexterm\" id=\"id1601\"></a>This refers to tasks whose time-to-complete is determined by the computation needed for the tasks. For example, password decryption is typically compute-bound due to the intensive mathematical calculations required to break encryption algorithms.</p></dd>\n<dt>Memory bandwidth-bound </dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"memory\" data-type=\"indexterm\" id=\"id1602\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"bandwidth-bound\" data-type=\"indexterm\" id=\"id1603\"></a>These tasks are constrained by the data transfer rate within the system, such as the speed of data movement between memory and processors. For example, if you store your data in the CPU memory and train a model on GPUs, you have to move data from the CPU to the GPU, which can take a long time. This can be shortened as bandwidth-bound. In literature, memory bandwidth-bound is often referred to as memory-bound.</p></dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "This refers to tasks whose time-to-complete is determined by the computation needed for the tasks. For example, password decryption is typically compute-bound due to the intensive mathematical calculations required to break encryption algorithms.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"compute-bound\" data-type=\"indexterm\" id=\"id1600\"></a><a contenteditable=\"false\" data-primary=\"compute-bound bottlenecks\" data-type=\"indexterm\" id=\"id1601\"></a>This refers to tasks whose time-to-complete is determined by the computation needed for the tasks. For example, password decryption is typically compute-bound due to the intensive mathematical calculations required to break encryption algorithms.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "These tasks are constrained by the data transfer rate within the system, such as the speed of data movement between memory and processors. For example, if you store your data in the CPU memory and train a model on GPUs, you have to move data from the CPU to the GPU, which can take a long time. This can be shortened as bandwidth-bound. In literature, memory bandwidth-bound is often referred to as memory-bound.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"memory\" data-type=\"indexterm\" id=\"id1602\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"bandwidth-bound\" data-type=\"indexterm\" id=\"id1603\"></a>These tasks are constrained by the data transfer rate within the system, such as the speed of data movement between memory and processors. For example, if you store your data in the CPU memory and train a model on GPUs, you have to move data from the CPU to the GPU, which can take a long time. This can be shortened as bandwidth-bound. In literature, memory bandwidth-bound is often referred to as memory-bound.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "aside",
      "content": "Terminology Ambiguity: Memory-Bound Versus Bandwidth-Bound\nMemory-bound is also used by some people to refer to tasks whose time-to-complete is constrained by memory capacity instead of memory bandwidth. This occurs when your hardware doesn’t have sufficient memory to handle the task, for example, if your machine doesn’t have enough memory to store the entire internet. This memory is often manifested in the error recognizable by engineers everywhere: OOM, out-of-memory.3\nHowever, this situation can often be mitigated by splitting your task into smaller pieces. For example, if you’re constrained by GPU memory and cannot fit an entire model into the GPU, you can split the model across GPU memory and CPU memory. This splitting will slow down your computation because of the time it takes to transfer data between the CPU and GPU. However, if data transfer is fast enough, this becomes less of an issue. Therefore, the memory capacity limitation is actually more about memory bandwidth.",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"ch09_terminology_ambiguity_memory_bound_versus_bandwid_1730130963006967\">\n<h1>Terminology Ambiguity: Memory-Bound Versus Bandwidth-Bound</h1>\n<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"memory-bound versus bandwidth-bound interference\" data-type=\"indexterm\" id=\"id1604\"></a><em>Memory-bound</em> is also used by some people to refer to tasks whose time-to-complete is constrained by memory capacity instead of memory bandwidth. This occurs when your hardware doesn’t have sufficient memory to handle the task, for example, if your machine doesn’t have enough memory to store the entire internet. This memory is often manifested in the error recognizable by engineers everywhere: OOM, out-of-memory.<sup><a data-type=\"noteref\" href=\"ch09.html#id1605\" id=\"id1605-marker\">3</a></sup></p>\n<p>However, this situation can often be mitigated by splitting your task into smaller pieces. For example, if you’re constrained by GPU memory and cannot fit an entire model into the GPU, you can split the model across GPU memory and CPU memory. This splitting will slow down your computation because of the time it takes to transfer data between the CPU and GPU. However, if data transfer is fast enough, this becomes less of an issue. Therefore, the memory capacity limitation is actually more about memory bandwidth.</p>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Terminology Ambiguity: Memory-Bound Versus Bandwidth-Bound",
      "id": "heading-7",
      "raw_html": "<h1>Terminology Ambiguity: Memory-Bound Versus Bandwidth-Bound</h1>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Memory-bound is also used by some people to refer to tasks whose time-to-complete is constrained by memory capacity instead of memory bandwidth. This occurs when your hardware doesn’t have sufficient memory to handle the task, for example, if your machine doesn’t have enough memory to store the entire internet. This memory is often manifested in the error recognizable by engineers everywhere: OOM, out-of-memory.3",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"memory-bound versus bandwidth-bound interference\" data-type=\"indexterm\" id=\"id1604\"></a><em>Memory-bound</em> is also used by some people to refer to tasks whose time-to-complete is constrained by memory capacity instead of memory bandwidth. This occurs when your hardware doesn’t have sufficient memory to handle the task, for example, if your machine doesn’t have enough memory to store the entire internet. This memory is often manifested in the error recognizable by engineers everywhere: OOM, out-of-memory.<sup><a data-type=\"noteref\" href=\"ch09.html#id1605\" id=\"id1605-marker\">3</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "However, this situation can often be mitigated by splitting your task into smaller pieces. For example, if you’re constrained by GPU memory and cannot fit an entire model into the GPU, you can split the model across GPU memory and CPU memory. This splitting will slow down your computation because of the time it takes to transfer data between the CPU and GPU. However, if data transfer is fast enough, this becomes less of an issue. Therefore, the memory capacity limitation is actually more about memory bandwidth.",
      "raw_html": "<p>However, this situation can often be mitigated by splitting your task into smaller pieces. For example, if you’re constrained by GPU memory and cannot fit an entire model into the GPU, you can split the model across GPU memory and CPU memory. This splitting will slow down your computation because of the time it takes to transfer data between the CPU and GPU. However, if data transfer is fast enough, this becomes less of an issue. Therefore, the memory capacity limitation is actually more about memory bandwidth.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The concepts of compute-bound or memory bandwidth-bound were introduced in the paper “Roofline” (Williams et al., 2009).4 Mathematically, an operation can be classified as compute-bound or memory bandwidth-bound based on its arithmetic intensity, which is the number of arithmetic operations per byte of memory access. Profiling tools like NVIDIA Nsight will show you a roofline chart to tell you whether your workload is compute-bound or memory bandwidth-bound, as shown in Figure 9-2. This chart is a roofline chart because it resembles a roof. Roofline charts are common in hardware performance analyses.",
      "raw_html": "<p>The concepts of compute-bound or memory bandwidth-bound were introduced in the paper “Roofline” (<a href=\"https://oreil.ly/M_aGR\">Williams et al., 2009</a>).<sup><a data-type=\"noteref\" href=\"ch09.html#id1606\" id=\"id1606-marker\">4</a></sup> Mathematically, an operation can be classified as compute-bound or memory bandwidth-bound based on its <a href=\"https://oreil.ly/K3j6t\"><em>arithmetic intensity</em></a>, which is the number of arithmetic operations per byte of memory access. Profiling tools like NVIDIA Nsight will show you a roofline chart to tell you whether your workload is compute-bound or memory bandwidth-bound, as shown in <a data-type=\"xref\" href=\"#ch09_figure_2_1730130962952613\">Figure 9-2</a>. This chart is a <em>roofline</em> chart because it resembles a roof. Roofline charts are common in hardware performance analyses.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Different optimization techniques aim to mitigate different bottlenecks. For example, a compute-bound workload might be sped up by spreading it out to more chips or by leveraging chips with more computational power (e.g., a higher FLOP/s number). A memory bandwidth-bound workload might be sped up by leveraging chips with higher bandwidth.",
      "raw_html": "<p>Different optimization techniques aim to mitigate different bottlenecks. For example, a compute-bound workload might be sped up by spreading it out to more chips or by leveraging chips with more computational power (e.g., a higher FLOP/s number). A memory bandwidth-bound workload might be sped up by leveraging chips with higher bandwidth.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_2_1730130962952613",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_2_1730130962952613\">\n<img alt=\"A graph with a line and a point\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0902.png\"/>\n<h6><span class=\"label\">Figure 9-2. </span>The roofline chart can help you visualize whether an operation is compute-bound or memory bandwidth-bound. This graph is on a log scale.</h6>\n</div></figure>",
      "image": "aien_0902.png",
      "alt": "A graph with a line and a point\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0902.png",
      "caption": {
        "label": "Figure 9-2.",
        "text": "The roofline chart can help you visualize whether an operation is compute-bound or memory bandwidth-bound. This graph is on a log scale."
      }
    },
    {
      "type": "paragraph",
      "content": "Different model architectures and workloads result in different computational bottlenecks. For example, inference for image generators like Stable Diffusion is typically compute-bound, whereas inference for autoregression language models is typically memory bandwidth-bound.",
      "raw_html": "<p>Different model architectures and workloads result in different computational bottlenecks. For example, inference for image generators like Stable Diffusion is typically compute-bound, whereas inference for autoregression language models is typically memory bandwidth-bound.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "As an illustration, let’s look into language model inference. Recall from Chapter 2 that inference for a transformer-based language model consists of two steps, prefilling and decoding:",
      "raw_html": "<p>As an illustration, let’s look into language model inference. Recall from <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a> that inference for a transformer-based language model consists of two steps, prefilling and decoding:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Prefill",
          "definition": "The model processes the input tokens in parallel.5 How many tokens can be processed at once is limited by the number of operations your hardware can execute in a given time. Therefore, prefilling is compute-bound."
        },
        {
          "term": "Decode",
          "definition": "The model generates one output token at a time. At a high level, this step typically involves loading large matrices (e.g., model weights) into GPUs, which is limited by how quickly your hardware can load data into memory. Decoding is, therefore, memory bandwidth-bound."
        }
      ],
      "raw_html": "<dl>\n<dt>Prefill</dt>\n<dd><p>The model processes the input tokens in parallel.<sup><a data-type=\"noteref\" href=\"ch09.html#id1607\" id=\"id1607-marker\">5</a></sup> How many tokens can be processed at once is limited by the number of operations your hardware can execute in a given time. Therefore, prefilling is <em>compute-bound</em>.</p></dd>\n<dt>Decode</dt>\n<dd><p>The model generates one output token at a time. At a high level, this step typically involves loading large matrices (e.g., model weights) into GPUs, which is limited by how quickly your hardware can load data into memory. Decoding is, therefore, <em>memory bandwidth-bound</em>.</p></dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "The model processes the input tokens in parallel.5 How many tokens can be processed at once is limited by the number of operations your hardware can execute in a given time. Therefore, prefilling is compute-bound.",
      "raw_html": "<p>The model processes the input tokens in parallel.<sup><a data-type=\"noteref\" href=\"ch09.html#id1607\" id=\"id1607-marker\">5</a></sup> How many tokens can be processed at once is limited by the number of operations your hardware can execute in a given time. Therefore, prefilling is <em>compute-bound</em>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The model generates one output token at a time. At a high level, this step typically involves loading large matrices (e.g., model weights) into GPUs, which is limited by how quickly your hardware can load data into memory. Decoding is, therefore, memory bandwidth-bound.",
      "raw_html": "<p>The model generates one output token at a time. At a high level, this step typically involves loading large matrices (e.g., model weights) into GPUs, which is limited by how quickly your hardware can load data into memory. Decoding is, therefore, <em>memory bandwidth-bound</em>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Figure 9-3 visualizes prefilling and decoding.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch09_figure_3_1730130962952638\">Figure 9-3</a> visualizes prefilling and decoding.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_3_1730130962952638",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_3_1730130962952638\">\n<img alt=\"A diagram of a computer\n\nDescription automatically generated\" src=\"assets/aien_0903.png\"/>\n<h6><span class=\"label\">Figure 9-3. </span>Autoregressive language models follow two steps for inference: prefill and decode. <code>&lt;eos&gt;</code> denotes the end of the sequence token.</h6>\n</div></figure>",
      "image": "aien_0903.png",
      "alt": "A diagram of a computer\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0903.png",
      "caption": {
        "label": "Figure 9-3.",
        "text": "Autoregressive language models follow two steps for inference: prefill and decode. <eos> denotes the end of the sequence token."
      }
    },
    {
      "type": "paragraph",
      "content": "Because prefill and decode have different computational profiles, they are often decoupled in production with separate machines. This technique will be discussed “Inference Service Optimization”.",
      "raw_html": "<p>Because prefill and decode have different computational profiles, they are often decoupled in production with separate machines. This technique will be discussed <a data-type=\"xref\" href=\"#ch09_inference_service_optimization_1730130963008735\">“Inference Service Optimization”</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The factors that affect the amount of prefilling and decoding computation in an LLM inference server, and therefore its bottlenecks, include context length, output length, and request batching strategies. Long context typically results in a memory bandwidth-bound workload, but clever optimization techniques, such as those discussed later in this chapter, can remove this bottleneck.",
      "raw_html": "<p>The factors that affect the amount of prefilling and decoding computation in an LLM inference server, and therefore its bottlenecks, include context length, output length, and request batching strategies. Long context typically results in a memory bandwidth-bound workload, but clever optimization techniques, such as those discussed later in this chapter, can remove this bottleneck.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "As of this writing, due to the prevalence of the transformer architecture and the limitations of the existing accelerator technologies, many AI and data workloads are memory bandwidth-bound. However, future software and hardware advancements will be able to make AI and data workloads compute-bound.",
      "raw_html": "<p>As of this writing, due to the prevalence of the transformer architecture and the limitations of the existing accelerator technologies, many AI and data workloads are memory bandwidth-bound. However, future software and hardware advancements will be able to make AI and data workloads compute-bound.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html5\" data-type=\"indexterm\" id=\"id1608\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html4\" data-type=\"indexterm\" id=\"id1609\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html3\" data-type=\"indexterm\" id=\"id1610\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Online and batch inference APIs",
      "id": "heading-7",
      "raw_html": "<h3>Online and batch inference APIs</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Many providers offer two types of inference APIs, online and batch:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference APIs\" data-type=\"indexterm\" id=\"ch09.html6b\"></a><a contenteditable=\"false\" data-primary=\"batching\" data-secondary=\"batch inference APIs\" data-type=\"indexterm\" id=\"ch09.html6a\"></a><a contenteditable=\"false\" data-primary=\"batch inference APIs\" data-type=\"indexterm\" id=\"ch09.html6\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference overview\" data-tertiary=\"online and batch inference APIs\" data-type=\"indexterm\" id=\"ch09.html7\"></a><a contenteditable=\"false\" data-primary=\"online inference APIs\" data-type=\"indexterm\" id=\"ch09.html8\"></a>Many providers offer two types of inference APIs, online and batch:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Online APIs optimize for latency. Requests are processed as soon as they arrive.",
        "Batch APIs optimize for cost. If your application doesn’t have strict latency requirements, you can send them to batch APIs for more efficient processing. Higher latency allows a broader range of optimization techniques, including batching requests together and using cheaper hardware. For example, as of this writing, both Google Gemini and OpenAI offer batch APIs at a 50% cost reduction and significantly higher turnaround time, i.e., in the order of hours instead of seconds or minutes.6"
      ],
      "raw_html": "<ul>\n<li><p>Online APIs optimize for latency. Requests are processed as soon as they arrive.</p></li>\n<li><p>Batch APIs optimize for cost. If your application doesn’t have strict latency requirements, you can send them to batch APIs for more efficient processing. Higher latency allows a broader range of optimization techniques, including batching requests together and using cheaper hardware. For example, as of this writing, both <a contenteditable=\"false\" data-primary=\"OpenAI\" data-secondary=\"batch APIs\" data-type=\"indexterm\" id=\"id1611\"></a>Google Gemini and OpenAI offer batch APIs at a 50% cost <span class=\"keep-together\">reduction</span> and significantly higher turnaround time, i.e., in the order of hours instead of seconds or minutes.<sup><a data-type=\"noteref\" href=\"ch09.html#id1612\" id=\"id1612-marker\">6</a></sup></p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Online APIs optimize for latency. Requests are processed as soon as they arrive.",
      "raw_html": "<p>Online APIs optimize for latency. Requests are processed as soon as they arrive.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Batch APIs optimize for cost. If your application doesn’t have strict latency requirements, you can send them to batch APIs for more efficient processing. Higher latency allows a broader range of optimization techniques, including batching requests together and using cheaper hardware. For example, as of this writing, both Google Gemini and OpenAI offer batch APIs at a 50% cost reduction and significantly higher turnaround time, i.e., in the order of hours instead of seconds or minutes.6",
      "raw_html": "<p>Batch APIs optimize for cost. If your application doesn’t have strict latency requirements, you can send them to batch APIs for more efficient processing. Higher latency allows a broader range of optimization techniques, including batching requests together and using cheaper hardware. For example, as of this writing, both <a contenteditable=\"false\" data-primary=\"OpenAI\" data-secondary=\"batch APIs\" data-type=\"indexterm\" id=\"id1611\"></a>Google Gemini and OpenAI offer batch APIs at a 50% cost <span class=\"keep-together\">reduction</span> and significantly higher turnaround time, i.e., in the order of hours instead of seconds or minutes.<sup><a data-type=\"noteref\" href=\"ch09.html#id1612\" id=\"id1612-marker\">6</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Online APIs might still batch requests together as long as it doesn’t significantly impact latency, as discussed in “Batching”. The only real difference is that an online API focuses on lower latency, whereas a batch API focuses on higher throughput.",
      "raw_html": "<p>Online APIs might still batch requests together as long as it doesn’t significantly impact latency, as discussed in <a data-type=\"xref\" href=\"#ch09_batching_1730130963008799\">“Batching”</a>. The only real difference is that an online API focuses on lower latency, whereas a batch API focuses on higher throughput.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Customer-facing use cases, such as chatbots and code generation, typically require lower latency, and, therefore, tend to use online APIs. Use cases with less stringent latency requirements, which are ideal for batch APIs, include the following:",
      "raw_html": "<p>Customer-facing use cases, such as chatbots and code generation, typically require lower latency, and, therefore, tend to use online APIs. Use cases with less stringent latency requirements, which are ideal for batch APIs, include the following:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Synthetic data generation",
        "Periodic reporting, such as summarizing Slack messages, sentiment analysis of brand mentions on social media, and analyzing customer support tickets",
        "Onboarding new customers who require processing of all their uploaded documents",
        "Migrating to a new model that requires reprocessing of all the data",
        "Generating personalized recommendations or newsletters for a large customer base",
        "Knowledge base updates by reindexing an organization’s data"
      ],
      "raw_html": "<ul>\n<li><p>Synthetic data generation</p></li>\n<li><p>Periodic reporting, such as summarizing Slack messages, sentiment analysis of brand mentions on social media, and analyzing customer support tickets</p></li>\n<li><p>Onboarding new customers who require processing of all their uploaded <span class=\"keep-together\">documents</span></p></li>\n<li><p>Migrating to a new model that requires reprocessing of all the data</p></li>\n<li><p>Generating personalized recommendations or newsletters for a large customer base</p></li>\n<li><p>Knowledge base updates by reindexing an organization’s data</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Synthetic data generation",
      "raw_html": "<p>Synthetic data generation</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Periodic reporting, such as summarizing Slack messages, sentiment analysis of brand mentions on social media, and analyzing customer support tickets",
      "raw_html": "<p>Periodic reporting, such as summarizing Slack messages, sentiment analysis of brand mentions on social media, and analyzing customer support tickets</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Onboarding new customers who require processing of all their uploaded documents",
      "raw_html": "<p>Onboarding new customers who require processing of all their uploaded <span class=\"keep-together\">documents</span></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Migrating to a new model that requires reprocessing of all the data",
      "raw_html": "<p>Migrating to a new model that requires reprocessing of all the data</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Generating personalized recommendations or newsletters for a large customer base",
      "raw_html": "<p>Generating personalized recommendations or newsletters for a large customer base</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Knowledge base updates by reindexing an organization’s data",
      "raw_html": "<p>Knowledge base updates by reindexing an organization’s data</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "APIs usually return complete responses by default. However, with autoregressive decoding, it can take a long time for a model to complete a response, and users are impatient. Many online APIs offer streaming mode, which returns each token as it’s generated. This reduces the time the users have to wait until the first token. The downside of this approach is that you can’t score a response before showing it to users, increasing the risk of users seeing bad responses. However, you can still retrospectively update or remove a response as soon as the risk is detected.",
      "raw_html": "<p>APIs usually return complete responses by default. However, with autoregressive decoding, it can take a long time for a model to complete a response, and users are impatient. Many online APIs offer <em>streaming mode</em>, which returns each token as it’s generated. This reduces the time the users have to wait until the first token. The downside of this approach is that you can’t score a response before showing it to users, increasing the risk of users seeing bad responses. However, you can still retrospectively update or remove a response as soon as the risk is detected.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Warning",
      "id": "heading-7",
      "raw_html": "<h6>Warning</h6>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "A batch API for foundation models differs from batch inference for traditional ML. In traditional ML:",
      "raw_html": "<p>A batch API for foundation models differs from batch inference for traditional ML. In traditional ML:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Online inference means that predictions are computed after requests have arrived.",
        "Batch inference means that predictions are precomputed before requests have arrived."
      ],
      "raw_html": "<ul>\n<li>\n<p>Online inference means that predictions are computed <em>after</em> requests have arrived.</p>\n</li>\n<li>\n<p>Batch inference means that predictions are precomputed <em>before</em> requests have arrived. </p>\n</li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Online inference means that predictions are computed after requests have arrived.",
      "raw_html": "<p>Online inference means that predictions are computed <em>after</em> requests have arrived.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Batch inference means that predictions are precomputed before requests have arrived.",
      "raw_html": "<p>Batch inference means that predictions are precomputed <em>before</em> requests have arrived. </p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Precompution is possible for use cases with finite and predictable inputs like recommendation systems, where recommendations can be generated for all users in advance. These precomputed predictions are fetched when requests arrive, e.g., when a user visits the website. However, with foundation model use cases where the inputs are open-ended, it’s hard to predict all user prompts.7",
      "raw_html": "<p>Precompution is possible for use cases with finite and predictable inputs like recommendation systems, where recommendations can be generated for all users in advance. These precomputed predictions are fetched when requests arrive, e.g., when a user visits the website. However, with foundation model use cases where the inputs are open-ended, it’s hard to predict all user prompts<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html8\" data-type=\"indexterm\" id=\"id1613\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html7\" data-type=\"indexterm\" id=\"id1614\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html6b\" data-type=\"indexterm\" id=\"id1615\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html6a\" data-type=\"indexterm\" id=\"id1616\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html6\" data-type=\"indexterm\" id=\"id1617\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html2\" data-type=\"indexterm\" id=\"id1618\"></a><sup><a data-type=\"noteref\" href=\"ch09.html#id1619\" id=\"id1619-marker\">7</a></sup> </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Inference Performance Metrics",
      "id": "heading-7",
      "raw_html": "<h2>Inference Performance Metrics</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "Before jumping into optimization, it’s important to understand what metrics to optimize for. From the user perspective, the central axis is latency (response quality is a property of the model itself, not of the inference service). However, application developers must also consider throughput and utilization as they determine the cost of their applications.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"metrics\" data-secondary=\"inference performance metrics\" data-type=\"indexterm\" id=\"ch09.html9a\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference performance metrics\" data-type=\"indexterm\" id=\"ch09.html9\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"understanding\" data-tertiary=\"inference performance metrics\" data-type=\"indexterm\" id=\"ch09.html10\"></a><a contenteditable=\"false\" data-primary=\"inference performance metrics\" data-type=\"indexterm\" id=\"ch09.html11\"></a>Before jumping into optimization, it’s important to understand what metrics to optimize for. From the user perspective, the central axis is latency (response quality is a property of the model itself, not of the inference service). However, application developers must also consider throughput and utilization as they determine the cost of their applications.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Latency, TTFT, and TPOT",
      "id": "heading-7",
      "raw_html": "<h3>Latency, TTFT, and TPOT</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Latency measures the time from when users send a query until they receive the complete response. For autoregressive generation, especially in the streaming mode, the overall latency can be broken into several metrics:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference performance metrics\" data-tertiary=\"latency, TTFT, and TPOT\" data-type=\"indexterm\" id=\"ch09.html12\"></a><a contenteditable=\"false\" data-primary=\"inference performance metrics\" data-secondary=\"latency, TTFT, and TPOT\" data-type=\"indexterm\" id=\"ch09.html13\"></a><a contenteditable=\"false\" data-primary=\"latency\" data-secondary=\"inference performance and\" data-type=\"indexterm\" id=\"ch09.html14\"></a><a contenteditable=\"false\" data-primary=\"time per output token (TPOT)\" data-type=\"indexterm\" id=\"ch09.html15\"></a><a contenteditable=\"false\" data-primary=\"time to first token (TTFT)\" data-type=\"indexterm\" id=\"ch09.html16\"></a><a contenteditable=\"false\" data-primary=\"TPOT (time per output token)\" data-type=\"indexterm\" id=\"ch09.html17\"></a><a contenteditable=\"false\" data-primary=\"TTFT (time to first token)\" data-type=\"indexterm\" id=\"ch09.html18\"></a>Latency measures the time from when users send a query until they receive the complete response. For autoregressive generation, especially in the streaming mode, the overall latency can be broken into several metrics:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Time to first token",
          "definition": "TTFT measures how quickly the first token is generated after users send a query. It corresponds to the duration of the prefill step and depends on the input’s length. Users might have different expectations for TTFT for different applications. For example, for conversational chatbots, the TTFT should be instantaneous.8 However, users might be willing to wait longer to summarize long documents."
        },
        {
          "term": "Time per output token",
          "definition": "TPOT measures how quickly each output token is generated after the first token. If each token takes 100 ms, a response of 1,000 tokens will take 100 s."
        },
        {
          "term": "Time between tokens and inter-token latency",
          "definition": "Variations of this metric include time between tokens (TBT) and inter-token latency (ITL).9 Both measure the time between output tokens."
        }
      ],
      "raw_html": "<dl>\n<dt>Time to first token </dt>\n<dd><p>TTFT measures how quickly the first token is generated after users send a query. It corresponds to the duration of the prefill step and depends on the input’s length. Users might have different expectations for TTFT for different applications. For example, for conversational chatbots, the TTFT should be instantaneous.<sup><a data-type=\"noteref\" href=\"ch09.html#id1620\" id=\"id1620-marker\">8</a></sup> However, users might be willing to wait longer to summarize long documents.</p></dd>\n<dt>Time per output token </dt>\n<dd><p>TPOT measures how quickly each output token is generated after the first token. If each token takes 100 ms, a response of 1,000 tokens will take 100 s.</p></dd>\n<dd><p>In the streaming mode, where users read each token as it’s generated, TPOT should be faster than human reading speed but doesn’t have to be much faster. A very fast reader can read 120 ms/token, so a TPOT of around 120 ms, or 6–8 tokens/second, is sufficient for most use cases.</p></dd>\n<dt>Time between tokens and inter-token latency</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"time between tokens (TBT)\" data-type=\"indexterm\" id=\"id1621\"></a><a contenteditable=\"false\" data-primary=\"inter-token latency (ITL)\" data-type=\"indexterm\" id=\"id1622\"></a>Variations of this metric include <em>time between tokens (TBT)</em> and i<em>nter-token latency (ITL)</em>.<sup><a data-type=\"noteref\" href=\"ch09.html#id1623\" id=\"id1623-marker\">9</a></sup> Both measure the time between output tokens.</p></dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "TTFT measures how quickly the first token is generated after users send a query. It corresponds to the duration of the prefill step and depends on the input’s length. Users might have different expectations for TTFT for different applications. For example, for conversational chatbots, the TTFT should be instantaneous.8 However, users might be willing to wait longer to summarize long documents.",
      "raw_html": "<p>TTFT measures how quickly the first token is generated after users send a query. It corresponds to the duration of the prefill step and depends on the input’s length. Users might have different expectations for TTFT for different applications. For example, for conversational chatbots, the TTFT should be instantaneous.<sup><a data-type=\"noteref\" href=\"ch09.html#id1620\" id=\"id1620-marker\">8</a></sup> However, users might be willing to wait longer to summarize long documents.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "TPOT measures how quickly each output token is generated after the first token. If each token takes 100 ms, a response of 1,000 tokens will take 100 s.",
      "raw_html": "<p>TPOT measures how quickly each output token is generated after the first token. If each token takes 100 ms, a response of 1,000 tokens will take 100 s.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In the streaming mode, where users read each token as it’s generated, TPOT should be faster than human reading speed but doesn’t have to be much faster. A very fast reader can read 120 ms/token, so a TPOT of around 120 ms, or 6–8 tokens/second, is sufficient for most use cases.",
      "raw_html": "<p>In the streaming mode, where users read each token as it’s generated, TPOT should be faster than human reading speed but doesn’t have to be much faster. A very fast reader can read 120 ms/token, so a TPOT of around 120 ms, or 6–8 tokens/second, is sufficient for most use cases.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Variations of this metric include time between tokens (TBT) and inter-token latency (ITL).9 Both measure the time between output tokens.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"time between tokens (TBT)\" data-type=\"indexterm\" id=\"id1621\"></a><a contenteditable=\"false\" data-primary=\"inter-token latency (ITL)\" data-type=\"indexterm\" id=\"id1622\"></a>Variations of this metric include <em>time between tokens (TBT)</em> and i<em>nter-token latency (ITL)</em>.<sup><a data-type=\"noteref\" href=\"ch09.html#id1623\" id=\"id1623-marker\">9</a></sup> Both measure the time between output tokens.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The total latency will equal TTFT + TPOT × (number of output tokens).",
      "raw_html": "<p>The total latency will equal <code>TTFT + TPOT </code>×<code> (number of output tokens).</code></p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Two applications with the same total latency can offer different user experiences with different TTFT and TPOT. Would your users prefer instant first tokens with a longer wait between tokens, or would they rather wait slightly longer for the first tokens but enjoy faster token generation afterward? User studies will be necessary to determine the optimal user experience. Reducing TTFT at the cost of higher TPOT is possible by shifting more compute instances from decoding to prefilling and vice versa.10",
      "raw_html": "<p>Two applications with the same total latency can offer different user experiences with different TTFT and TPOT. Would your users prefer instant first tokens with a longer wait between tokens, or would they rather wait slightly longer for the first tokens but enjoy faster token generation afterward? User studies will be necessary to determine the optimal user experience. Reducing TTFT at the cost of higher TPOT is possible by shifting more compute instances from decoding to prefilling and vice versa.<sup><a data-type=\"noteref\" href=\"ch09.html#id1624\" id=\"id1624-marker\">10</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "It’s important to note that the TTFT and TPOT values observed by users might differ from those observed by models, especially in scenarios involving CoT (chain-of-thought) or agentic queries where models generate intermediate steps not shown to users. Some teams use the metric time to publish to make it explicit that it measures time to the first token users see.",
      "raw_html": "<p>It’s important to note that the TTFT and TPOT values observed by users might differ from those observed by models, especially in scenarios involving CoT (chain-of-thought) or agentic queries where models generate intermediate steps not shown to users. Some teams use the metric <em>time to publish</em> to make it explicit that it measures time to the first token users see.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Consider the scenario where, after a user sends a query, the model performs the following steps:",
      "raw_html": "<p>Consider the scenario where, after a user sends a query, the model performs the following steps:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Generate a plan, which consists of a sequence of actions. This plan isn’t shown to the user.",
        "Take actions and log their outputs. These outputs aren’t shown to the user.",
        "Based on these outputs, generate a final response to show the user."
      ],
      "raw_html": "<ol>\n<li><p>Generate a plan, which consists of a sequence of actions. This plan isn’t shown to the user.</p></li>\n<li><p>Take actions and log their outputs. These outputs aren’t shown to the user.</p></li>\n<li><p>Based on these outputs, generate a final response to show the user.</p></li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Generate a plan, which consists of a sequence of actions. This plan isn’t shown to the user.",
      "raw_html": "<p>Generate a plan, which consists of a sequence of actions. This plan isn’t shown to the user.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Take actions and log their outputs. These outputs aren’t shown to the user.",
      "raw_html": "<p>Take actions and log their outputs. These outputs aren’t shown to the user.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Based on these outputs, generate a final response to show the user.",
      "raw_html": "<p>Based on these outputs, generate a final response to show the user.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "From the model’s perspective, the first token is generated in step 1. This is when the model internally begins its token generation process. The user, however, only sees the first token of the final output generated in step 3. Thus, from their perspective, TTFT is much longer.",
      "raw_html": "<p class=\"pagebreak-before\">From the model’s perspective, the first token is generated in step 1. This is when the model internally begins its token generation process. The user, however, only sees the first token of the final output generated in step 3. Thus, from their perspective, TTFT is much longer.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because latency is a distribution, the average can be misleading. Imagine you have 10 requests whose TTFT values are 100 ms, 102 ms, 100 ms, 100 ms, 99 ms, 104 ms, 110 ms, 90 ms, 3,000 ms, 95 ms. The average TTFT value is 390 ms, which makes your inference service seem slower than it is. There might have been a network error that slowed down one request or a particularly long prompt that took a much longer time to prefill. Either way, you should investigate. With a large volume of requests, outliers that skew the average latency are almost inevitable.",
      "raw_html": "<p>Because latency is a distribution, the average can be misleading. Imagine you have 10 requests whose TTFT values are 100 ms, 102 ms, 100 ms, 100 ms, 99 ms, 104 ms, 110 ms, 90 ms, 3,000 ms, 95 ms. The average TTFT value is 390 ms, which makes your inference service seem slower than it is. There might have been a network error that slowed down one request or a particularly long prompt that took a much longer time to prefill. Either way, you should investigate. With a large volume of requests, outliers that skew the average latency are almost inevitable.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "It’s more helpful to look at latency in percentiles, as they tell you something about a certain percentage of your requests. The most common percentile is the 50th percentile, abbreviated as p50 (median). If the median is 100 ms, half of the requests take longer than 100 ms to generate the first token, and half take less than 100 ms. Percentiles also help you discover outliers, which might be symptoms of something wrong. Typically, the percentiles you’ll want to look at are p90, p95, and p99. It’s also helpful to plot TTFT values against inputs’ lengths.",
      "raw_html": "<p>It’s more helpful to look at latency in percentiles, as they tell you something about a certain percentage of your requests. The most common percentile is the 50th percentile, abbreviated as p50 (median). If the median is 100 ms, half of the requests take longer than 100 ms to generate the first token, and half take less than 100 ms. Percentiles also help you discover outliers, which might be symptoms of something wrong. Typically, the percentiles you’ll want to look at are p90, p95, and p99. It’s also helpful to plot TTFT values against inputs’ lengths.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html18\" data-type=\"indexterm\" id=\"id1625\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html17\" data-type=\"indexterm\" id=\"id1626\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html16\" data-type=\"indexterm\" id=\"id1627\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html15\" data-type=\"indexterm\" id=\"id1628\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html14\" data-type=\"indexterm\" id=\"id1629\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html13\" data-type=\"indexterm\" id=\"id1630\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html12\" data-type=\"indexterm\" id=\"id1631\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Throughput and goodput",
      "id": "heading-7",
      "raw_html": "<h3>Throughput and goodput</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Throughput measures the number of output tokens per second an inference service can generate across all users and requests.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference service\" data-secondary=\"throughput/goodput\" data-type=\"indexterm\" id=\"ch09.html19a\"></a><a contenteditable=\"false\" data-primary=\"goodput\" data-type=\"indexterm\" id=\"ch09.html19\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference performance metrics\" data-tertiary=\"throughput/goodput\" data-type=\"indexterm\" id=\"ch09.html20\"></a><a contenteditable=\"false\" data-primary=\"inference performance metrics\" data-secondary=\"throughput/goodput\" data-type=\"indexterm\" id=\"ch09.html21\"></a><a contenteditable=\"false\" data-primary=\"throughput\" data-type=\"indexterm\" id=\"ch09.html22\"></a>Throughput measures the number of output tokens per second an inference service can generate across all users and requests.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Some teams count both input and output tokens in throughput calculation. However, since processing input tokens (prefilling) and generating output tokens (decoding) have different computational bottlenecks and are often decoupled in modern inference servers, input and output throughput should be counted separately. When throughput is used without any modifier, it usually refers to output tokens.",
      "raw_html": "<p>Some teams count both input and output tokens in throughput calculation. However, since processing input tokens (prefilling) and generating output tokens (decoding) have different computational bottlenecks and are often decoupled in modern inference servers, input and output throughput should be counted separately. When throughput is used without any modifier, it usually refers to output tokens.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Throughput is typically measured as tokens/s (TPS). If you serve multiple users, tokens/s/user is also used to evaluate how the system scales with more users.",
      "raw_html": "<p>Throughput is typically measured as tokens/s (TPS). If you serve multiple users, tokens/s/user is also used to evaluate how the system scales with more users.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Throughput can also be measured as the number of completed requests during a given time. Many applications use requests per second (RPS). However, for applications built on top of foundation models, a request might take seconds to complete, so many people use completed requests per minute (RPM) instead. Tracking this metric is useful for understanding how an inference service handles concurrent requests. Some providers might throttle your service if you send too many concurrent requests at the same time.",
      "raw_html": "<p>Throughput can also be measured as the number of <em>completed</em> requests during a given time. Many applications use requests per second (RPS). However, for applications built on top of foundation models, a request might take seconds to complete, so many people use completed requests per minute (RPM) instead. Tracking this metric is useful for understanding how an inference service handles concurrent requests. Some providers might throttle your service if you send too many concurrent requests at the same time.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Throughput is directly linked to compute cost. A higher throughput typically means lower cost. If your system costs $2/h in compute and its throughput is 100 tokens/s, it costs around $5.556 per 1M output tokens. If each request generates 200 output tokens on average, the cost for decoding 1K requests would be $1.11.",
      "raw_html": "<p class=\"pagebreak-before\">Throughput is directly linked to compute cost. A higher throughput typically means lower cost. If your system costs $2/h in compute and its throughput is 100 tokens/s, it costs around $5.556 per 1M output tokens. If each request generates 200 output tokens on average, the cost for decoding 1K requests would be $1.11.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The prefill cost can be similarly calculated. If your hardware costs $2 per hour and it can prefill 100 requests per minute, the cost for prefilling 1K requests would be $0.33.",
      "raw_html": "<p>The prefill cost can be similarly calculated. If your hardware costs $2 per hour and it can prefill 100 requests per minute, the cost for prefilling 1K requests would be $0.33.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The total cost per request is the sum of the prefilling and decoding costs. In this example, the total cost for 1K requests would be $1.11 + $0.33 = $1.44.",
      "raw_html": "<p>The total cost per request is the sum of the prefilling and decoding costs. In this example, the total cost for 1K requests would be $1.11 + $0.33 = $1.44.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "What’s considered good throughput depends on the model, the hardware, and the workload. Smaller models and higher-end chips typically result in higher throughput. Workloads with consistent input and output lengths are easier to optimize than workloads with variable lengths.",
      "raw_html": "<p>What’s considered good throughput depends on the model, the hardware, and the workload. Smaller models and higher-end chips typically result in higher throughput. Workloads with consistent input and output lengths are easier to optimize than workloads with variable lengths.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Even for similarly sized models, hardware, and workloads, direct throughput comparisons might be only approximate because token count depends on what constitutes a token, and different models have different tokenizers. It’s better to compare the efficiency of inference servers using metrics such as cost per request.",
      "raw_html": "<p>Even for similarly sized models, hardware, and workloads, direct throughput comparisons might be only approximate because token count depends on what constitutes a token, and different models have different tokenizers. It’s better to compare the efficiency of inference servers using metrics such as cost per request. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Just like most other software applications, AI applications have the latency/throughput trade-off. Techniques like batching can improve throughput but reduce latency. According to the LinkedIn AI team in their reflection after a year of deploying generative AI products (LinkedIn, 2024), it’s not uncommon to double or triple the throughput if you’re willing to sacrifice TTFT and TPOT.",
      "raw_html": "<p>Just like most other software applications, AI applications have the latency/throughput trade-off. Techniques like batching can improve throughput but reduce latency. According to the LinkedIn AI team in their reflection after a year of deploying generative AI products (<a href=\"https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US\">LinkedIn, 2024</a>), it’s not uncommon to double or triple the throughput if you’re willing to sacrifice TTFT and TPOT.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Due to this trade-off, focusing on an inference service based solely on its throughput and cost can lead to a bad user experience. Instead, some teams focus on goodput, a metric adapted from networking for LLM applications. Goodput measures the number of requests per second that satisfies the SLO, software-level objective.",
      "raw_html": "<p>Due to this trade-off, focusing on an inference service based solely on its throughput and cost can lead to a bad user experience. Instead, some teams focus on <a href=\"https://en.wikipedia.org/wiki/Goodput\"><em>goodput</em></a>, a metric adapted from networking for LLM applications. Goodput measures the number of requests per second that satisfies the SLO, software-level objective.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Imagine that your application has the following objectives: TTFT of at most 200 ms and TPOT of at most 100 ms. Let’s say that your inference service can complete 100 requests per minute. However, out of these 100 requests, only 30 satisfy the SLO. Then, the goodput of this service is 30 requests per minute. A visualization of this is shown in Figure 9-4.",
      "raw_html": "<p>Imagine that your application has the following objectives: TTFT of at most 200 ms and TPOT of at most 100 ms. Let’s say that your inference service can complete 100 requests per minute. However, out of these 100 requests, only 30 satisfy the SLO. Then, the goodput of this service is 30 requests per minute.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html22\" data-type=\"indexterm\" id=\"id1632\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html21\" data-type=\"indexterm\" id=\"id1633\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html20\" data-type=\"indexterm\" id=\"id1634\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html19a\" data-type=\"indexterm\" id=\"id1635\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html19\" data-type=\"indexterm\" id=\"id1636\"></a> A visualization of this is shown in <a data-type=\"xref\" href=\"#ch09_figure_4_1730130962952660\">Figure 9-4</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_4_1730130962952660",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_4_1730130962952660\">\n<img alt=\"A graph showing different colored bars\n\nDescription automatically generated\" src=\"assets/aien_0904.png\"/>\n<h6><span class=\"label\">Figure 9-4. </span>If an inference service can complete 10 RPS but only 3 satisfy the SLO, then its goodput is 3 RPS.</h6>\n</div></figure>",
      "image": "aien_0904.png",
      "alt": "A graph showing different colored bars\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0904.png",
      "caption": {
        "label": "Figure 9-4.",
        "text": "If an inference service can complete 10 RPS but only 3 satisfy the SLO, then its goodput is 3 RPS."
      }
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Utilization, MFU, and MBU",
      "id": "heading-7",
      "raw_html": "<h3>Utilization, MFU, and MBU</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Utilization metrics measure how efficiently a resource is being used. It typically quantifies the proportion of the resource actively being used compared to its total available capacity.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference performance metrics\" data-tertiary=\"utilization, MFU, and MBU\" data-type=\"indexterm\" id=\"ch09.html23\"></a><a contenteditable=\"false\" data-primary=\"inference performance metrics\" data-secondary=\"utilization, MFU, and MBU\" data-type=\"indexterm\" id=\"ch09.html24\"></a><a contenteditable=\"false\" data-primary=\"MBU (model bandwidth utilization)\" data-type=\"indexterm\" id=\"ch09.html25\"></a><a contenteditable=\"false\" data-primary=\"MFU (model FLOPs utilization)\" data-type=\"indexterm\" id=\"ch09.html26\"></a><a contenteditable=\"false\" data-primary=\"model bandwidth utilization (MBU)\" data-type=\"indexterm\" id=\"ch09.html27\"></a><a contenteditable=\"false\" data-primary=\"model FLOPs utilization (MFU)\" data-type=\"indexterm\" id=\"ch09.html28\"></a>Utilization metrics measure how efficiently a resource is being used. It typically quantifies the proportion of the resource actively being used compared to its total available capacity.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A common but often misunderstood metric is GPU utilization, and NVIDIA is partially to blame for this misunderstanding. The official NVIDIA tool for monitoring GPU usage is nvidia-smi—SMI stands for System Management Interface. One metric this tool shows is GPU utilization, which represents the percentage of time during which the GPU is actively processing tasks. For example, if you run inference on a GPU cluster for 10 hours, and the GPUs are actively processing tasks for 5 of those hours, your GPU utilization would be 50%.",
      "raw_html": "<p>A common but often misunderstood metric is <em>GPU utilization</em>, and NVIDIA is partially to blame for this misunderstanding. The official NVIDIA tool for monitoring GPU usage is <a href=\"https://oreil.ly/ludJ2\"><code>nvidia-smi</code></a>—SMI stands for System Management Interface. One metric this tool shows is GPU utilization, which represents the percentage of time during which the GPU is actively processing tasks. For example, if you run inference on a GPU cluster for 10 hours, and the GPUs are actively processing tasks for 5 of those hours, your GPU utilization would be 50%.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "However, actively processing tasks doesn’t mean doing so efficiently. For simplicity, consider a tiny GPU capable of doing 100 operations per second. In nvidia-smi’s definition of utilization, this GPU can report 100% utilization even if it’s only doing one operation per second.",
      "raw_html": "<p>However, actively processing tasks doesn’t mean doing so efficiently. For simplicity, consider a tiny GPU capable of doing 100 operations per second. In <code>nvidia-smi</code>’s definition of utilization, this GPU can report 100% utilization even if it’s only doing one operation per second.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If you pay for a machine that can do 100 operations and use it for only 1 operation, you’re wasting money. nvidia-smi’s GPU optimization metric is, therefore, not very useful. A utilization metric you might care about, out of all the operations a machine is capable of computing, is how many it’s doing in a given time. This metric is called MFU (Model FLOP/s Utilization), which distinguishes it from the NVIDIA GPU utilization metric.",
      "raw_html": "<p>If you pay for a machine that can do 100 operations and use it for only 1 operation, you’re wasting money. <code>nvidia-smi</code>’s GPU optimization metric is, therefore, not very useful. A utilization metric you might care about, out of all the operations a machine is capable of computing, is how many it’s doing in a given time. This metric is called <em>MFU (Model FLOP/s Utilization)</em>, which distinguishes it from the NVIDIA GPU utilization metric.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "MFU is the ratio of the observed throughput (tokens/s) relative to the theoretical maximum throughput of a system operating at peak FLOP/s. If at the peak FLOP/s advertised by the chip maker, the chip can generate 100 tokens/s, but when used for your inference service, it can generate only 20 tokens/s, your MFU is 20%.11",
      "raw_html": "<p>MFU is the ratio of the observed throughput (tokens/s) relative to the theoretical maximum throughput of a system operating at peak FLOP/s. If at the peak FLOP/s advertised by the chip maker, the chip can generate 100 tokens/s, but when used for your inference service, it can generate only 20 tokens/s, your MFU is 20%.<sup><a data-type=\"noteref\" href=\"ch09.html#id1637\" id=\"id1637-marker\">11</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Similarly, because memory bandwidth is expensive, you might also want to know how efficiently your hardware’s bandwidth is utilized. MBU (Model Bandwidth Utilization) measures the percentage of achievable memory bandwidth used. If the chip’s peak bandwidth is 1 TB/s and your inference uses only 500 GB/s, your MBU is 50%.",
      "raw_html": "<p>Similarly, because memory bandwidth is expensive, you might also want to know how efficiently your hardware’s bandwidth is utilized. <em>MBU (Model Bandwidth Utilization)</em> measures the percentage of achievable memory bandwidth used. If the chip’s peak bandwidth is 1 TB/s and your inference uses only 500 GB/s, your MBU is 50%.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Computing the memory bandwidth being used for LLM inference is straightforward:",
      "raw_html": "<p>Computing the memory bandwidth being used for LLM inference is straightforward:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "parameter count × bytes/param × tokens/s",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">parameter count × bytes/param × tokens/s</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "MBU is computed as follows:",
      "raw_html": "<p>MBU is computed as follows:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "(parameter count × bytes/param × tokens/s) / (theoretical bandwidth)",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">(parameter count × bytes/param × tokens/s) / (theoretical bandwidth)</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "For example, if you use a 7B-parameter model in FP16 (two bytes per parameter) and achieve 100 tokens/s, the bandwidth used is:",
      "raw_html": "<p>For example, if you use a 7B-parameter model in FP16 (two bytes per parameter) and achieve 100 tokens/s, the bandwidth used is:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "7B × 2 × 100 = 700 GB/s",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">7B × 2 × 100 = 700 GB/s</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "This underscores the importance of quantization (discussed in Chapter 7). Fewer bytes per parameter mean your model consumes less valuable bandwidth.",
      "raw_html": "<p>This underscores the importance of quantization (discussed in <a data-type=\"xref\" href=\"ch07.html#ch07\">Chapter 7</a>). Fewer bytes per parameter mean your model consumes less valuable bandwidth.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If this is done on an A100-80GB GPU with a theoretical 2 TB/s of memory bandwidth, the MBU is:",
      "raw_html": "<p>If this is done on an A100-80GB GPU with a theoretical 2 TB/s of memory bandwidth, the MBU is:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "code",
      "content": "(700 GB/s) / (2 TB/s) = 70%",
      "language": "text",
      "raw_html": "<pre data-type=\"programlisting\">(700 GB/s) / (2 TB/s) = 70%</pre>",
      "data_type": "programlisting",
      "line_count": 1
    },
    {
      "type": "paragraph",
      "content": "The relationships between throughput (tokens/s) and MBU and between throughput and MFU are linear, so some people might use throughput to refer to MBU and MFU.",
      "raw_html": "<p>The relationships between throughput (tokens/s) and MBU and between throughput and MFU are linear, so some people might use throughput to refer to MBU and MFU.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "What’s considered a good MFU and MBU depends on the model, hardware, and workload. Compute-bound workloads typically have higher MFU and lower MBU, while bandwidth-bound workloads often show lower MFU and higher MBU.",
      "raw_html": "<p>What’s considered a good MFU and MBU depends on the model, hardware, and workload. Compute-bound workloads typically have higher MFU and lower MBU, while bandwidth-bound workloads often show lower MFU and higher MBU.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Because training can benefit from more efficient optimization (e.g., better batching), thanks to having more predictable workloads, MFU for training is typically higher than MFU for inference. For inference, since prefill is compute-bound and decode is memory bandwidth-bound, MFU during prefilling is typically higher than MFU during decoding. For model training, as of this writing, an MFU above 50% is generally considered good, but it can be hard to achieve on specific hardware.12 Table 9-1 shows MFU for several models and accelerators.",
      "raw_html": "<p>Because training can benefit from more efficient optimization (e.g., better batching), thanks to having more predictable workloads, MFU for training is typically higher than MFU for inference. For inference, since prefill is compute-bound and decode is memory bandwidth-bound, MFU during prefilling is typically higher than MFU during decoding. For model training, as of this writing, an MFU above 50% is generally considered good, but it can be hard to achieve on specific hardware.<sup><a data-type=\"noteref\" href=\"ch09.html#id1638\" id=\"id1638-marker\">12</a></sup> <a data-type=\"xref\" href=\"#ch09_table_1_1730130962971021\">Table 9-1</a> shows MFU for several models and accelerators. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch09_table_1_1730130962971021",
      "raw_html": "<table id=\"ch09_table_1_1730130962971021\">\n<caption><span class=\"label\">Table 9-1. </span>MFU examples from “PaLM: Scaling Language Modeling with Pathways” (Chowdhery et al., 2022).</caption>\n<thead>\n<tr>\n<th>Model</th>\n<th>Number of parameters (in billions)</th>\n<th>Accelerator chips</th>\n<th>Model FLOP/s utilization</th>\n</tr>\n</thead>\n<tr>\n<td>GPT-3</td>\n<td>175B</td>\n<td>V100</td>\n<td>21.3%</td>\n</tr>\n<tr>\n<td>Gopher</td>\n<td>280B</td>\n<td>4096 TPU v3</td>\n<td>32.5%</td>\n</tr>\n<tr>\n<td>Megatron-Turing NLG</td>\n<td>530B</td>\n<td>2240 A100</td>\n<td>30.2%</td>\n</tr>\n<tr>\n<td>PaLM</td>\n<td>540B</td>\n<td>6144 TPU v4</td>\n<td>46.2%</td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 9-1.",
        "text": "MFU examples from “PaLM: Scaling Language Modeling with Pathways” (Chowdhery et al., 2022)."
      },
      "headers": [
        "Model",
        "Number of parameters (in billions)",
        "Accelerator chips",
        "Model FLOP/s utilization"
      ],
      "rows": [
        [
          "GPT-3",
          "175B",
          "V100",
          "21.3%"
        ],
        [
          "Gopher",
          "280B",
          "4096 TPU v3",
          "32.5%"
        ],
        [
          "Megatron-Turing NLG",
          "530B",
          "2240 A100",
          "30.2%"
        ],
        [
          "PaLM",
          "540B",
          "6144 TPU v4",
          "46.2%"
        ]
      ],
      "row_count": 4,
      "column_count": 4
    },
    {
      "type": "paragraph",
      "content": "Figure 9-5 shows the MBU for the inference process using Llama 2-70B in FP16 on different hardware. The decline is likely due to the higher computational load per second with more users, shifting the workload from being bandwidth-bound to compute-bound.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch09_figure_5_1730130962952692\">Figure 9-5</a> shows the MBU for the inference process using Llama 2-70B in FP16 on different hardware. The decline is likely due to the higher computational load per second with more users, shifting the workload from being bandwidth-bound to compute-bound.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_5_1730130962952692",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_5_1730130962952692\">\n<img alt=\"A graph of a number of users\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0905.png\"/>\n<h6><span class=\"label\">Figure 9-5. </span>Bandwidth utilization for Llama 2-70B in FP16 across three different chips shows a decrease in MBU as the number of concurrent users increases. Image from “LLM Training and Inference with Intel Gaudi 2 AI Accelerators” (<a href=\"https://oreil.ly/tOOOD\">Databricks, 2024</a>).</h6>\n</div></figure>",
      "image": "aien_0905.png",
      "alt": "A graph of a number of users\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0905.png",
      "caption": {
        "label": "Figure 9-5.",
        "text": "Bandwidth utilization for Llama 2-70B in FP16 across three different chips shows a decrease in MBU as the number of concurrent users increases. Image from “LLM Training and Inference with Intel Gaudi 2 AI Accelerators” (Databricks, 2024)."
      }
    },
    {
      "type": "paragraph",
      "content": "Utilization metrics are helpful to track your system’s efficiency. Higher utilization rates for similar workloads on the same hardware generally mean that your services are becoming more efficient. However, the goal isn’t to get the chips with the highest utilization. What you really care about is how to get your jobs done faster and cheaper. A higher utilization rate means nothing if the cost and latency both increase.",
      "raw_html": "<p>Utilization metrics are helpful to track your system’s efficiency. Higher utilization rates for similar workloads on the same hardware generally mean that your services are becoming more efficient. However, <em>the goal isn’t to get the chips with the highest utilization</em>. What you really care about is how to get your jobs done faster and cheaper. A higher utilization rate means nothing if the cost and latency both increase<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html28\" data-type=\"indexterm\" id=\"id1639\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html27\" data-type=\"indexterm\" id=\"id1640\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html26\" data-type=\"indexterm\" id=\"id1641\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html25\" data-type=\"indexterm\" id=\"id1642\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html24\" data-type=\"indexterm\" id=\"id1643\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html23\" data-type=\"indexterm\" id=\"id1644\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html11\" data-type=\"indexterm\" id=\"id1645\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html10\" data-type=\"indexterm\" id=\"id1646\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html9\" data-type=\"indexterm\" id=\"id1647\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html9a\" data-type=\"indexterm\" id=\"id1648\"></a></p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 2,
      "content": "AI Accelerators",
      "id": "heading-7",
      "raw_html": "<h2>AI Accelerators</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "How fast and cheap software can run depends on the hardware it runs on. While there are optimization techniques that work across hardware, understanding hardware allows for deeper optimization. This section looks at hardware from an inference perspective, but it can be applied to training as well.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"accelerators\" data-type=\"indexterm\" id=\"ch09.html29\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"understanding\" data-tertiary=\"AI accelerators\" data-type=\"indexterm\" id=\"ch09.html30\"></a>How fast and cheap software can run depends on the hardware it runs on. While there are optimization techniques that work across hardware, understanding hardware allows for deeper optimization. This section looks at hardware from an inference perspective, but it can be applied to training as well.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The development of AI models and hardware has always been intertwined. The lack of sufficiently powerful computers was one of the contributing factors to the first AI winter in the 1970s.13",
      "raw_html": "<p>The development of AI models and hardware has always been intertwined. The lack of sufficiently powerful computers was one of the contributing factors to the first AI winter in the 1970s.<sup><a data-type=\"noteref\" href=\"ch09.html#id1649\" id=\"id1649-marker\">13</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The revival of interest in deep learning in 2012 was also closely tied to compute. One commonly acknowledged reason for the popularity of AlexNet (Krizhevsky et al., 2012) is that it was the first paper to successfully use GPUs, graphics processing units, to train neural networks.14 Before GPUs, if you wanted to train a model at AlexNet’s scale, you’d have to use thousands of CPUs, like the one Google released just a few months before AlexNet. Compared to thousands of CPUs, a couple of GPUs were a lot more accessible to PhD students and researchers, setting off the deep learning research boom.",
      "raw_html": "<p>The revival of interest in deep learning in 2012 was also closely tied to compute. One commonly acknowledged reason for the popularity of AlexNet (<a href=\"https://oreil.ly/Yv4V7\">Krizhevsky et al., 2012</a>) is that it was the first paper to successfully use <a href=\"https://en.wikipedia.org/wiki/Graphics_processing_unit\">GPUs</a>, graphics processing units, to train neural networks.<sup><a data-type=\"noteref\" href=\"ch09.html#id1650\" id=\"id1650-marker\">14</a></sup> Before GPUs, if you wanted to train a model at AlexNet’s scale, you’d have to use thousands of CPUs, like the one <a href=\"https://oreil.ly/Xpwco\">Google released just a few months before AlexNet</a>. Compared to thousands of CPUs, a couple of GPUs were a lot more accessible to PhD students and researchers, setting off the deep learning research boom.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "What’s an accelerator?",
      "id": "heading-7",
      "raw_html": "<h3 class=\"less_space\">What’s an accelerator?</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "An accelerator is a chip designed to accelerate a specific type of computational workload. An AI accelerator is designed for AI workloads. The dominant type of AI accelerator is GPUs, and the biggest economic driver during the AI boom in the early 2020s is undoubtedly NVIDIA.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"accelerators\" data-secondary=\"defined\" data-type=\"indexterm\" id=\"ch09.html31\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"AI accelerators\" data-tertiary=\"defined\" data-type=\"indexterm\" id=\"ch09.html32\"></a>An accelerator is a chip designed to accelerate a specific type of computational workload. An AI accelerator is designed for AI workloads. The dominant type of AI accelerator is GPUs, and the biggest economic driver during the AI boom in the early 2020s is undoubtedly NVIDIA.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The main difference between CPUs and GPUs is that CPUs are designed for general-purpose usage, whereas GPUs are designed for parallel processing:",
      "raw_html": "<p>The main difference between CPUs and GPUs is that CPUs are designed for general-purpose usage, whereas GPUs are designed for parallel processing:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "CPUs have a few powerful cores, typically up to 64 cores for high-end consumer machines. While many CPU cores can handle multi-threaded workloads effectively, they excel at tasks requiring high single-thread performance, such as running an operating system, managing I/O (input/output) operations, or handling complex, sequential processes.",
        "GPUs have thousands of smaller, less powerful cores optimized for tasks that can be broken down into many smaller, independent calculations, such as graphics rendering and machine learning. The operation that constitutes most ML workloads is matrix multiplication, which is highly parallelizable.15"
      ],
      "raw_html": "<ul>\n<li><p>CPUs have a few powerful cores, typically up to 64 cores for high-end consumer machines. While many CPU cores can handle multi-threaded workloads effectively, they excel at tasks requiring high single-thread performance, such as running an operating system, managing I/O (input/output) operations, or handling complex, sequential processes.</p></li>\n<li><p>GPUs have thousands of smaller, less powerful cores optimized for tasks that can be broken down into many smaller, independent calculations, such as graphics rendering and machine learning. The operation that constitutes most ML workloads is matrix multiplication, which is highly parallelizable.<sup><a data-type=\"noteref\" href=\"ch09.html#id1651\" id=\"id1651-marker\">15</a></sup></p>\n</li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "CPUs have a few powerful cores, typically up to 64 cores for high-end consumer machines. While many CPU cores can handle multi-threaded workloads effectively, they excel at tasks requiring high single-thread performance, such as running an operating system, managing I/O (input/output) operations, or handling complex, sequential processes.",
      "raw_html": "<p>CPUs have a few powerful cores, typically up to 64 cores for high-end consumer machines. While many CPU cores can handle multi-threaded workloads effectively, they excel at tasks requiring high single-thread performance, such as running an operating system, managing I/O (input/output) operations, or handling complex, sequential processes.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "GPUs have thousands of smaller, less powerful cores optimized for tasks that can be broken down into many smaller, independent calculations, such as graphics rendering and machine learning. The operation that constitutes most ML workloads is matrix multiplication, which is highly parallelizable.15",
      "raw_html": "<p>GPUs have thousands of smaller, less powerful cores optimized for tasks that can be broken down into many smaller, independent calculations, such as graphics rendering and machine learning. The operation that constitutes most ML workloads is matrix multiplication, which is highly parallelizable.<sup><a data-type=\"noteref\" href=\"ch09.html#id1651\" id=\"id1651-marker\">15</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While the pursuit of efficient parallel processing increases computational capabilities, it imposes challenges on memory design and power consumption.",
      "raw_html": "<p>While the pursuit of efficient parallel processing increases computational capabilities, it imposes challenges on memory design and power consumption.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The success of NVIDIA GPUs has inspired many accelerators designed to speed up AI workloads, including Advanced Micro Devices (AMD)’s newer generations of GPUs, Google’s TPU (Tensor Processing Unit), Intel’s Habana Gaudi, Graphcore’s Intelligent Processing Unit (IPU), Groq’s Language Processing Unit (LPU), Cerebras’ Wafer-Scale Quant Processing Unit (QPU), and many more being introduced.",
      "raw_html": "<p>The success of NVIDIA GPUs has inspired many accelerators designed to speed up AI workloads, including <a href=\"https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units\">Advanced Micro Devices (AMD)’s newer generations of GPUs</a>, Google’s TPU (<a href=\"https://en.wikipedia.org/wiki/Tensor_Processing_Unit\">Tensor Processing Unit</a>), <a href=\"https://oreil.ly/oDQOk\">Intel’s Habana Gaudi</a>, <a href=\"https://oreil.ly/6ySTY\">Graphcore’s Intelligent Processing Unit</a> (IPU), <a href=\"https://oreil.ly/R7gXn\">Groq’s Language Processing Unit</a> (LPU), <a href=\"https://oreil.ly/ACIty\">Cerebras’ Wafer-Scale</a> <a href=\"https://en.wikipedia.org/wiki/List_of_quantum_processors\">Quant Processing Unit</a> (QPU), and many more being introduced.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While many chips can handle both training and inference, one big theme emerging is specialized chips for inference. A survey by Desislavov et al. (2023) shares that inference can exceed the cost of training in commonly used systems, and that inference accounts for up to 90% of the machine learning costs for deployed AI systems.",
      "raw_html": "<p>While many chips can handle both training and inference, one big theme emerging is specialized chips for inference. A survey by <a href=\"https://oreil.ly/qSpMK\">Desislavov et al. (2023)</a> shares that inference can exceed the cost of training in commonly used systems, and that inference accounts for up to 90% of the machine learning costs for deployed AI systems.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "As discussed in Chapter 7, training demands much more memory due to backpropagation and is generally more difficult to perform in lower precision. Furthermore, training usually emphasizes throughput, whereas inference aims to minimize latency.",
      "raw_html": "<p class=\"pagebreak-before\">As discussed in <a data-type=\"xref\" href=\"ch07.html#ch07\">Chapter 7</a>, training demands much more memory due to backpropagation and is generally more difficult to perform in lower precision. Furthermore, training usually emphasizes throughput, whereas inference aims to minimize latency.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Consequently, chips designed for inference are often optimized for lower precision and faster memory access, rather than large memory capacity. Examples of such chips include the Apple Neural Engine, AWS Inferentia, and MTIA (Meta Training and Inference Accelerator). Chips designed for edge computing, like Google’s Edge TPU and the NVIDIA Jetson Xavier, are also typically geared toward inference.",
      "raw_html": "<p>Consequently, chips designed for inference are often optimized for lower precision and faster memory access, rather than large memory capacity. Examples of such chips include the Apple <a href=\"https://en.wikipedia.org/wiki/Neural_Engine\">Neural Engine</a>, <a href=\"https://oreil.ly/42LSB\">AWS Inferentia</a>, and <a href=\"https://oreil.ly/XH2bh\">MTIA</a> (Meta Training and Inference Accelerator). Chips designed for edge computing, like <a href=\"https://oreil.ly/m8daG\">Google’s Edge TPU</a> and the <a href=\"https://oreil.ly/PRZSQ\">NVIDIA Jetson Xavier</a>, are also typically geared toward inference.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "There are also chips specialized for different model architectures, such as chips specialized for the transformer.16 Many chips are designed for data centers, with more and more being designed for consumer devices (such as phones and laptops).",
      "raw_html": "<p>There are also chips specialized for different model architectures, such as chips specialized for the transformer.<sup><a data-type=\"noteref\" href=\"ch09.html#id1652\" id=\"id1652-marker\">16</a></sup> Many chips are designed for data centers, with more and more being designed for consumer devices (such as phones and laptops).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Different hardware architectures have different memory layouts and specialized compute units that evolve over time. These units are optimized for specific data types, such as scalars, vectors, or tensors, as shown in Figure 9-6.",
      "raw_html": "<p>Different hardware architectures have different memory layouts and specialized compute units that evolve over time. These units are optimized for specific data types, such as scalars, vectors, or tensors, as shown in <a data-type=\"xref\" href=\"#ch09_figure_6_1730130962952710\">Figure 9-6</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_6_1730130962952710",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_6_1730130962952710\">\n<img alt=\"A diagram of a computer\n\nDescription automatically generated\" src=\"assets/aien_0906.png\"/>\n<h6><span class=\"label\">Figure 9-6. </span>Different compute primitives. Image inspired by <a href=\"https://arxiv.org/abs/1802.04799\">Chen et al. (2018)</a>.</h6>\n</div></figure>",
      "image": "aien_0906.png",
      "alt": "A diagram of a computer\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0906.png",
      "caption": {
        "label": "Figure 9-6.",
        "text": "Different compute primitives. Image inspired by Chen et al. (2018)."
      }
    },
    {
      "type": "paragraph",
      "content": "A chip might have a mixture of different compute units optimized for various data types. For example, GPUs traditionally supported vector operations, but many modern GPUs now include tensor cores optimized for matrix and tensor computations. TPUs, on the other hand, are designed with tensor operations as their primary compute primitive. To efficiently operate a model on a hardware architecture, its memory layout and compute primitives need to be taken into account.",
      "raw_html": "<p>A chip might have a mixture of different compute units optimized for various data types. For example, GPUs traditionally supported vector operations, but many modern GPUs now include tensor cores optimized for matrix and tensor computations. TPUs, on the other hand, are designed with tensor operations as their primary compute primitive. To efficiently operate a model on a hardware architecture, its memory layout and compute primitives need to be taken into account.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A chip’s specifications contain many details that can be useful when evaluating this chip for each specific use case. However, the main characteristics that matter across use cases are computational capabilities, memory size and bandwidth, and power consumption. I’ll use GPUs as examples to illustrate these characteristics.",
      "raw_html": "<p>A chip’s specifications contain many details that can be useful when evaluating this chip for each specific use case. However, the main characteristics that matter across use cases are computational capabilities, memory size and bandwidth, and power consumption. I’ll use GPUs as examples to illustrate these characteristics.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html32\" data-type=\"indexterm\" id=\"id1653\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html31\" data-type=\"indexterm\" id=\"id1654\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Computational capabilities",
      "id": "heading-7",
      "raw_html": "<h3>Computational capabilities</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Computational capabilities are typically measured by the number of operations a chip can perform in a given time. The most common metric is FLOP/s, often written as FLOPS, which measures the peak number of floating-point operations per second. In reality, however, it’s very unlikely that an application can achieve this peak FLOP/s. The ratio between the actual FLOP/s and the theoretical FLOP/s is one utilization metric.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"accelerators\" data-secondary=\"computational capabilities\" data-type=\"indexterm\" id=\"id1655\"></a><a contenteditable=\"false\" data-primary=\"computational capabilities, of AI accelerators\" data-type=\"indexterm\" id=\"id1656\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"AI accelerators\" data-tertiary=\"computational capabilities\" data-type=\"indexterm\" id=\"id1657\"></a>Computational capabilities are typically measured by the number of operations a chip can perform in a given time. The most common metric is <em>FLOP/s</em>, often written as FLOPS, which measures the <em>peak</em> number of floating-point operations per second. In reality, however, it’s very unlikely that an application can achieve this peak FLOP/s. The ratio between the actual FLOP/s and the theoretical FLOP/s is one <em>utilization</em> metric.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The number of operations a chip can perform in a second depends on the numerical precision—the higher the precision, the fewer operations the chip can execute. Think about how adding two 32-bit numbers generally requires twice the computation of adding two 16-bit numbers. The number of 32-bit operations a chip can perform in a given time is not exactly half that of 16-bit operations because of different chips’ optimization. For an overview of numerical precision, revisit “Numerical Representations”.",
      "raw_html": "<p>The number of operations a chip can perform in a second depends on the numerical precision—the higher the precision, the fewer operations the chip can execute. Think about how adding two 32-bit numbers generally requires twice the computation of adding two 16-bit numbers. The number of 32-bit operations a chip can perform in a given time is not exactly half that of 16-bit operations because of different chips’ optimization. For an overview of numerical precision, revisit <a data-type=\"xref\" href=\"ch07.html#ch07b_numerical_representations_1730159634259493\">“Numerical Representations”</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Table 9-2 shows the FLOP/s specs for different precision formats for NVIDIA H100 SXM chips.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch09_table_2_1730130962971057\">Table 9-2</a> shows the FLOP/s specs for different precision formats for <a href=\"https://oreil.ly/bNAOG\">NVIDIA H100 SXM chips</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch09_table_2_1730130962971057",
      "raw_html": "<table id=\"ch09_table_2_1730130962971057\">\n<caption><span class=\"label\">Table 9-2. </span>FLOP/s specs for NVIDIA H100 SXM chips.</caption>\n<thead>\n<tr>\n<th>Numerical precision</th>\n<th>teraFLOP/s (trillion FLOP/s) with sparsity</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>TF32 Tensor Core<sup><a data-type=\"noteref\" href=\"ch09.html#id1658\" id=\"id1658-marker\">a</a></sup></td>\n<td>989</td>\n</tr>\n<tr>\n<td>BFLOAT16 Tensor Core</td>\n<td>1,979</td>\n</tr>\n<tr>\n<td>FP16 Tensor Core</td>\n<td>1,979</td>\n</tr>\n<tr>\n<td>FP8 Tensor Core</td>\n<td>3,958</td>\n</tr>\n</tbody>\n<tbody><tr class=\"footnotes\"><td colspan=\"2\"><p data-type=\"footnote\" id=\"id1658\"><sup><a href=\"ch09.html#id1658-marker\">a</a></sup> Recall from <a data-type=\"xref\" href=\"ch07.html#ch07\">Chapter 7</a> that TF32 is a 19-bit, not 32-bit, format.</p></td></tr></tbody></table>",
      "caption": {
        "label": "Table 9-2.",
        "text": "FLOP/s specs for NVIDIA H100 SXM chips."
      },
      "headers": [
        "Numerical precision",
        "teraFLOP/s (trillion FLOP/s) with sparsity"
      ],
      "rows": [
        [
          "TF32 Tensor Corea",
          "989"
        ],
        [
          "BFLOAT16 Tensor Core",
          "1,979"
        ],
        [
          "FP16 Tensor Core",
          "1,979"
        ],
        [
          "FP8 Tensor Core",
          "3,958"
        ]
      ],
      "row_count": 4,
      "column_count": 2
    },
    {
      "type": "paragraph",
      "content": "a Recall from Chapter 7 that TF32 is a 19-bit, not 32-bit, format.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1658\"><sup><a href=\"ch09.html#id1658-marker\">a</a></sup> Recall from <a data-type=\"xref\" href=\"ch07.html#ch07\">Chapter 7</a> that TF32 is a 19-bit, not 32-bit, format.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Memory size and bandwidth",
      "id": "heading-7",
      "raw_html": "<h3>Memory size and bandwidth</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Because a GPU has many cores working in parallel, data often needs to be moved from the memory to these cores, and, therefore, data transfer speed is important. Data transfer is crucial when working with AI models that involve large weight matrices and training data. These large amounts of data need to be moved quickly to keep the cores efficiently occupied. Therefore, GPU memory needs to have higher bandwidth and lower latency than CPU memory, and thus, GPU memory requires more advanced memory technologies. This is one of the factors that makes GPU memory more expensive than CPU memory.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"accelerators\" data-secondary=\"memory size and bandwidth\" data-type=\"indexterm\" id=\"ch09.html33\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"AI accelerators\" data-tertiary=\"memory size and bandwidth\" data-type=\"indexterm\" id=\"ch09.html34\"></a><a contenteditable=\"false\" data-primary=\"memory bottlenecks\" data-secondary=\"size and bandwidth\" data-type=\"indexterm\" id=\"ch09.html35\"></a>Because a GPU has many cores working in parallel, data often needs to be moved from the memory to these cores, and, therefore, data transfer speed is important. Data transfer is crucial when working with AI models that involve large weight matrices and training data. These large amounts of data need to be moved quickly to keep the cores efficiently occupied. Therefore, GPU memory needs to have higher bandwidth and lower latency than CPU memory, and thus, GPU memory requires more advanced memory technologies. This is one of the factors that makes GPU memory more expensive than CPU memory.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "To be more specific, CPUs typically use DDR SDRAM (Double Data Rate Synchronous Dynamic Random-Access Memory), which has a 2D structure. GPUs, particularly high-end ones, often use HBM (high-bandwidth memory), which has a 3D stacked structure.17",
      "raw_html": "<p class=\"pagebreak-before\"><a contenteditable=\"false\" data-primary=\"DDR SDRAM (doubled data rate synchronous dynamic random-access memory)\" data-type=\"indexterm\" id=\"id1659\"></a><a contenteditable=\"false\" data-primary=\"doubled data rate synchronous dynamic random-access memory (DDR SDRAM)\" data-type=\"indexterm\" id=\"id1660\"></a>To be more specific, CPUs typically use <a href=\"https://en.wikipedia.org/wiki/DDR_SDRAM\">DDR SDRAM</a> (Double Data Rate Synchronous Dynamic Random-Access Memory), which has a 2D structure. GPUs, particularly high-end ones, often use <a href=\"https://en.wikipedia.org/wiki/High_Bandwidth_Memory\">HBM</a> (high-bandwidth memory), which has a 3D stacked structure.<sup><a data-type=\"noteref\" href=\"ch09.html#id1661\" id=\"id1661-marker\">17</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "An accelerator’s memory is measured by its size and bandwidth. These numbers need to be evaluated within the system an accelerator is part of. An accelerator, such as a GPU, typically interacts with three levels of memory, as visualized in Figure 9-7:",
      "raw_html": "<p>An accelerator’s memory is measured by its <em>size and bandwidth</em>. These numbers need to be evaluated within the system an accelerator is part of. An accelerator, such as a GPU, typically interacts with three levels of memory, as visualized in <a data-type=\"xref\" href=\"#ch09_figure_7_1730130962952731\">Figure 9-7</a>:</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "CPU memory (DRAM)",
          "definition": "Accelerators are usually deployed alongside CPUs, giving them access to the CPU memory (also known as system memory, host memory, or just CPU DRAM)."
        },
        {
          "term": "GPU high-bandwidth memory (HBM)",
          "definition": "This is the memory dedicated to the GPU, located close to the GPU for faster access than CPU memory."
        },
        {
          "term": "GPU on-chip SRAM",
          "definition": "Integrated directly into the chip, this memory is used to store frequently accessed data and instructions for nearly instant access. It includes L1 and L2 caches made of SRAM, and, in some architectures, L3 caches as well. These caches are part of the broader on-chip memory, which also includes other components like register files and shared memory."
        }
      ],
      "raw_html": "<dl>\n<dt>CPU memory (DRAM)</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"CPU memory (DRAM)\" data-type=\"indexterm\" id=\"id1662\"></a><a contenteditable=\"false\" data-primary=\"DRAM (CPU memory)\" data-type=\"indexterm\" id=\"id1663\"></a>Accelerators are usually deployed alongside CPUs, giving them access to the CPU memory (also known as system memory, host memory, or just CPU DRAM).</p></dd>\n<dd><p>CPU memory usually has the lowest bandwidth among these memory types, with data transfer speeds ranging from 25 GB/s to 50 GB/s. CPU memory size varies. Average laptops might have around 16–64 GB, whereas high-end workstations can have one TB or more.</p></dd>\n<dt>GPU high-bandwidth memory (HBM)</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"high-bandwidth memory (HBM)\" data-type=\"indexterm\" id=\"id1664\"></a>This is the memory dedicated to the GPU, located close to the GPU for faster access than CPU memory.</p></dd>\n<dd><p>HBM provides significantly higher bandwidth, with data transfer speeds typically ranging from 256 GB/s to over 1.5 TB/s. This speed is essential for efficiently handling large data transfers and high-throughput tasks. A consumer GPU has around 24–80 GB of HBM.</p></dd>\n<dt>GPU on-chip SRAM</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"GPU on-chip SRAM\" data-type=\"indexterm\" id=\"id1665\"></a>Integrated directly into the chip, this memory is used to store frequently accessed data and instructions for nearly instant access. It includes L1 and L2 caches made of SRAM, and, in some architectures, L3 caches as well. These caches are part of the broader on-chip memory, which also includes other components like register files and shared memory.</p></dd>\n<dd><p>RAM has extremely high data transfer speeds, often exceeding 10 TB/s. The size of GPU SRAM is small, typically 40 MB or under.</p></dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "Accelerators are usually deployed alongside CPUs, giving them access to the CPU memory (also known as system memory, host memory, or just CPU DRAM).",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"CPU memory (DRAM)\" data-type=\"indexterm\" id=\"id1662\"></a><a contenteditable=\"false\" data-primary=\"DRAM (CPU memory)\" data-type=\"indexterm\" id=\"id1663\"></a>Accelerators are usually deployed alongside CPUs, giving them access to the CPU memory (also known as system memory, host memory, or just CPU DRAM).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "CPU memory usually has the lowest bandwidth among these memory types, with data transfer speeds ranging from 25 GB/s to 50 GB/s. CPU memory size varies. Average laptops might have around 16–64 GB, whereas high-end workstations can have one TB or more.",
      "raw_html": "<p>CPU memory usually has the lowest bandwidth among these memory types, with data transfer speeds ranging from 25 GB/s to 50 GB/s. CPU memory size varies. Average laptops might have around 16–64 GB, whereas high-end workstations can have one TB or more.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This is the memory dedicated to the GPU, located close to the GPU for faster access than CPU memory.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"high-bandwidth memory (HBM)\" data-type=\"indexterm\" id=\"id1664\"></a>This is the memory dedicated to the GPU, located close to the GPU for faster access than CPU memory.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "HBM provides significantly higher bandwidth, with data transfer speeds typically ranging from 256 GB/s to over 1.5 TB/s. This speed is essential for efficiently handling large data transfers and high-throughput tasks. A consumer GPU has around 24–80 GB of HBM.",
      "raw_html": "<p>HBM provides significantly higher bandwidth, with data transfer speeds typically ranging from 256 GB/s to over 1.5 TB/s. This speed is essential for efficiently handling large data transfers and high-throughput tasks. A consumer GPU has around 24–80 GB of HBM.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Integrated directly into the chip, this memory is used to store frequently accessed data and instructions for nearly instant access. It includes L1 and L2 caches made of SRAM, and, in some architectures, L3 caches as well. These caches are part of the broader on-chip memory, which also includes other components like register files and shared memory.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"GPU on-chip SRAM\" data-type=\"indexterm\" id=\"id1665\"></a>Integrated directly into the chip, this memory is used to store frequently accessed data and instructions for nearly instant access. It includes L1 and L2 caches made of SRAM, and, in some architectures, L3 caches as well. These caches are part of the broader on-chip memory, which also includes other components like register files and shared memory.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "RAM has extremely high data transfer speeds, often exceeding 10 TB/s. The size of GPU SRAM is small, typically 40 MB or under.",
      "raw_html": "<p>RAM has extremely high data transfer speeds, often exceeding 10 TB/s. The size of GPU SRAM is small, typically 40 MB or under.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_7_1730130962952731",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_7_1730130962952731\">\n<img alt=\"A colorful pyramid with multiple layers\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0907.png\"/>\n<h6><span class=\"label\">Figure 9-7. </span>The memory hierarchy of an AI accelerator. The numbers are for reference only. The actual numbers vary for each chip.</h6>\n</div></figure>",
      "image": "aien_0907.png",
      "alt": "A colorful pyramid with multiple layers\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0907.png",
      "caption": {
        "label": "Figure 9-7.",
        "text": "The memory hierarchy of an AI accelerator. The numbers are for reference only. The actual numbers vary for each chip."
      }
    },
    {
      "type": "paragraph",
      "content": "A lot of GPU optimization is about how to make the most out of this memory hierarchy. However, as of this writing, popular frameworks such as PyTorch and TensorFlow don’t yet allow fine-grained control of memory access. This has led many AI researchers and engineers to become interested in GPU programming languages such as CUDA (originally Compute Unified Device Architecture), OpenAI’s Triton, and ROCm (Radeon Open Compute). The latter is AMD’s open source alternative to NVIDIA’s proprietary CUDA.",
      "raw_html": "<p>A lot of GPU optimization is about how to make the most out of this memory hierarchy. However, as of this writing, popular frameworks such as PyTorch and TensorFlow don’t yet allow fine-grained control of memory access. This has led many AI researchers and engineers to become interested in GPU programming languages such as <a href=\"https://en.wikipedia.org/wiki/CUDA\">CUDA</a> (originally Compute Unified Device Architecture), <a href=\"https://github.com/triton-lang/triton\">OpenAI’s Triton</a>, and <a href=\"https://github.com/ROCm/ROCm\">ROCm</a> (Radeon Open Compute). The latter is AMD’s open source alternative to NVIDIA’s proprietary CUDA.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html35\" data-type=\"indexterm\" id=\"id1666\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html34\" data-type=\"indexterm\" id=\"id1667\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html33\" data-type=\"indexterm\" id=\"id1668\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Power consumption",
      "id": "heading-7",
      "raw_html": "<h3>Power consumption</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Chips rely on transistors to perform computation. Each computation is done by transistors switching on and off, which requires energy. A GPU can have billions of transistors—an NVIDIA A100 has 54 billion transistors, while an NVIDIA H100 has 80 billion. When an accelerator is used efficiently, billions of transistors rapidly switch states, consuming a substantial amount of energy and generating a nontrivial amount of heat. This heat requires cooling systems, which also consume electricity, adding to data centers’ overall energy consumption.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"accelerators\" data-secondary=\"power consumption\" data-type=\"indexterm\" id=\"ch09.html36\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"AI accelerators\" data-tertiary=\"power consumption\" data-type=\"indexterm\" id=\"ch09.html37\"></a><a contenteditable=\"false\" data-primary=\"power consumption\" data-type=\"indexterm\" id=\"ch09.html38\"></a>Chips rely on transistors to perform computation. Each computation is done by transistors switching on and off, which requires energy. A GPU can have billions of transistors—an NVIDIA A100 has <a href=\"https://oreil.ly/5vRsP\">54 billion</a> transistors, while an NVIDIA H100 has <a href=\"https://en.wikipedia.org/wiki/Hopper_(microarchitecture)\">80 billion</a>. When an accelerator is used efficiently, billions of transistors rapidly switch states, consuming a substantial amount of energy and generating a nontrivial amount of heat. This heat requires cooling systems, which also consume electricity, adding to data centers’ overall energy consumption.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Chip energy consumption threatens to have a staggering impact on the environment, increasing the pressure on companies to invest in technologies for green data centers. An NVIDIA H100 running at its peak for a year consumes approximately 7,000 kWh. For comparison, the average US household’s annual electricity consumption is 10,000 kWh. That’s why electricity is a bottleneck to scaling up compute.18",
      "raw_html": "<p>Chip energy consumption threatens to have a staggering impact on the <a href=\"https://oreil.ly/RqY-3\">environment</a>, increasing the pressure on companies to invest in technologies for <a href=\"https://en.wikipedia.org/wiki/Green_data_center\">green data centers</a>. An NVIDIA H100 running at its peak for a year consumes approximately 7,000 kWh. For comparison, the average US household’s annual electricity consumption is 10,000 kWh. That’s why electricity is a bottleneck to scaling up compute.<sup><a data-type=\"noteref\" href=\"ch09.html#id1669\" id=\"id1669-marker\">18</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Accelerators typically specify their power consumption under maximum power draw or a proxy metric TDP (thermal design power):",
      "raw_html": "<p>Accelerators typically specify their power consumption under <em>maximum power draw</em> or a proxy metric <em>TDP (thermal design power):</em></p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Maximum power draw indicates the peak power that the chip could draw under full load.",
        "TDP represents the maximum heat a cooling system needs to dissipate when the chip operates under typical workloads. While it’s not an exact measure of power consumption, it’s an indication of the expected power draw. For CPUs and GPUs, the maximum power draw can be roughly 1.1 to 1.5 times the TDP, though the exact relationship varies depending on the specific architecture and workload."
      ],
      "raw_html": "<ul>\n<li><p>Maximum power draw indicates the peak power that the chip could draw under full load. </p></li>\n<li><p><em>TDP</em> represents the maximum heat a cooling system needs to dissipate when the chip operates under typical workloads. While it’s not an exact measure of power consumption, it’s an indication of the expected power draw. For CPUs and GPUs, the maximum power draw can be roughly 1.1 to 1.5 times the TDP, though the exact relationship varies depending on the specific architecture and workload.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Maximum power draw indicates the peak power that the chip could draw under full load.",
      "raw_html": "<p>Maximum power draw indicates the peak power that the chip could draw under full load. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "TDP represents the maximum heat a cooling system needs to dissipate when the chip operates under typical workloads. While it’s not an exact measure of power consumption, it’s an indication of the expected power draw. For CPUs and GPUs, the maximum power draw can be roughly 1.1 to 1.5 times the TDP, though the exact relationship varies depending on the specific architecture and workload.",
      "raw_html": "<p><em>TDP</em> represents the maximum heat a cooling system needs to dissipate when the chip operates under typical workloads. While it’s not an exact measure of power consumption, it’s an indication of the expected power draw. For CPUs and GPUs, the maximum power draw can be roughly 1.1 to 1.5 times the TDP, though the exact relationship varies depending on the specific architecture and workload.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "If you opt for cloud providers, you won’t need to worry about cooling or electricity. However, these numbers can still be of interest to understand the impact of accelerators on the environment and the overall electricity demand.",
      "raw_html": "<p>If you opt for cloud providers, you won’t need to worry about cooling or electricity. However, these numbers can still be of interest to understand the impact of accelerators on the environment and the overall electricity demand. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "aside",
      "content": "Selecting Accelerators\nWhat accelerators to use depends on your workload. If your workloads are compute-bound, you might want to look for chips with more FLOP/s. If your workloads are memory-bound, shelling out money for chips with higher bandwidth and more memory will make your life easier.\nWhen evaluating which chips to buy, there are three main questions:\n\nCan the hardware run your workloads?\nHow long does it take to do so?\nHow much does it cost?\n\nFLOP/s, memory size, and memory bandwidth are the three big numbers that help you answer the first two questions. The last question is straightforward. Cloud providers’ pricing is typically usage-based and fairly similar across providers. If you buy your hardware, the cost can be calculated based on the initial price and ongoing power consumption.",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"id1670\">\n<h1>Selecting Accelerators</h1>\n<p>What accelerators to use depends on your workload. If your workloads are compute-bound, you might want to look for chips with more FLOP/s. If your workloads are memory-bound, shelling out money for chips with higher bandwidth and more memory will make your life easier.</p>\n<p>When evaluating which chips to buy, there are three main questions:</p>\n<ul>\n<li><p>Can the hardware run your workloads?</p></li>\n<li><p>How long does it take to do so?</p></li>\n<li><p>How much does it cost?</p></li>\n</ul>\n<p>FLOP/s, memory size, and memory bandwidth are the three big numbers that help you answer the first two questions. The last question is straightforward. Cloud providers’ pricing is typically usage-based and fairly similar across providers. If you buy your hardware, the cost can be calculated based on the initial price and ongoing power <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html38\" data-type=\"indexterm\" id=\"id1671\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html37\" data-type=\"indexterm\" id=\"id1672\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html36\" data-type=\"indexterm\" id=\"id1673\"></a>consumption<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html30\" data-type=\"indexterm\" id=\"id1674\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html29\" data-type=\"indexterm\" id=\"id1675\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html1\" data-type=\"indexterm\" id=\"id1676\"></a></p>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Selecting Accelerators",
      "id": "heading-7",
      "raw_html": "<h1>Selecting Accelerators</h1>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "What accelerators to use depends on your workload. If your workloads are compute-bound, you might want to look for chips with more FLOP/s. If your workloads are memory-bound, shelling out money for chips with higher bandwidth and more memory will make your life easier.",
      "raw_html": "<p>What accelerators to use depends on your workload. If your workloads are compute-bound, you might want to look for chips with more FLOP/s. If your workloads are memory-bound, shelling out money for chips with higher bandwidth and more memory will make your life easier.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "When evaluating which chips to buy, there are three main questions:",
      "raw_html": "<p>When evaluating which chips to buy, there are three main questions:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Can the hardware run your workloads?",
        "How long does it take to do so?",
        "How much does it cost?"
      ],
      "raw_html": "<ul>\n<li><p>Can the hardware run your workloads?</p></li>\n<li><p>How long does it take to do so?</p></li>\n<li><p>How much does it cost?</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Can the hardware run your workloads?",
      "raw_html": "<p>Can the hardware run your workloads?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "How long does it take to do so?",
      "raw_html": "<p>How long does it take to do so?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "How much does it cost?",
      "raw_html": "<p>How much does it cost?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "FLOP/s, memory size, and memory bandwidth are the three big numbers that help you answer the first two questions. The last question is straightforward. Cloud providers’ pricing is typically usage-based and fairly similar across providers. If you buy your hardware, the cost can be calculated based on the initial price and ongoing power consumption.",
      "raw_html": "<p>FLOP/s, memory size, and memory bandwidth are the three big numbers that help you answer the first two questions. The last question is straightforward. Cloud providers’ pricing is typically usage-based and fairly similar across providers. If you buy your hardware, the cost can be calculated based on the initial price and ongoing power <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html38\" data-type=\"indexterm\" id=\"id1671\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html37\" data-type=\"indexterm\" id=\"id1672\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html36\" data-type=\"indexterm\" id=\"id1673\"></a>consumption<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html30\" data-type=\"indexterm\" id=\"id1674\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html29\" data-type=\"indexterm\" id=\"id1675\"></a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html1\" data-type=\"indexterm\" id=\"id1676\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Inference Optimization",
      "id": "heading-7",
      "raw_html": "<h1 class=\"less_space\">Inference Optimization </h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "Inference optimization can be done at the model, hardware, or service level. To illustrate their differences, consider archery. Model-level optimization is like crafting better arrows. Hardware-level optimization is like training a stronger and better archer. Service-level optimization is like refining the entire shooting process, including the bow and aiming conditions.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"at model/hardware/service levels\" data-secondary-sortas=\"model\" data-type=\"indexterm\" id=\"id1677\"></a>Inference optimization can be done at the model, hardware, or service level. To illustrate their differences, consider archery. Model-level optimization is like crafting better arrows. Hardware-level optimization is like training a stronger and better archer. Service-level optimization is like refining the entire shooting process, including the bow and aiming conditions.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Ideally, optimizing a model for speed and cost shouldn’t change the model’s quality. However, many techniques might cause model degradation. Figure 9-8 shows the same Llama models’ performance on different benchmarks, served by different inference service providers.",
      "raw_html": "<p>Ideally, optimizing a model for speed and cost shouldn’t change the model’s quality. However, many techniques might cause model degradation. <a data-type=\"xref\" href=\"#ch09_figure_8_1730130962952759\">Figure 9-8</a> shows the same Llama models’ performance on different benchmarks, served by different inference service providers.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_8_1730130962952759",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_8_1730130962952759\">\n<img alt=\"A graph of different types of numbers\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0908.png\"/>\n<h6><span class=\"label\">Figure 9-8. </span>An inference service provider might use optimization techniques that can alter a model’s behavior, causing different providers to have slight model quality variations. The experiment was conducted by <a href=\"https://oreil.ly/5hFSF\">Cerebras (2024)</a>.</h6>\n</div></figure>",
      "image": "aien_0908.png",
      "alt": "A graph of different types of numbers\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0908.png",
      "caption": {
        "label": "Figure 9-8.",
        "text": "An inference service provider might use optimization techniques that can alter a model’s behavior, causing different providers to have slight model quality variations. The experiment was conducted by Cerebras (2024)."
      }
    },
    {
      "type": "paragraph",
      "content": "Since hardware design is outside the scope of this book, I’ll discuss techniques at the model and service levels. While the techniques are discussed separately, keep in mind that, in production, optimization typically involves techniques at more than one level.",
      "raw_html": "<p>Since hardware design is outside the scope of this book, I’ll discuss techniques at the model and service levels. While the techniques are discussed separately, keep in mind that, in production, optimization typically involves techniques at more than one level.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Model Optimization",
      "id": "heading-7",
      "raw_html": "<h2>Model Optimization</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "Model-level optimization aims to make the model more efficient, often by modifying the model itself, which can alter its behavior. As of this writing, many foundation models follow the transformer architecture and include an autoregressive language model component. These models have three characteristics that make inference resource-intensive: model size, autoregressive decoding, and the attention mechanism. Let’s discuss approaches to address these challenges.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"model optimization\" data-type=\"indexterm\" id=\"ch09.html40\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-type=\"indexterm\" id=\"ch09.html41\"></a>Model-level optimization aims to make the model more efficient, often by modifying the model itself, which can alter its behavior. As of this writing, many foundation models follow the transformer architecture and include an autoregressive language model component. These models have three characteristics that make inference resource-intensive: model size, autoregressive decoding, and the attention mechanism. Let’s discuss approaches to address these challenges.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Model compression",
      "id": "heading-7",
      "raw_html": "<h3>Model compression</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Model compression involves techniques that reduce a model’s size. Making a model smaller can also make it faster. This book has already discussed two model compression techniques: quantization and distillation. Quantization, reducing the precision of a model to reduce its memory footprint and increase its throughput, is discussed in Chapter 7. Model distillation, training a small model to mimic the behavior of the large model, is discussed in Chapter 8.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"model optimization\" data-tertiary=\"model compression\" data-type=\"indexterm\" id=\"id1678\"></a><a contenteditable=\"false\" data-primary=\"model compression\" data-type=\"indexterm\" id=\"id1679\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"model compression\" data-type=\"indexterm\" id=\"id1680\"></a>Model compression involves techniques that reduce a model’s size. Making a model smaller can also make it faster. This book has already discussed two model compression techniques: quantization and distillation. Quantization, reducing the precision of a model to reduce its memory footprint and increase its throughput, is discussed in <a data-type=\"xref\" href=\"ch07.html#ch07\">Chapter 7</a>. Model distillation, training a small model to mimic the behavior of the large model, is discussed in <a data-type=\"xref\" href=\"ch08.html#ch08_dataset_engineering_1730130932019888\">Chapter 8</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model distillation suggests that it’s possible to capture a large model’s behaviors using fewer parameters. Could it be that within the large model, there exists a subset of parameters capable of capturing the entire model’s behavior? This is the core concept behind pruning.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"distillation\" data-secondary=\"model distillation\" data-type=\"indexterm\" id=\"id1681\"></a>Model distillation suggests that it’s possible to capture a large model’s behaviors using fewer parameters. Could it be that within the large model, there exists a subset of parameters capable of capturing the entire model’s behavior? This is the core concept behind pruning.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Pruning, in the context of neural networks, has two meanings. One is to remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters. Another is to find parameters least useful to predictions and set them to zero. In this case, pruning doesn’t reduce the total number of parameters, only the number of non-zero parameters. This makes the model more sparse, which both reduces the model’s storage space and speeds up computation.",
      "raw_html": "<p>Pruning, in the context of neural networks, has two meanings. One is to remove entire nodes of a neural network, which means changing its architecture and reducing its number of parameters. Another is to find parameters least useful to predictions and set them to zero. <a contenteditable=\"false\" data-primary=\"sparse models\" data-type=\"indexterm\" id=\"id1682\"></a>In this case, pruning doesn’t reduce the total number of parameters, only the number of non-zero parameters. This makes the model more sparse, which both reduces the model’s storage space and speeds up computation.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Pruned models can be used as-is or be further finetuned to adjust the remaining parameters and restore any performance degradation caused by the pruning process. Pruning can help discover promising model architectures (Liu et al., 2018). These pruned architectures, smaller than the pre-pruned architectures, can also be trained from scratch (Zhu et al., 2017).",
      "raw_html": "<p>Pruned models can be used as-is or be further finetuned to adjust the remaining parameters and restore any performance degradation caused by the pruning process. Pruning can help discover promising model architectures (<a href=\"https://arxiv.org/abs/1810.05270\">Liu et al., 2018</a>). These pruned architectures, smaller than the pre-pruned architectures, can also be trained from scratch (<a href=\"https://arxiv.org/abs/1710.01878\">Zhu et al., 2017</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In the literature, there have been many encouraging pruning results. For example, Frankle and Carbin (2019) showed that pruning techniques can reduce the non-zero parameter counts of certain trained networks by over 90%, decreasing memory footprints and improving speed without compromising accuracy. However, in practice, as of this writing, pruning is less common. It’s harder to do, as it requires an understanding of the original model’s architecture, and the performance boost it can bring is often much less than that of other approaches. Pruning also results in sparse models, and not all hardware architectures are designed to take advantage of the resulting sparsity.",
      "raw_html": "<p>In the literature, there have been many encouraging pruning results. For example, <a href=\"https://oreil.ly/qwlHE\">Frankle and Carbin (2019)</a> showed that pruning techniques can reduce the non-zero parameter counts of certain trained networks by over 90%, decreasing memory footprints and improving speed without compromising accuracy. However, in practice, as of this writing, pruning is less common. It’s harder to do, as it requires an understanding of the original model’s architecture, and the performance boost it can bring is often much less than that of other approaches. Pruning also results in sparse models, and not all hardware architectures are designed to take advantage of the resulting sparsity.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Weight-only quantization is by far the most popular approach since it’s easy to use, works out of the box for many models, and is extremely effective. Reducing a model’s precision from 32 bits to 16 bits reduces its memory footprint by half. However, we’re close to the limit of quantization—we can’t go lower than 1 bit per value. Distillation is also common because it can result in a smaller model whose behavior is comparative to that of a much larger one for your needs.",
      "raw_html": "<p><em>Weight-only quantization is by far the most popular approach since it’s easy to use, works out of the box for many models, and is extremely effective.</em> Reducing a model’s precision from 32 bits to 16 bits reduces its memory footprint by half. However, we’re close to the limit of quantization—we can’t go lower than 1 bit per value. Distillation is also common because it can result in a smaller model whose behavior is comparative to that of a much larger one for your needs.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Overcoming the autoregressive decoding bottleneck",
      "id": "heading-7",
      "raw_html": "<h3>Overcoming the autoregressive decoding bottleneck</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "As discussed in Chapter 2, autoregressive language models generate one token after another. If it takes 100 ms to generate one token, a response of 100 tokens will take 10 s.19 This process is not just slow, it’s also expensive. Across model API providers, an output token costs approximately two to four times an input token. In an experiment, Anyscale found that a single output token can have the same impact on latency as 100 input tokens (Kadous et al., 2023). Improving the autoregressive generation process by a small percentage can significantly improve user experience.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"decoding\" data-secondary=\"autoregressive decoding bottleneck\" data-type=\"indexterm\" id=\"ch09.html42a\"></a><a contenteditable=\"false\" data-primary=\"autoregressive decoding bottleneck\" data-type=\"indexterm\" id=\"ch09.html42\"></a><a contenteditable=\"false\" data-primary=\"bottlenecks\" data-secondary=\"autoregressive decoding\" data-type=\"indexterm\" id=\"ch09.html43\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"model optimization\" data-tertiary=\"autoregressive decoding bottleneck\" data-type=\"indexterm\" id=\"ch09.html44\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"autoregressive decoding bottleneck\" data-type=\"indexterm\" id=\"ch09.html45\"></a>As discussed in <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a>, autoregressive language models generate one token after another. If it takes 100 ms to generate one token, a response of 100 tokens will take <span class=\"keep-together\">10 s</span>.<sup><a data-type=\"noteref\" href=\"ch09.html#id1683\" id=\"id1683-marker\">19</a></sup> This process is not just slow, it’s also expensive. Across model API providers, an output token costs approximately two to four times an input token. In an experiment, Anyscale found that a single output token can have the same impact on latency as 100 input tokens (<a href=\"https://oreil.ly/QYdG8\">Kadous et al., 2023</a>). Improving the autoregressive generation process by a small percentage can significantly improve user experience.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "As the space is rapidly evolving, new techniques are being developed to overcome this seemingly impossible bottleneck. Perhaps one day, there will be architectures that don’t have this bottleneck. The techniques covered here are to illustrate what the solution might look like, but the techniques are still evolving.",
      "raw_html": "<p>As the space is rapidly evolving, new techniques are being developed to overcome this seemingly impossible bottleneck. Perhaps one day, there will be architectures that don’t have this bottleneck. The techniques covered here are to illustrate what the solution might look like, but the techniques are still evolving.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Speculative decoding",
      "id": "heading-7",
      "raw_html": "<h4>Speculative decoding</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Speculative decoding (also called speculative sampling) uses a faster but less powerful model to generate a sequence of tokens, which are then verified by the target model. The target model is the model you want to use. The faster model is called the draft or proposal model because it proposes the draft output.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"autoregressive decoding bottleneck\" data-secondary=\"speculative decoding\" data-type=\"indexterm\" id=\"ch09.html46\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"autoregressive decoding bottleneck\" data-tertiary=\"speculative decoding\" data-type=\"indexterm\" id=\"ch09.html47\"></a><a contenteditable=\"false\" data-primary=\"speculative decoding\" data-type=\"indexterm\" id=\"ch09.html48\"></a>Speculative decoding (also called speculative sampling) uses a faster but less powerful model to generate a sequence of tokens, which are then verified by the target model. The target model is the model you want to use. The faster model is called the draft or proposal model because it proposes the draft output.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Imagine the input tokens are x1, x2, …, xt:",
      "raw_html": "<p>Imagine the input tokens are <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em></sub>:</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "The draft model generates a sequence of K tokens: xt + 1, xt + 2, …, xt + K.",
        "The target model verifies these K generated tokens in parallel.",
        "The target model accepts the longest subsequence of draft tokens, from left to right, which the target model agrees to use.",
        "Let’s say the target model accepts j draft tokens, xt + 1, xt + 2, …, xt + j. The target model then generates one extra token, xt + j + 1."
      ],
      "raw_html": "<ol>\n<li>\n<p>The draft model generates a sequence of <em>K</em> tokens: <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>K</em></sub>.</p>\n</li>\n<li>\n<p>The target model verifies these <em>K</em> generated tokens in parallel.</p>\n</li>\n<li>\n<p>The target model <em>accepts</em> the longest subsequence of draft tokens, from left to right, which the target model agrees to use.</p>\n</li>\n<li>\n<p>Let’s say the target model accepts <em>j</em> draft tokens, <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>j</em></sub>. The target model then generates one extra token, <em>x</em><sub><em>t</em> + <em>j</em> + 1</sub>.</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "The draft model generates a sequence of K tokens: xt + 1, xt + 2, …, xt + K.",
      "raw_html": "<p>The draft model generates a sequence of <em>K</em> tokens: <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>K</em></sub>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The target model verifies these K generated tokens in parallel.",
      "raw_html": "<p>The target model verifies these <em>K</em> generated tokens in parallel.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The target model accepts the longest subsequence of draft tokens, from left to right, which the target model agrees to use.",
      "raw_html": "<p>The target model <em>accepts</em> the longest subsequence of draft tokens, from left to right, which the target model agrees to use.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Let’s say the target model accepts j draft tokens, xt + 1, xt + 2, …, xt + j. The target model then generates one extra token, xt + j + 1.",
      "raw_html": "<p>Let’s say the target model accepts <em>j</em> draft tokens, <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>j</em></sub>. The target model then generates one extra token, <em>x</em><sub><em>t</em> + <em>j</em> + 1</sub>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The process returns to step 1, with the draft model generating K tokens conditioned on x1, x2, …, xt, xt + 1, xt + 2, …, xt + j. The process is visualized in Figure 9-9.",
      "raw_html": "<p> The process returns to step 1, with the draft model generating <em>K</em> tokens conditioned on <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em></sub>, <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>, …, <em>x</em><sub><em>t</em> + <em>j</em></sub>. The process is visualized in <a data-type=\"xref\" href=\"#ch09_figure_9_1730130962952786\">Figure 9-9</a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "If no draft token is accepted, this loop produces only one token generated by the target model. If all draft tokens are accepted, this loop produces K + 1 tokens, with K generated by the draft model and one by the target model.",
      "raw_html": "<p>If no draft token is accepted, this loop produces only one token generated by the target model. If all draft tokens are accepted, this loop produces <em>K</em> + 1 tokens, with <em>K</em> generated by the draft model and one by the target model. </p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch09_figure_9_1730130962952786",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_9_1730130962952786\">\n<img alt=\"A diagram of words\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0909.png\"/>\n<h6><span class=\"label\">Figure 9-9. </span>A draft model generates a sequence of K tokens, and the main model accepts the longest subsequence that it agrees with. The image is from “Blockwise Parallel Decoding for Deep Autoregressive Models” (<a href=\"https://arxiv.org/abs/1811.03115\">Stern et al., 2018</a>).</h6>\n</div></figure>",
      "image": "aien_0909.png",
      "alt": "A diagram of words\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0909.png",
      "caption": {
        "label": "Figure 9-9.",
        "text": "A draft model generates a sequence of K tokens, and the main model accepts the longest subsequence that it agrees with. The image is from “Blockwise Parallel Decoding for Deep Autoregressive Models” (Stern et al., 2018)."
      }
    },
    {
      "type": "paragraph",
      "content": "If all draft sequences are rejected, the target model must generate the entire response in addition to verifying it, potentially leading to increased latency. However, this can be avoided because of these three insights:",
      "raw_html": "<p>If all draft sequences are rejected, the target model must generate the entire response in addition to verifying it, potentially leading to increased latency. However, this can be avoided because of these three insights:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "The time it takes for the target model to verify a sequence of tokens is less than the time it takes to generate it, because verification is parallelizable, while generation is sequential. Speculative decoding effectively turns the computation profile of decoding into that of prefilling.",
        "In an output token sequence, some tokens are easier to predict than others. It’s possible to find a weaker draft model capable of getting these easier-to-predict tokens right, leading to a high acceptance rate of the draft tokens.",
        "Decoding is memory bandwidth-bound, which means that during the coding process, there are typically idle FLOPs that can be used for free verification.20"
      ],
      "raw_html": "<ol>\n<li>\n<p>The time it takes for the target model to verify a sequence of tokens is less than the time it takes to generate it, because verification is parallelizable, while generation is sequential. Speculative decoding effectively turns the computation profile of decoding into that of prefilling.</p>\n</li>\n<li>\n<p>In an output token sequence, some tokens are easier to predict than others. It’s possible to find a weaker draft model capable of getting these easier-to-predict tokens right, leading to a high acceptance rate of the draft tokens.</p>\n</li>\n<li>\n<p>Decoding is memory bandwidth-bound, which means that during the coding process, there are typically idle FLOPs that can be used for free verification.<sup><a data-type=\"noteref\" href=\"ch09.html#id1684\" id=\"id1684-marker\">20</a></sup></p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "The time it takes for the target model to verify a sequence of tokens is less than the time it takes to generate it, because verification is parallelizable, while generation is sequential. Speculative decoding effectively turns the computation profile of decoding into that of prefilling.",
      "raw_html": "<p>The time it takes for the target model to verify a sequence of tokens is less than the time it takes to generate it, because verification is parallelizable, while generation is sequential. Speculative decoding effectively turns the computation profile of decoding into that of prefilling.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "In an output token sequence, some tokens are easier to predict than others. It’s possible to find a weaker draft model capable of getting these easier-to-predict tokens right, leading to a high acceptance rate of the draft tokens.",
      "raw_html": "<p>In an output token sequence, some tokens are easier to predict than others. It’s possible to find a weaker draft model capable of getting these easier-to-predict tokens right, leading to a high acceptance rate of the draft tokens.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Decoding is memory bandwidth-bound, which means that during the coding process, there are typically idle FLOPs that can be used for free verification.20",
      "raw_html": "<p>Decoding is memory bandwidth-bound, which means that during the coding process, there are typically idle FLOPs that can be used for free verification.<sup><a data-type=\"noteref\" href=\"ch09.html#id1684\" id=\"id1684-marker\">20</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Acceptance rates are domain-dependent. For texts that follow specific structures like code, the acceptance rate is typically higher. Larger values of K mean fewer verifying calls for the target model but a low acceptance rate of the draft tokens. The draft model can be of any architecture, though ideally it should share the same vocabulary and tokenizer as the target model. You can train a custom draft model or use an existing weaker model.",
      "raw_html": "<p>Acceptance rates are domain-dependent. For texts that follow specific structures like code, the acceptance rate is typically higher. Larger values of <em>K</em> mean fewer verifying calls for the target model but a low acceptance rate of the draft tokens. The draft model can be of any architecture, though ideally it should share the same vocabulary and tokenizer as the target model. You can train a custom draft model or use an existing weaker model.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "For example, to speed up the decoding process of Chinchilla-70B, DeepMind trained a 4B-parameter draft model of the same architecture (Chen et al., 2023). The draft model can generate a token eight times faster than the target model (1.8 ms/token compared to 14.1 ms/token). This reduces the overall response latency by more than half without compromising response quality. A similar speed-up was achieved for T5-XXL (Laviathan et al., 2022).",
      "raw_html": "<p>For example, to speed up the decoding process of Chinchilla-70B, DeepMind trained a 4B-parameter draft model of the same architecture (<a href=\"https://arxiv.org/abs/2302.01318\">Chen et al., 2023</a>). The draft model can generate a token eight times faster than the target model (1.8 ms/token compared to 14.1 ms/token). This reduces the overall response latency by more than half without compromising response quality. A similar speed-up was achieved for T5-XXL (<a href=\"https://arxiv.org/abs/2211.17192\">Laviathan et al., 2022</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This approach has gained traction because it’s relatively easy to implement and doesn’t change a model’s quality. For example, it’s possible to do so in 50 lines of code in PyTorch. It’s been incorporated into popular inference frameworks such as vLLM, TensorRT-LLM, and llama.cpp.",
      "raw_html": "<p>This approach has gained traction because it’s relatively easy to implement and doesn’t change a model’s quality. For example, it’s possible to do so in <a href=\"https://oreil.ly/IaPOB\">50 lines of code in PyTorch</a>. It’s been incorporated into popular inference frameworks such as <a href=\"https://oreil.ly/uzg1s\">vLLM</a>, <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">TensorRT-LLM</a>, and <a href=\"https://github.com/ggerganov/llama.cpp/pull/2926\">llama.cpp</a>.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html48\" data-type=\"indexterm\" id=\"id1685\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html47\" data-type=\"indexterm\" id=\"id1686\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html46\" data-type=\"indexterm\" id=\"id1687\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Inference with reference",
      "id": "heading-7",
      "raw_html": "<h4>Inference with reference</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Often, a response needs to reference tokens from the input. For example, if you ask your model a question about an attached document, the model might repeat a chunk of text verbatim from the document. Another example is if you ask the model to fix bugs in a piece of code, the model might reuse the majority of the original code with minor changes. Instead of making the model generate these repeated tokens, what if we copy these tokens from the input to speed up the generation? This is the core idea behind inference with reference.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"autoregressive decoding bottleneck\" data-secondary=\"inference with reference\" data-type=\"indexterm\" id=\"id1688\"></a><a contenteditable=\"false\" data-primary=\"inference with reference\" data-type=\"indexterm\" id=\"id1689\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"autoregressive decoding bottleneck\" data-tertiary=\"inference with reference\" data-type=\"indexterm\" id=\"id1690\"></a>Often, a response needs to reference tokens from the input. For example, if you ask your model a question about an attached document, the model might repeat a chunk of text verbatim from the document. Another example is if you ask the model to fix bugs in a piece of code, the model might reuse the majority of the original code with minor changes. Instead of making the model generate these repeated tokens, what if we copy these tokens from the input to speed up the generation? This is the core idea behind inference with reference.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Inference with reference is similar to speculative decoding, but instead of using a model to generate draft tokens, it selects draft tokens from the input. The key challenge is to develop an algorithm to identify the most relevant text span from the context at each decoding step. The simplest option is to find a text span that matches the current tokens.",
      "raw_html": "<p>Inference with reference is similar to speculative decoding, but instead of using a model to generate draft tokens, it selects draft tokens from the input. The key challenge is to develop an algorithm to identify the most relevant text span from the context at each decoding step. The simplest option is to find a text span that matches the current tokens.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Unlike speculative decoding, inference with reference doesn’t require an extra model. However, it’s useful only in generation scenarios where there’s a significant overlap between contexts and outputs, such as in retrieval systems, coding, or multi-turn conversations. In “Inference with Reference: Lossless Acceleration of Large Language Models” (Yang et al., 2023), this technique helps achieve two times generation speedup in such use cases.",
      "raw_html": "<p>Unlike speculative decoding, inference with reference doesn’t require an extra model. However, it’s useful only in generation scenarios where there’s a significant overlap between contexts and outputs, such as in retrieval systems, coding, or multi-turn conversations. In “Inference with Reference: Lossless Acceleration of Large Language Models” (<a href=\"https://arxiv.org/abs/2304.04487\">Yang et al., 2023</a>), this technique helps achieve two times generation speedup in such use cases.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Examples of how inference with reference works are shown in Figure 9-10.",
      "raw_html": "<p>Examples of how inference with reference works are shown in <a data-type=\"xref\" href=\"#ch09_figure_10_1730130962952808\">Figure 9-10</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_10_1730130962952808",
      "raw_html": "<figure class=\"width-60\"><div class=\"figure\" id=\"ch09_figure_10_1730130962952808\">\n<img alt=\"A screenshot of a diagram\n\nDescription automatically generated\" src=\"assets/aien_0910.png\"/>\n<h6><span class=\"label\">Figure 9-10. </span>Two examples of inference with reference. The text spans that are successfully copied from the input are in red and green. Image from Yang et al. (2023). The image is licensed under CC BY 4.0.</h6>\n</div></figure>",
      "image": "aien_0910.png",
      "alt": "A screenshot of a diagram\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0910.png",
      "caption": {
        "label": "Figure 9-10.",
        "text": "Two examples of inference with reference. The text spans that are successfully copied from the input are in red and green. Image from Yang et al. (2023). The image is licensed under CC BY 4.0."
      }
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Parallel decoding",
      "id": "heading-7",
      "raw_html": "<h4>Parallel decoding</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Instead of making autoregressive generation faster with draft tokens, some techniques aim to break the sequential dependency. Given an existing sequence of tokens x1, x2,…,xt, these techniques attempt to generate xt + 1, xt + 2,…,xt + k simultaneously. This means that the model generates xt + 2 before it knows that the token before it is xt + 1.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"autoregressive decoding bottleneck\" data-secondary=\"parallel decoding\" data-type=\"indexterm\" id=\"id1691\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"autoregressive decoding bottleneck\" data-tertiary=\"parallel decoding\" data-type=\"indexterm\" id=\"id1692\"></a><a contenteditable=\"false\" data-primary=\"parallel decoding\" data-type=\"indexterm\" id=\"id1693\"></a>Instead of making autoregressive generation faster with draft tokens, some techniques aim to break the sequential dependency. Given an existing sequence of tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>,…,<em>x</em><sub><em>t</em></sub>, these techniques attempt to generate <em>x</em><sub><em>t</em> + 1</sub>, <em>x</em><sub><em>t</em> + 2</sub>,…,<em>x</em><sub><em>t</em> + <em>k</em></sub> simultaneously. This means that the model generates <em>x</em><sub><em>t</em> + 2</sub> before it knows that the token before it is <em>x</em><sub><em>t</em> + 1</sub>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "This can work because the knowledge of the existing sequence often is sufficient to predict the next few tokens. For example, given “the cat sits”, without knowing that the next token is “on”, “under”, or “behind”, you might still predict that the word after it is “the”.",
      "raw_html": "<p>This can work because the knowledge of the existing sequence often is sufficient to predict the next few tokens. For example, given “the cat sits”, without knowing that the next token is “on”, “under”, or “behind”, you might still predict that the word after it is “the”.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The parallel tokens can be generated by the same decoder, as in Lookahead decoding (Fu et al., 2024), or by different decoding heads, as in Medusa (Cai et al., 2024). In Medusa, the original model is extended with multiple decoding heads, and each head is a small neural network layer that is then trained to predict a future token at a specific position. If the original model is trained to predict the next token xt + 1, the kth head will predict the token xt + k + 1. These heads are trained together with the original model, but the original model is frozen. NVIDIA claimed Medusa helped boost Llama 3.1 token generation by up to 1.9× on their HGX H200 GPUs (Eassa et al., 2024).",
      "raw_html": "<p>The parallel tokens can be generated by the same decoder, as in Lookahead decoding (<a href=\"https://arxiv.org/abs/2402.02057\">Fu et al., 2024</a>), or by different decoding heads, as in Medusa (<a href=\"https://arxiv.org/abs/2401.10774\">Cai et al., 2024</a>). In Medusa, the original model is extended with multiple decoding heads, and each head is a small neural network layer that is then trained to predict a future token at a specific position. If the original model is trained to predict the next token <em>x</em><sub><em>t</em> + 1</sub>, the <em>k</em><sup><em>th</em></sup> head will predict the token <em>x</em><sub><em>t</em> + <em>k</em> + 1</sub>. These heads are trained together with the original model, but the original model is frozen. NVIDIA claimed Medusa helped boost Llama 3.1 token generation by up to 1.9× on their HGX H200 GPUs (<a href=\"https://oreil.ly/FWYf5\">Eassa et al., 2024</a>).</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "However, because these tokens aren’t generated sequentially, they need to be verified to make sure that they fit together. An essential part of parallel decoding is verification and integration. Lookahead decoding uses the Jacobi method21 to verify the generated tokens, which works as follows:",
      "raw_html": "<p>However, because these tokens aren’t generated sequentially, they need to be verified to make sure that they fit together. An essential part of parallel decoding is verification and integration. Lookahead decoding uses the <a href=\"https://en.wikipedia.org/wiki/Jacobi_method\">Jacobi method</a><sup><a data-type=\"noteref\" href=\"ch09.html#id1694\" id=\"id1694-marker\">21</a></sup> to verify the generated tokens, which works as follows:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "K future tokens are generated in parallel.",
        "These K tokens are verified for coherence and consistency with the context.",
        "If one or more tokens fail verification, instead of aggregating all K future tokens, the model regenerates or adjusts only these failed tokens."
      ],
      "raw_html": "<ol>\n<li>\n<p>K future tokens are generated in parallel.</p>\n</li>\n<li>\n<p>These <em>K</em> tokens are verified for coherence and consistency with the context.</p>\n</li>\n<li>\n<p>If one or more tokens fail verification, instead of aggregating all <em>K</em> future tokens, the model regenerates or adjusts only these failed tokens.</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "K future tokens are generated in parallel.",
      "raw_html": "<p>K future tokens are generated in parallel.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "These K tokens are verified for coherence and consistency with the context.",
      "raw_html": "<p>These <em>K</em> tokens are verified for coherence and consistency with the context.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "If one or more tokens fail verification, instead of aggregating all K future tokens, the model regenerates or adjusts only these failed tokens.",
      "raw_html": "<p>If one or more tokens fail verification, instead of aggregating all <em>K</em> future tokens, the model regenerates or adjusts only these failed tokens.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "The model keeps refining the generated tokens until they all pass verification and are integrated into the final output. This family of parallel decoding algorithms is also called Jacobi decoding.",
      "raw_html": "<p>The model keeps refining the generated tokens until they all pass verification and are integrated into the final output. This family of parallel decoding algorithms is also called Jacobi decoding.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "On the other hand, Medusa uses a tree-based attention mechanism to verify and integrate tokens. Each Medusa head produces several options for each position. These options are then organized into a tree-like structure to select the most promising combination. The process is visualized in Figure 9-11.",
      "raw_html": "<p>On the other hand, Medusa uses a tree-based attention mechanism to verify and integrate tokens. Each Medusa head produces several options for each position. These options are then organized into a tree-like structure to select the most promising combination. The process is visualized in <a data-type=\"xref\" href=\"#ch09_figure_11_1730130962952823\">Figure 9-11</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_11_1730130962952823",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_11_1730130962952823\">\n<img alt=\"A diagram of a model\n\nDescription automatically generated\" src=\"assets/aien_0911.png\"/>\n<h6><span class=\"label\">Figure 9-11. </span>In Medusa (Cai et al., 2024), each head predicts several options for a token position. The most promising sequence from these options is selected. Image adapted from the paper, which is licensed under CC BY 4.0.</h6>\n</div></figure>",
      "image": "aien_0911.png",
      "alt": "A diagram of a model\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0911.png",
      "caption": {
        "label": "Figure 9-11.",
        "text": "In Medusa (Cai et al., 2024), each head predicts several options for a token position. The most promising sequence from these options is selected. Image adapted from the paper, which is licensed under CC BY 4.0."
      }
    },
    {
      "type": "paragraph",
      "content": "While the perspective of being able to circumvent sequential dependency is appealing, parallel decoding is not intuitive, and some techniques, like Medusa, can be challenging to implement.",
      "raw_html": "<p>While the perspective of being able to circumvent sequential dependency is appealing, parallel decoding is not intuitive, and some techniques, like Medusa, can be challenging to implement. <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html45\" data-type=\"indexterm\" id=\"id1695\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html44\" data-type=\"indexterm\" id=\"id1696\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html43\" data-type=\"indexterm\" id=\"id1697\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html42a\" data-type=\"indexterm\" id=\"id1698\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html42\" data-type=\"indexterm\" id=\"id1699\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Attention mechanism optimization",
      "id": "heading-7",
      "raw_html": "<h3>Attention mechanism optimization</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Recall from Chapter 2 that generating the next token requires the key and value vectors for all previous tokens. This means that the following applies:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"attention mechanisms\" data-secondary=\"optimization\" data-type=\"indexterm\" id=\"ch09.html49\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"model optimization\" data-tertiary=\"attention mechanism optimization\" data-type=\"indexterm\" id=\"ch09.html50\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"attention mechanism optimization\" data-type=\"indexterm\" id=\"ch09.html51\"></a>Recall from <a data-type=\"xref\" href=\"ch02.html#ch02_understanding_foundation_models_1730147895571359\">Chapter 2</a> that generating the next token requires the key and value vectors for all previous tokens. This means that the following applies:</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "Generating token xt requires the key and value vectors for tokens x1, x2, …, xt – 1.",
        "Generating token xt + 1 requires the key and value vectors for tokens x1, x2, …,xt – 1, xt."
      ],
      "raw_html": "<ul>\n<li><p>Generating token <em>x</em><sub><em>t</em></sub> requires the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em> – 1</sub>.</p></li>\n<li><p>Generating token <em>x</em><sub><em>t</em> + 1</sub> requires the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …,<em>x</em><sub><em>t</em> – 1</sub>, <em>x</em><sub><em>t</em></sub>.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "Generating token xt requires the key and value vectors for tokens x1, x2, …, xt – 1.",
      "raw_html": "<p>Generating token <em>x</em><sub><em>t</em></sub> requires the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em> – 1</sub>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Generating token xt + 1 requires the key and value vectors for tokens x1, x2, …,xt – 1, xt.",
      "raw_html": "<p>Generating token <em>x</em><sub><em>t</em> + 1</sub> requires the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …,<em>x</em><sub><em>t</em> – 1</sub>, <em>x</em><sub><em>t</em></sub>.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "When generating token xt + 1, instead of computing the key and value vectors for tokens x1, x2, …, xt – 1 again, you reuse these vectors from the previous step. This means that you’ll need to compute the key and value vectors for only the most recent token, xt. The cache that stores key and value vectors for reuse is called the KV cache. The newly computed key and value vectors are then added to the KV cache, which is visualized in Figure 9-12.",
      "raw_html": "<p>When generating token <em>x</em><sub><em>t</em> + 1</sub>, instead of computing the key and value vectors for tokens <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>t</em> – 1</sub> again, you reuse these vectors from the previous step. This means that you’ll need to compute the key and value vectors for only the most recent token, <em>x</em><sub><em>t</em></sub>. The cache that stores key and value vectors for reuse is called the KV cache. <a contenteditable=\"false\" data-primary=\"key-value (KV) cache\" data-type=\"indexterm\" id=\"ch09.html51a\"></a>The newly computed key and value vectors are then added to the KV cache, which is visualized in <a data-type=\"xref\" href=\"#ch09_figure_12_1730130962952844\">Figure 9-12</a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch09_figure_12_1730130962952844",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_12_1730130962952844\">\n<img alt=\"A diagram of a graph\n\nDescription automatically generated\" src=\"assets/aien_0912.png\"/>\n<h6><span class=\"label\">Figure 9-12. </span>To avoid recomputing the key and value vectors at each decoding step, use a KV cache to store these vectors to reuse.</h6>\n</div></figure>",
      "image": "aien_0912.png",
      "alt": "A diagram of a graph\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0912.png",
      "caption": {
        "label": "Figure 9-12.",
        "text": "To avoid recomputing the key and value vectors at each decoding step, use a KV cache to store these vectors to reuse."
      }
    },
    {
      "type": "callout",
      "callout_type": "note",
      "content": "A KV cache is used only during inference, not training. During training, because all tokens in a sequence are known in advance, next token generation can be computed all at once instead of sequentially, as during inference. Therefore, there’s no need for a KV cache.",
      "raw_html": "<div data-type=\"note\" epub:type=\"note\"><h6>Note</h6>\n<p>A KV cache is used only during inference, not training. During training, because all tokens in a sequence are known in advance, next token generation can be computed all at once instead of sequentially, as during inference. Therefore, there’s no need for a KV cache.</p>\n</div>"
    },
    {
      "type": "paragraph",
      "content": "Because generating a token requires computing the attention scores with all previous tokens, the number of attention computations grows exponentially with sequence length.22 The KV cache size, on the other hand, grows linearly with sequence length.",
      "raw_html": "<p>Because generating a token requires computing the attention scores with all previous tokens, the number of attention computations grows exponentially with sequence length.<sup><a data-type=\"noteref\" href=\"ch09.html#id1700\" id=\"id1700-marker\">22</a></sup> The KV cache size, on the other hand, grows linearly with sequence length.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The KV cache size also grows with larger batch sizes. A Google paper calculated that for a 500B+ model with multi-head attention, batch size 512, and context length 2048, the KV cache totals 3TB (Pope et al., 2022). This is three times the size of that model’s weights.",
      "raw_html": "<p>The KV cache size also grows with larger batch sizes. A Google paper calculated that for a 500B+ model with multi-head attention, batch size 512, and context length 2048, the KV cache totals 3TB <a href=\"https://arxiv.org/abs/2211.05102\">(Pope et al., 2022)</a>. This is three times the size of that model’s weights.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The KV cache size is ultimately limited by the available hardware storage, creating a bottleneck for running applications with long context. A large cache size also takes time to load into memory, which can be an issue for applications with strict latency.",
      "raw_html": "<p>The KV cache size is ultimately limited by the available hardware storage, creating a bottleneck for running applications with long context. A large cache size also takes time to load into memory, which can be an issue for applications with strict latency. </p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The computation and memory requirements of the attention mechanism are one of the reasons why it’s so hard to have longer context.",
      "raw_html": "<p>The computation and memory requirements of the attention mechanism are one of the reasons why it’s so hard to have longer context.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Many techniques have been developed to make the attention mechanism more efficient. In general, they fall into three buckets: redesigning the attention mechanism, optimizing the KV cache, and writing kernels for attention computation.",
      "raw_html": "<p>Many techniques have been developed to make the attention mechanism more efficient. In general, they fall into three buckets: redesigning the attention mechanism, optimizing the KV cache, and writing kernels for attention computation.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "aside",
      "content": "Calculating the KV Cache Size\nThe memory needed for the KV cache, without any optimization, is calculated as follows:\n\n2 × B × S × L × H × M\n\n\n\nB: batch size\n\n\nS: sequence length\n\n\nL: number of transformer layers\n\n\nH: model dimension\n\n\nM: memory needed for the cache’s numerical representation (e.g., FP16 or FP32).\n\n\nThis value can become substantial as the context length increases. For example, LLama 2 13B has 40 layers and a model dimension of 5,120. With a batch size of 32, sequence length of 2,048, and 2 bytes per value, the memory needed for its KV cache, without any optimization, is 2 × 32 × 2,048 × 40 × 5,120 × 2 = 54 GB.",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"ch09_calculating_the_kv_cache_size_1730130963008252\">\n<h1>Calculating the KV Cache Size</h1>\n<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"KV cache size calculation\" data-type=\"indexterm\" id=\"id1701\"></a>The memory needed for the KV cache, without any optimization, is calculated as <span class=\"keep-together\">follows:</span></p>\n<ul class=\"simplelist\">\n<li><p>2 × <em>B</em> × <em>S</em> × <em>L</em> × <em>H</em> × <em>M</em></p></li>\n</ul>\n<ul>\n<li>\n<p><em>B</em>: batch size</p>\n</li>\n<li>\n<p><em>S</em>: sequence length</p>\n</li>\n<li>\n<p><em>L</em>: number of transformer layers</p>\n</li>\n<li>\n<p><em>H</em>: model dimension</p>\n</li>\n<li>\n<p><em>M</em>: memory needed for the cache’s numerical representation (e.g., FP16 or FP32).</p>\n</li>\n</ul>\n<p>This value can become substantial as the context length increases. For example, LLama 2 13B has 40 layers and a model dimension of 5,120. With a batch size of 32, sequence length of 2,048, and 2 bytes per value, the memory needed for its KV cache, without any optimization, is 2 × 32 × 2,048 × 40 × 5,120 × 2 = 54 GB.</p>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Calculating the KV Cache Size",
      "id": "heading-7",
      "raw_html": "<h1>Calculating the KV Cache Size</h1>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "The memory needed for the KV cache, without any optimization, is calculated as follows:",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"KV cache size calculation\" data-type=\"indexterm\" id=\"id1701\"></a>The memory needed for the KV cache, without any optimization, is calculated as <span class=\"keep-together\">follows:</span></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "2 × B × S × L × H × M"
      ],
      "raw_html": "<ul class=\"simplelist\">\n<li><p>2 × <em>B</em> × <em>S</em> × <em>L</em> × <em>H</em> × <em>M</em></p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "2 × B × S × L × H × M",
      "raw_html": "<p>2 × <em>B</em> × <em>S</em> × <em>L</em> × <em>H</em> × <em>M</em></p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "B: batch size",
        "S: sequence length",
        "L: number of transformer layers",
        "H: model dimension",
        "M: memory needed for the cache’s numerical representation (e.g., FP16 or FP32)."
      ],
      "raw_html": "<ul>\n<li>\n<p><em>B</em>: batch size</p>\n</li>\n<li>\n<p><em>S</em>: sequence length</p>\n</li>\n<li>\n<p><em>L</em>: number of transformer layers</p>\n</li>\n<li>\n<p><em>H</em>: model dimension</p>\n</li>\n<li>\n<p><em>M</em>: memory needed for the cache’s numerical representation (e.g., FP16 or FP32).</p>\n</li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "B: batch size",
      "raw_html": "<p><em>B</em>: batch size</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "S: sequence length",
      "raw_html": "<p><em>S</em>: sequence length</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "L: number of transformer layers",
      "raw_html": "<p><em>L</em>: number of transformer layers</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "H: model dimension",
      "raw_html": "<p><em>H</em>: model dimension</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "M: memory needed for the cache’s numerical representation (e.g., FP16 or FP32).",
      "raw_html": "<p><em>M</em>: memory needed for the cache’s numerical representation (e.g., FP16 or FP32).</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "This value can become substantial as the context length increases. For example, LLama 2 13B has 40 layers and a model dimension of 5,120. With a batch size of 32, sequence length of 2,048, and 2 bytes per value, the memory needed for its KV cache, without any optimization, is 2 × 32 × 2,048 × 40 × 5,120 × 2 = 54 GB.",
      "raw_html": "<p>This value can become substantial as the context length increases. For example, LLama 2 13B has 40 layers and a model dimension of 5,120. With a batch size of 32, sequence length of 2,048, and 2 bytes per value, the memory needed for its KV cache, without any optimization, is 2 × 32 × 2,048 × 40 × 5,120 × 2 = 54 GB.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Redesigning the attention mechanism",
      "id": "heading-7",
      "raw_html": "<h4>Redesigning the attention mechanism</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "These techniques involve altering how the attention mechanism works. Even though these techniques help optimize inference, because they change a model’s architecture directly, they can be applied only during training or finetuning.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"attention mechanisms\" data-secondary=\"optimization\" data-tertiary=\"attention mechanism redesign\" data-type=\"indexterm\" id=\"id1702\"></a><a contenteditable=\"false\" data-primary=\"attention mechanisms\" data-secondary=\"redesign\" data-type=\"indexterm\" id=\"id1703\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"attention mechanism optimization\" data-tertiary=\"attention mechanism redesign\" data-type=\"indexterm\" id=\"id1704\"></a>These techniques involve altering how the attention mechanism works. Even though these techniques help optimize inference, because they change a model’s architecture directly, they can be applied only during training or finetuning. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "For example, when generating a new token, instead of attending to all previous tokens, local windowed attention attends only to a fixed size window of nearby tokens (Beltagy et al., 2020). This reduces the effective sequence length to a fixed size window, reducing both the KV cache and the attention computation. If the average sequence length is 10,000 tokens, attending to a window size of 1,000 tokens reduces the KV cache size by 10 times.",
      "raw_html": "<p>For example, when generating a new token, instead of attending to all previous tokens, <em>local windowed attention</em> attends only to a fixed size window of nearby tokens (<a href=\"https://arxiv.org/abs/2004.05150v2\">Beltagy et al., 2020</a>). This reduces the effective sequence length to a fixed size window, reducing both the KV cache and the attention computation. If the average sequence length is 10,000 tokens, attending to a window size of 1,000 tokens reduces the KV cache size by 10 times.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Local windowed attention can be interleaved with global attention, with local attention capturing nearby context; the global attention captures task-specific information across the document.",
      "raw_html": "<p>Local windowed attention can be interleaved with global attention, with local attention capturing nearby context; the global attention captures task-specific information across the document.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Both cross-layer attention (Brandon et al., 2024) and multi-query attention (Shazeer, 2019) reduce the memory footprint of the KV cache by reducing the number of key-value pairs. Cross-layer attention shares key and value vectors across adjacent layers. Having three layers sharing the same key-value vectors means reducing the KV cache three times. On the other hand, multi-query attention shares key-value vectors across query heads.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"cross-layer attention\" data-type=\"indexterm\" id=\"id1705\"></a><a contenteditable=\"false\" data-primary=\"multi-query attention\" data-type=\"indexterm\" id=\"id1706\"></a>Both <em>cross-layer attention</em> (<a href=\"https://arxiv.org/abs/2405.12981?ref=research.character.ai\">Brandon et al., 2024</a>) and <em>multi-query attention</em> (<a href=\"https://arxiv.org/abs/1911.02150?ref=research.character.ai\">Shazeer, 2019</a>) reduce the memory footprint of the KV cache by reducing the number of key-value pairs. Cross-layer attention shares key and value vectors across adjacent layers. Having three layers sharing the same key-value vectors means reducing the KV cache three times. On the other hand, multi-query attention shares key-value vectors across query heads.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Grouped-query attention (Ainslie et al., 2023) is a generalization of multi-query attention. Instead of using only one set of key-value pairs for all query heads, its grouped-query attention puts query heads into smaller groups and shares key-value pairs only among query heads in the same group. This allows for a more flexible balance between the number of query heads and the number of key-value pairs.",
      "raw_html": "<p class=\"pagebreak-before\"><a contenteditable=\"false\" data-primary=\"grouped-query attention\" data-type=\"indexterm\" id=\"id1707\"></a><em>Grouped-query attention</em> (<a href=\"https://arxiv.org/abs/2305.13245\">Ainslie et al., 2023</a>) is a generalization of multi-query attention. Instead of using only one set of key-value pairs for all query heads, its grouped-query attention puts query heads into smaller groups and shares key-value pairs only among query heads in the same group. This allows for a more flexible balance between the number of query heads and the number of key-value pairs.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Character.AI, an AI chatbot application, shares that their average conversation has a dialogue history of 180 messages (2024). Given the typically long sequences, the primary bottleneck for inference throughput is the KV cache size. Three attention mechanism designs—multi-query attention, interleaving local attention and global attention, and cross-layer attention—help them reduce KV cache by over 20 times. More importantly, this significant KV cache reduction means that memory is no longer a bottleneck for them for serving large batch sizes.",
      "raw_html": "<p>Character.AI, an AI chatbot application, shares that their average conversation has a dialogue history of <a href=\"https://oreil.ly/nLt6A\">180 messages</a> (2024). Given the typically long sequences, the primary bottleneck for inference throughput is the KV cache size. Three attention mechanism designs—multi-query attention, interleaving local attention and global attention, and cross-layer attention—help them <em>reduce KV cache by over 20 times</em>. More importantly, this significant KV cache reduction means that memory is no longer a bottleneck for them for serving large batch sizes.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Optimizing the KV cache size",
      "id": "heading-7",
      "raw_html": "<h4>Optimizing the KV cache size</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "The way the KV cache is managed is critical in mitigating the memory bottleneck during inference and enabling a larger batch size, especially for applications with long context. Many techniques are actively being developed to reduce and manage the KV cache.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"attention mechanism optimization\" data-tertiary=\"KV cache size optimization\" data-type=\"indexterm\" id=\"id1708\"></a>The way the KV cache is managed is critical in mitigating the memory bottleneck during inference and enabling a larger batch size, especially for applications with long context. Many techniques are actively being developed to reduce and manage the KV cache. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One of the fastest growing inference frameworks, vLLM, gained popularity for introducing PagedAttention, which optimizes memory management by dividing the KV cache into non-contiguous blocks, reducing fragmentation, and enabling flexible memory sharing to improve LLM serving efficiency (Kwon et al., 2023).",
      "raw_html": "<p>One of the fastest growing inference frameworks, <a href=\"https://github.com/vllm-project/vllm\">vLLM</a>, gained popularity for introducing PagedAttention, which optimizes memory management by dividing the KV cache into non-contiguous blocks, reducing fragmentation, and enabling flexible memory sharing to improve LLM serving efficiency (<a href=\"https://arxiv.org/abs/2309.06180\">Kwon et al., 2023</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Other techniques include KV cache quantization (Hooper et al., 2024; Kang et al., 2024), adaptive KV cache compression (Ge et al., 2023), and selective KV cache (Liu et al., 2024).",
      "raw_html": "<p>Other techniques include KV cache quantization (<a href=\"https://arxiv.org/abs/2401.18079\">Hooper et al., 2024</a>; <a href=\"https://arxiv.org/abs/2403.05527\">Kang et al., 2024</a>), adaptive KV cache compression (<a href=\"https://arxiv.org/abs/2310.01801\">Ge et al., 2023</a>), and selective KV cache (<a href=\"https://oreil.ly/ixtBl\">Liu et al., 2024</a>).<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html51a\" data-type=\"indexterm\" id=\"id1709\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 4,
      "content": "Writing kernels for attention computation",
      "id": "heading-7",
      "raw_html": "<h4>Writing kernels for attention computation</h4>",
      "section_type": "sect4"
    },
    {
      "type": "paragraph",
      "content": "Instead of changing the mechanism design or optimizing the storage, this approach looks into how attention scores are computed and finds ways to make this computation more efficient. This approach is the most effective when it takes into account the hardware executing the computation. The code optimized for a specific chip is called a kernel. Kernel writing will be discussed further in the next section.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"attention mechanisms\" data-secondary=\"optimization\" data-tertiary=\"wiring kernels for attention computation\" data-type=\"indexterm\" id=\"id1710\"></a><a contenteditable=\"false\" data-primary=\"kernels\" data-type=\"indexterm\" id=\"id1711\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"attention mechanism optimization\" data-tertiary=\"write kernels for attention computation\" data-type=\"indexterm\" id=\"id1712\"></a>Instead of changing the mechanism design or optimizing the storage, this approach looks into how attention scores are computed and finds ways to make this computation more efficient. This approach is the most effective when it takes into account the hardware executing the computation. The code optimized for a specific chip is called a kernel. Kernel writing will be discussed further in the next section.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One of the most well-known kernels optimized for attention computation is FlashAttention (Dao et al., 2022). This kernel fused together many operations commonly used in a transformer-based model to make them run faster, as shown in Figure 9-13.",
      "raw_html": "<p>One of the most well-known kernels optimized for attention computation is <a href=\"https://github.com/Dao-AILab/flash-attention\">FlashAttention</a> (Dao et al., 2022). This kernel fused together many operations commonly used in a transformer-based model to make them run faster, as shown in <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html51\" data-type=\"indexterm\" id=\"id1713\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html50\" data-type=\"indexterm\" id=\"id1714\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html49\" data-type=\"indexterm\" id=\"id1715\"></a><a data-type=\"xref\" href=\"#ch09_figure_13_1730130962952862\">Figure 9-13</a>. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_13_1730130962952862",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_13_1730130962952862\">\n<img alt=\"A graph of a graph with text\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0913.png\"/>\n<h6><span class=\"label\">Figure 9-13. </span>FlashAttention is a kernel that fuses together several common operators. Adapted from an original image licensed under BSD 3-Clause.</h6>\n</div></figure>",
      "image": "aien_0913.png",
      "alt": "A graph of a graph with text\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0913.png",
      "caption": {
        "label": "Figure 9-13.",
        "text": "FlashAttention is a kernel that fuses together several common operators. Adapted from an original image licensed under BSD 3-Clause."
      }
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Kernels and compilers",
      "id": "heading-7",
      "raw_html": "<h3>Kernels and compilers</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Kernels are specialized pieces of code optimized for specific hardware accelerators, such as GPUs or TPUs. They are typically written to perform computationally intensive routines that need to be executed repeatedly, often in parallel, to maximize the performance of these accelerators.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"model optimization\" data-tertiary=\"kernels and compilers\" data-type=\"indexterm\" id=\"ch09.html52\"></a><a contenteditable=\"false\" data-primary=\"kernels\" data-type=\"indexterm\" id=\"ch09.html53\"></a><a contenteditable=\"false\" data-primary=\"model optimization\" data-secondary=\"kernels and compilers\" data-type=\"indexterm\" id=\"ch09.html54\"></a>Kernels are specialized pieces of code optimized for specific hardware accelerators, such as GPUs or TPUs. They are typically written to perform computationally intensive routines that need to be executed repeatedly, often in parallel, to maximize the performance of these accelerators.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Common AI operations, including matrix multiplication, attention computation, and convolution operation, all have specialized kernels to make their computation more efficient on different hardware.23",
      "raw_html": "<p>Common AI operations, including matrix multiplication, attention computation, and convolution operation, all have specialized kernels to make their computation more efficient on different hardware.<sup><a data-type=\"noteref\" href=\"ch09.html#id1716\" id=\"id1716-marker\">23</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Writing kernels requires a deep understanding of the underlying hardware architecture. This includes knowledge about how the memory hierarchy is structured (such as caches, global memory, shared memory, and registers) and how data is accessed and moved between these different levels.",
      "raw_html": "<p>Writing kernels requires a deep understanding of the underlying hardware architecture. This includes knowledge about how the memory hierarchy is structured (such as caches, global memory, shared memory, and registers) and how data is accessed and moved between these different levels.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Moreover, kernels are typically written in lower-level programming languages like CUDA (for NVIDIA GPUs), Triton (a language developed by OpenAI for writing custom kernels), and ROCm (for AMD GPUs). These languages allow fine-grained control over thread management and memory access but are also harder to learn than the languages that most AI engineers are familiar with, like Python.",
      "raw_html": "<p>Moreover, kernels are typically written in lower-level programming languages like CUDA (for NVIDIA GPUs), Triton (a language developed by OpenAI for writing custom kernels), and ROCm (for AMD GPUs). These languages allow fine-grained control over thread management and memory access but are also harder to learn than the languages that most AI engineers are familiar with, like Python.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Due to this entry barrier, writing kernels used to be a dark art practiced by a few. Chip makers like NVIDIA and AMD employ optimization engineers to write kernels to make their hardware efficient for AI workloads, whereas AI frameworks like PyTorch and TensorFlow employ kernel engineers to optimize their frameworks on different accelerators.",
      "raw_html": "<p>Due to this entry barrier, writing kernels used to be a dark art practiced by a few. Chip makers like NVIDIA and AMD employ optimization engineers to write kernels to make their hardware efficient for AI workloads, whereas AI frameworks like PyTorch and TensorFlow employ kernel engineers to optimize their frameworks on different accelerators.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "However, with the rising demand for inference optimization and the ubiquity of accelerators, more AI engineers have taken an interest in writing kernels. There are many great online tutorials for kernel writing. Here, I’ll cover four common techniques often used to speed up computation:",
      "raw_html": "<p>However, with the rising demand for inference optimization and the ubiquity of accelerators, more AI engineers have taken an interest in writing kernels. There are many great online tutorials for kernel writing. Here, I’ll cover four common techniques often used to speed up computation:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "definition_list",
      "definitions": [
        {
          "term": "Vectorization",
          "definition": "Given a loop or a nested loop, instead of processing one data element at a time, simultaneously execute multiple data elements that are contiguous in memory. This reduces latency by minimizing data I/O operations."
        },
        {
          "term": "Parallelization",
          "definition": "Divide an input array (or n-dimensional array) into independent chunks that can be processed simultaneously on different cores or threads, speeding up the computation."
        },
        {
          "term": "Loop tiling",
          "definition": "Optimize the data accessing order in a loop for the hardware’s memory layout and cache. This optimization is hardware-dependent. An efficient CPU tiling pattern may not work well on GPUs."
        },
        {
          "term": "Operator fusion",
          "definition": "Combine multiple operators into a single pass to avoid redundant memory access. For example, if two loops operate over the same array, they can be fused into one, reducing the number of times data is read and written."
        }
      ],
      "raw_html": "<dl>\n<dt>Vectorization</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"vectorization\" data-type=\"indexterm\" id=\"id1717\"></a>Given a loop or a nested loop, instead of processing one data element at a time, simultaneously execute multiple data elements that are contiguous in memory. This reduces latency by minimizing data I/O operations.</p></dd>\n<dt>Parallelization</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"parallelization\" data-type=\"indexterm\" id=\"id1718\"></a>Divide an input array (or n-dimensional array) into independent chunks that can be processed simultaneously on different cores or threads, speeding up the computation.</p></dd>\n<dt>Loop tiling</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"loop tiling\" data-type=\"indexterm\" id=\"id1719\"></a>Optimize the data accessing order in a loop for the hardware’s memory layout and cache. This optimization is hardware-dependent. An efficient CPU tiling pattern may not work well on GPUs.</p></dd>\n<dt>Operator fusion</dt>\n<dd><p><a contenteditable=\"false\" data-primary=\"operator fusion\" data-type=\"indexterm\" id=\"id1720\"></a>Combine multiple operators into a single pass to avoid redundant memory access. For example, if two loops operate over the same array, they can be fused into one, reducing the number of times data is read and written.</p></dd>\n<dd><p>While vectorization, parallelization, and loop tiling can be applied broadly across different models, operator fusion requires a deeper understanding of a model’s specific operators and architecture. As a result, operator fusion demands more attention from optimization engineers.</p></dd>\n</dl>"
    },
    {
      "type": "paragraph",
      "content": "Given a loop or a nested loop, instead of processing one data element at a time, simultaneously execute multiple data elements that are contiguous in memory. This reduces latency by minimizing data I/O operations.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"vectorization\" data-type=\"indexterm\" id=\"id1717\"></a>Given a loop or a nested loop, instead of processing one data element at a time, simultaneously execute multiple data elements that are contiguous in memory. This reduces latency by minimizing data I/O operations.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Divide an input array (or n-dimensional array) into independent chunks that can be processed simultaneously on different cores or threads, speeding up the computation.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"parallelization\" data-type=\"indexterm\" id=\"id1718\"></a>Divide an input array (or n-dimensional array) into independent chunks that can be processed simultaneously on different cores or threads, speeding up the computation.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Optimize the data accessing order in a loop for the hardware’s memory layout and cache. This optimization is hardware-dependent. An efficient CPU tiling pattern may not work well on GPUs.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"loop tiling\" data-type=\"indexterm\" id=\"id1719\"></a>Optimize the data accessing order in a loop for the hardware’s memory layout and cache. This optimization is hardware-dependent. An efficient CPU tiling pattern may not work well on GPUs.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Combine multiple operators into a single pass to avoid redundant memory access. For example, if two loops operate over the same array, they can be fused into one, reducing the number of times data is read and written.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"operator fusion\" data-type=\"indexterm\" id=\"id1720\"></a>Combine multiple operators into a single pass to avoid redundant memory access. For example, if two loops operate over the same array, they can be fused into one, reducing the number of times data is read and written.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While vectorization, parallelization, and loop tiling can be applied broadly across different models, operator fusion requires a deeper understanding of a model’s specific operators and architecture. As a result, operator fusion demands more attention from optimization engineers.",
      "raw_html": "<p>While vectorization, parallelization, and loop tiling can be applied broadly across different models, operator fusion requires a deeper understanding of a model’s specific operators and architecture. As a result, operator fusion demands more attention from optimization engineers.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Kernels are optimized for a hardware architecture. This means that whenever a new hardware architecture is introduced, new kernels need to be developed. For example, FlashAttention (Dao et al., 2022) was originally developed primarily for NVIDIA A100 GPUs. Later on, FlashAttention-3 was introduced for H100 GPUs (Shah et al., 2024).",
      "raw_html": "<p>Kernels are optimized for a hardware architecture. This means that whenever a new hardware architecture is introduced, new kernels need to be developed. For example, <a href=\"https://github.com/Dao-AILab/flash-attention\">FlashAttention</a> (Dao et al., 2022) was originally developed primarily for NVIDIA A100 GPUs. Later on, FlashAttention-3 was introduced for H100 GPUs (<a href=\"https://arxiv.org/abs/2407.08608\">Shah et al., 2024</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A model script specifies a series of operations that need to be performed to execute that model. To run this code on a piece of hardware, such as a GPU, it has to be converted into a language compatible with that hardware. This process is called lowering. A tool that lowers code to run a specific hardware is called a compiler. Compilers bridge ML models and the hardware they run on. During the lowering process, whenever possible, these operations are converted into specialized kernels to run faster on the target hardware.",
      "raw_html": "<p>A model script specifies a series of operations that need to be performed to execute that model. To run this code on a piece of hardware, such as a GPU, it has to be converted into a language compatible with that hardware. This process is called <em>lowering</em>. <a contenteditable=\"false\" data-primary=\"compilers\" data-type=\"indexterm\" id=\"id1721\"></a>A tool that <em>lowers</em> code to run a specific hardware is called a compiler. Compilers bridge ML models and the hardware they run on. During the lowering process, whenever possible, these operations are converted into specialized kernels to run faster on the target hardware.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "aside",
      "content": "Inference Optimization Case Study from PyTorch\nFigure 9-14 shows how much throughput improvement the PyTorch team could give to Llama-7B through the following optimization steps (PyTorch, 2023):\n\n\nCall torch.compile to compile the model into more efficient kernels.\n\n\nQuantize the model weights to INT8.\n\n\nFurther quantize the model weights to INT4.\n\n\nAdd speculative decoding.\n\n\n\n\nFigure 9-14. Throughput improvement by different optimization techniques in PyTorch. Image from PyTorch (2023).\n\nThe experiment was run on an A100 GPU with 80 GB of memory. It was unclear how these optimization steps impact the model’s output quality.",
      "raw_html": "<aside data-type=\"sidebar\" epub:type=\"sidebar\"><div class=\"sidebar\" id=\"ch09_inference_optimization_case_study_from_pytorch_1730130963008570\">\n<h1>Inference Optimization Case Study from PyTorch</h1>\n<p><a contenteditable=\"false\" data-primary=\"Llama\" data-secondary=\"inference optimization\" data-type=\"indexterm\" id=\"id1722\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"case study from PyTorch\" data-type=\"indexterm\" id=\"id1723\"></a><a data-type=\"xref\" href=\"#ch09_figure_14_1730130962952879\">Figure 9-14</a> shows how much throughput improvement the PyTorch team could give to Llama-7B through the following optimization steps (<a href=\"https://oreil.ly/_5Nqa\">PyTorch, 2023</a>):</p>\n<ol>\n<li>\n<p>Call torch.compile to compile the model into more efficient kernels.</p>\n</li>\n<li>\n<p>Quantize the model weights to INT8.</p>\n</li>\n<li>\n<p>Further quantize the model weights to INT4.</p>\n</li>\n<li>\n<p>Add speculative decoding.</p>\n</li>\n</ol>\n<figure><div class=\"figure\" id=\"ch09_figure_14_1730130962952879\">\n<img alt=\"A graph with numbers and a bar\n\nDescription automatically generated\" src=\"assets/aien_0914.png\"/>\n<h6><span class=\"label\">Figure 9-14. </span>Throughput improvement by different optimization techniques in PyTorch. Image from PyTorch (2023).</h6>\n</div></figure>\n<p>The experiment was run on an A100 GPU with 80 GB of memory. It was unclear how these optimization steps impact the model’s output quality.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html41\" data-type=\"indexterm\" id=\"id1724\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html40\" data-type=\"indexterm\" id=\"id1725\"></a></p>\n</div></aside>",
      "data_type": "sidebar"
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Inference Optimization Case Study from PyTorch",
      "id": "heading-7",
      "raw_html": "<h1>Inference Optimization Case Study from PyTorch</h1>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Figure 9-14 shows how much throughput improvement the PyTorch team could give to Llama-7B through the following optimization steps (PyTorch, 2023):",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"Llama\" data-secondary=\"inference optimization\" data-type=\"indexterm\" id=\"id1722\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"case study from PyTorch\" data-type=\"indexterm\" id=\"id1723\"></a><a data-type=\"xref\" href=\"#ch09_figure_14_1730130962952879\">Figure 9-14</a> shows how much throughput improvement the PyTorch team could give to Llama-7B through the following optimization steps (<a href=\"https://oreil.ly/_5Nqa\">PyTorch, 2023</a>):</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "ordered",
      "items": [
        "Call torch.compile to compile the model into more efficient kernels.",
        "Quantize the model weights to INT8.",
        "Further quantize the model weights to INT4.",
        "Add speculative decoding."
      ],
      "raw_html": "<ol>\n<li>\n<p>Call torch.compile to compile the model into more efficient kernels.</p>\n</li>\n<li>\n<p>Quantize the model weights to INT8.</p>\n</li>\n<li>\n<p>Further quantize the model weights to INT4.</p>\n</li>\n<li>\n<p>Add speculative decoding.</p>\n</li>\n</ol>"
    },
    {
      "type": "paragraph",
      "content": "Call torch.compile to compile the model into more efficient kernels.",
      "raw_html": "<p>Call torch.compile to compile the model into more efficient kernels.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Quantize the model weights to INT8.",
      "raw_html": "<p>Quantize the model weights to INT8.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Further quantize the model weights to INT4.",
      "raw_html": "<p>Further quantize the model weights to INT4.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Add speculative decoding.",
      "raw_html": "<p>Add speculative decoding.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_14_1730130962952879",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_14_1730130962952879\">\n<img alt=\"A graph with numbers and a bar\n\nDescription automatically generated\" src=\"assets/aien_0914.png\"/>\n<h6><span class=\"label\">Figure 9-14. </span>Throughput improvement by different optimization techniques in PyTorch. Image from PyTorch (2023).</h6>\n</div></figure>",
      "image": "aien_0914.png",
      "alt": "A graph with numbers and a bar\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0914.png",
      "caption": {
        "label": "Figure 9-14.",
        "text": "Throughput improvement by different optimization techniques in PyTorch. Image from PyTorch (2023)."
      }
    },
    {
      "type": "paragraph",
      "content": "The experiment was run on an A100 GPU with 80 GB of memory. It was unclear how these optimization steps impact the model’s output quality.",
      "raw_html": "<p>The experiment was run on an A100 GPU with 80 GB of memory. It was unclear how these optimization steps impact the model’s output quality.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html41\" data-type=\"indexterm\" id=\"id1724\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html40\" data-type=\"indexterm\" id=\"id1725\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Compilers can be standalone tools, such as Apache TVM and MLIR (Multi-Level Intermediate Representation) or integrated into ML and inference frameworks, like torch.compile (a feature in PyTorch), XLA (Accelerated Linear Algebra, originally developed by TensorFlow, with an open source version called OpenXLA), and the compiler built into the TensorRT, which is optimized for NVIDIA GPUs. AI companies might have their own compilers, with their proprietary kernels designed to speed up their own workloads.24",
      "raw_html": "<p class=\"pagebreak-before\">Compilers can be standalone tools, such as <a href=\"https://github.com/apache/tvm\">Apache TVM</a> and <a href=\"https://mlir.llvm.org\">MLIR</a> (Multi-Level Intermediate Representation) or integrated into ML and inference frameworks, like <a href=\"https://oreil.ly/6bjVM\"><code>torch.compile</code></a> (a feature in PyTorch), <a href=\"https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra\">XLA</a> (Accelerated Linear Algebra, originally developed by TensorFlow, with an open source version called <a href=\"https://github.com/openxla/xla\">OpenXLA</a>), and the compiler built into the <a href=\"https://github.com/NVIDIA/TensorRT\">TensorRT</a>, which is optimized for NVIDIA GPUs. AI companies might have their own compilers, with their proprietary kernels designed to speed up their own workloads<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html54\" data-type=\"indexterm\" id=\"id1726\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html53\" data-type=\"indexterm\" id=\"id1727\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html52\" data-type=\"indexterm\" id=\"id1728\"></a>.<sup><a data-type=\"noteref\" href=\"ch09.html#id1729\" id=\"id1729-marker\">24</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 2,
      "content": "Inference Service Optimization",
      "id": "heading-7",
      "raw_html": "<h2>Inference Service Optimization</h2>",
      "section_type": "sect2"
    },
    {
      "type": "paragraph",
      "content": "Most service-level optimization techniques focus on resource management. Given a fixed amount of resources (compute and memory) and dynamic workloads (inference requests from users that may involve different models), the goal is to efficiently allocate resources to these workloads to optimize for latency and cost. Unlike many model-level techniques, service-level techniques don’t modify models and shouldn’t change the output quality.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference service optimization\" data-type=\"indexterm\" id=\"ch09.html55\"></a><a contenteditable=\"false\" data-primary=\"inference service optimization\" data-type=\"indexterm\" id=\"ch09.html56\"></a>Most service-level optimization techniques focus on resource management. Given a fixed amount of resources (compute and memory) and dynamic workloads (inference requests from users that may involve different models), the goal is to efficiently allocate resources to these workloads to optimize for latency and cost. Unlike many model-level techniques, service-level techniques don’t modify models and shouldn’t change the output quality.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Batching",
      "id": "heading-7",
      "raw_html": "<h3>Batching</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "One of the easiest ways to reduce your cost is batching. In production, your inference service might receive multiple requests simultaneously. Instead of processing each request separately, batching the requests that arrive around the same time together can significantly reduce the service’s throughput. If processing each request separately is like everyone driving their own car, batching is like putting them together on a bus. A bus can move more people, but it can also make each person’s journey longer. However, if you do it intelligently, the impact on latency can be minimal.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference service optimization\" data-tertiary=\"batching\" data-type=\"indexterm\" id=\"id1730\"></a>One of the easiest ways to reduce your cost is batching. In production, your inference service might receive multiple requests simultaneously. Instead of processing each request separately, batching the requests that arrive around the same time together can significantly reduce the service’s throughput. If processing each request separately is like everyone driving their own car, batching is like putting them together on a bus. A bus can move more people, but it can also make each person’s journey longer. However, if you do it intelligently, the impact on latency can be minimal.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The three main techniques for batching are: static batching, dynamic batching, and continuous batching.",
      "raw_html": "<p>The three main techniques for batching are: static batching, dynamic batching, and continuous batching.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The simplest batching technique is static batching. The service groups a fixed number of inputs together in a batch. It’s like a bus that waits until every seat is filled before departing. The drawback of static batching is that all requests have to wait until the batch is full to be executed. Thus the first request in a batch is delayed until the batch’s last request arrives, no matter how late the last request is.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"batching\" data-secondary=\"static\" data-type=\"indexterm\" id=\"id1731\"></a><a contenteditable=\"false\" data-primary=\"static batching\" data-type=\"indexterm\" id=\"id1732\"></a>The simplest batching technique is <em>static batching</em>. The service groups a fixed number of inputs together in a batch. It’s like a bus that waits until every seat is filled before departing. The drawback of static batching is that all requests have to wait until the batch is full to be executed. Thus the first request in a batch is delayed until the batch’s last request arrives, no matter how late the last request is.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Dynamic batching, on the other hand, sets a maximum time window for each batch. If the batch size is four and the window is 100 ms, the server processes the batch either when it has four requests or when 100 ms has passed, whichever happens first. It’s like a bus that leaves on a fixed schedule or when it’s full. This approach keeps latency under control, so earlier requests aren’t held up by later ones. The downside is that batches may not always be full when processed, possibly leading to wasted compute. Static batching and dynamic batching are visualized in Figure 9-15.",
      "raw_html": "<p class=\"pagebreak-before\"><a contenteditable=\"false\" data-primary=\"batching\" data-secondary=\"dynamic\" data-type=\"indexterm\" id=\"id1733\"></a><a contenteditable=\"false\" data-primary=\"dynamic batching\" data-type=\"indexterm\" id=\"id1734\"></a><em>Dynamic batching</em>, on the other hand, sets a maximum time window for each batch. If the batch size is four and the window is 100 ms, the server processes the batch either when it has four requests or when 100 ms has passed, whichever happens first. It’s like a bus that leaves on a fixed schedule or when it’s full. This approach keeps latency under control, so earlier requests aren’t held up by later ones. The downside is that batches may not always be full when processed, possibly leading to wasted compute. Static batching and dynamic batching are visualized in <a data-type=\"xref\" href=\"#ch09_figure_15_1730130962952896\">Figure 9-15</a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch09_figure_15_1730130962952896",
      "raw_html": "<figure class=\"width-95\"><div class=\"figure\" id=\"ch09_figure_15_1730130962952896\">\n<img alt=\"A screenshot of a computer\n\nDescription automatically generated\" src=\"assets/aien_0915.png\"/>\n<h6><span class=\"label\">Figure 9-15. </span>Dynamic batching keeps the latency manageable but might be less compute-efficient.</h6>\n</div></figure>",
      "image": "aien_0915.png",
      "alt": "A screenshot of a computer\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0915.png",
      "caption": {
        "label": "Figure 9-15.",
        "text": "Dynamic batching keeps the latency manageable but might be less compute-efficient."
      }
    },
    {
      "type": "paragraph",
      "content": "In naive batching implementations, all batch requests have to be completed before their responses are returned. For LLMs, some requests might take much longer than others. If one request in a batch generates only 10 response tokens and another request generates 1,000 response tokens, the short response has to wait until the long response is completed before being returned to the user. This results in unnecessary latency for short requests.",
      "raw_html": "<p>In naive batching implementations, all batch requests have to be completed before their responses are returned. For LLMs, some requests might take much longer than others. If one request in a batch generates only 10 response tokens and another request generates 1,000 response tokens, the short response has to wait until the long response is completed before being returned to the user. This results in unnecessary latency for short requests.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Continuous batching allows responses in a batch to be returned to users as soon as they are completed. It works by selectively batching operations that don’t cause the generation of one response to hold up another, as introduced in the paper Orca (Yu et al., 2022). After a request in a batch is completed and its response returned, the service can add another request into the batch in its place, making the batching continuous. It’s like a bus that, after dropping off one passenger, can immediately pick up another passenger to maximize its occupancy rate. Continuous batching, also called in-flight batching, is visualized in Figure 9-16.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"continuous batching\" data-type=\"indexterm\" id=\"id1735\"></a><a contenteditable=\"false\" data-primary=\"batching\" data-secondary=\"continuous\" data-type=\"indexterm\" id=\"id1736\"></a><em>Continuous batching</em> allows responses in a batch to be returned to users as soon as they are completed. It works by selectively batching operations that don’t cause the generation of one response to hold up another, as introduced in the paper Orca (<a href=\"https://oreil.ly/SJ7Mb\">Yu et al., 2022</a>). After a request in a batch is completed and its response returned, the service can add another request into the batch in its place, making the batching continuous. It’s like a bus that, after dropping off one passenger, can immediately pick up another passenger to maximize its occupancy rate. Continuous batching, also called <a href=\"https://oreil.ly/DlIPs\"><em>in-flight batching</em></a>, is visualized in <a data-type=\"xref\" href=\"#ch09_figure_16_1730130962952915\">Figure 9-16</a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch09_figure_16_1730130962952915",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_16_1730130962952915\">\n<img alt=\"A screenshot of a diagram\n\nDescription automatically generated\" src=\"assets/aien_0916.png\"/>\n<h6><span class=\"label\">Figure 9-16. </span>With continuous batching, completed responses can be returned immediately to users, and new requests can be processed in their place.</h6>\n</div></figure>",
      "image": "aien_0916.png",
      "alt": "A screenshot of a diagram\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0916.png",
      "caption": {
        "label": "Figure 9-16.",
        "text": "With continuous batching, completed responses can be returned immediately to users, and new requests can be processed in their place."
      }
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Decoupling prefill and decode",
      "id": "heading-7",
      "raw_html": "<h3>Decoupling prefill and decode</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "LLM inference consists of two steps: prefill and decode. Because prefill is compute-bound and decode is memory bandwidth-bound, using the same machine to perform both can cause them to inefficiently compete for resources and significantly slow down both TTFT and TPOT. Imagine a GPU that is already handling prefilling and decoding near its peak computational capacity. It might be able to handle another low computational job like decoding. However, adding a new query to this GPU means introducing a prefilling job along with a decoding job. This one prefilling job can drain computational resources from existing decoding jobs, slowing down TPOT for these requests.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"decoding\" data-secondary=\"decoupling from prefilling\" data-type=\"indexterm\" id=\"id1737\"></a><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference service optimization\" data-tertiary=\"decoupling prefill and decode\" data-type=\"indexterm\" id=\"id1738\"></a><a contenteditable=\"false\" data-primary=\"inference service optimization\" data-secondary=\"decoupling prefill and decode\" data-type=\"indexterm\" id=\"id1739\"></a><a contenteditable=\"false\" data-primary=\"prefilling, decoupling from decoding\" data-type=\"indexterm\" id=\"id1740\"></a>LLM inference consists of two steps: prefill and decode. Because prefill is compute-bound and decode is memory bandwidth-bound, using the same machine to perform both can cause them to inefficiently compete for resources and significantly slow down both TTFT and TPOT. Imagine a GPU that is already handling prefilling and decoding near its peak computational capacity. It might be able to handle another low computational job like decoding. However, adding a new query to this GPU means introducing a prefilling job along with a decoding job. This one prefilling job can drain computational resources from existing decoding jobs, slowing down TPOT for these requests.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "One common optimization technique for inference servers is to disaggregate prefill and decode. “DistServe” (Zhong et al., 2024) and “Inference Without Interference” (Hu et al., 2024) show that for various popular LLMs and applications, assigning prefill and decode operations to different instances (e.g., different GPUs) can significantly improve the volume of processed requests while adhering to latency requirements. Even though decoupling requires transferring intermediate states from prefill instances to decode instances, the paper shows communication overhead is not substantial in modern GPU clusters with high-bandwidth connections such as NVLink within a node.",
      "raw_html": "<p>One common optimization technique for inference servers is to disaggregate prefill and decode. “DistServe” (<a href=\"https://arxiv.org/html/2401.09670v1\">Zhong et al., 2024</a>) and “Inference Without Interference” (<a href=\"https://arxiv.org/abs/2401.11181\">Hu et al., 2024</a>) show that for various popular LLMs and applications, assigning prefill and decode operations to different instances (e.g., different GPUs) can significantly improve the volume of processed requests while adhering to latency requirements. Even though decoupling requires transferring intermediate states from prefill instances to decode instances, the paper shows communication overhead is not substantial in modern GPU clusters with high-bandwidth connections such as <a href=\"https://en.wikipedia.org/wiki/NVLink\">NVLink</a> within a node.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The ratio of prefill instances to decode instances depends on many factors, such as the workload characteristics (e.g., longer input lengths require more prefill compute) and latency requirements (e.g., whether you want lower TTFT or TPOT). For example, if input sequences are usually long and you want to prioritize TTFT, this ratio can be between 2:1 and 4:1. If input sequences are short and you want to prioritize TPOT, this ratio can be 1:2 to 1:1.25",
      "raw_html": "<p class=\"pagebreak-before\">The ratio of prefill instances to decode instances depends on many factors, such as the workload characteristics (e.g., longer input lengths require more prefill compute) and latency requirements (e.g., whether you want lower TTFT or TPOT). For example, if input sequences are usually long and you want to prioritize TTFT, this ratio can be between 2:1 and 4:1. If input sequences are short and you want to prioritize TPOT, this ratio can be 1:2 to 1:1.<sup><a data-type=\"noteref\" href=\"ch09.html#id1741\" id=\"id1741-marker\">25</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Prompt caching",
      "id": "heading-7",
      "raw_html": "<h3>Prompt caching</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Many prompts in an application have overlapping text segments. A prompt cache stores these overlapping segments for reuse, so you only need to process them once. A common overlapping text segment in different prompts is the system prompt. Without a prompt cache, your model needs to process the system prompt with every query. With a prompt cache, the system prompt needs to be processed just once for the first query.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference service optimization\" data-tertiary=\"prompt caching\" data-type=\"indexterm\" id=\"ch09.html57\"></a><a contenteditable=\"false\" data-primary=\"inference service optimization\" data-secondary=\"prompt caching\" data-type=\"indexterm\" id=\"ch09.html58\"></a><a contenteditable=\"false\" data-primary=\"prompt caching\" data-type=\"indexterm\" id=\"ch09.html59\"></a>Many prompts in an application have overlapping text segments. A prompt cache stores these overlapping segments for reuse, so you only need to process them once. A common overlapping text segment in different prompts is the system prompt. Without a prompt cache, your model needs to process the system prompt with every query. With a prompt cache, the system prompt needs to be processed just once for the first query.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Prompt caching is useful for queries that involve long documents. For example, if many of your user queries are related to the same long document (such as a book or a codebase), this long document can be cached for reuse across queries. It’s also useful for long conversations when the processing of earlier messages can be cached and reused when predicting future messages.",
      "raw_html": "<p>Prompt caching is useful for queries that involve long documents. For example, if many of your user queries are related to the same long document (such as a book or a codebase), this long document can be cached for reuse across queries. It’s also useful for long conversations when the processing of earlier messages can be cached and reused when predicting future messages.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "A prompt cache is visualized in Figure 9-17. It’s also called a context cache or prefix cache.",
      "raw_html": "<p>A prompt cache is visualized in <a data-type=\"xref\" href=\"#ch09_figure_17_1730130962952933\">Figure 9-17</a>. It’s also called a context cache or prefix cache.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_17_1730130962952933",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_17_1730130962952933\">\n<img alt=\"A screenshot of a computer\n\nDescription automatically generated\" src=\"assets/aien_0917.png\"/>\n<h6><span class=\"label\">Figure 9-17. </span>With a prompt cache, overlapping segments in different prompts can be cached and reused.</h6>\n</div></figure>",
      "image": "aien_0917.png",
      "alt": "A screenshot of a computer\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0917.png",
      "caption": {
        "label": "Figure 9-17.",
        "text": "With a prompt cache, overlapping segments in different prompts can be cached and reused."
      }
    },
    {
      "type": "paragraph",
      "content": "For applications with long system prompts, prompt caching can significantly reduce both latency and cost. If your system prompt is 1,000 tokens, and your application generates one million model API calls daily, a prompt cache will save you from processing approximately one billion repetitive input tokens a day! However, this isn’t entirely free. Like the KV cache, prompt cache size can be quite large and take up memory space. Unless you use a model API with this functionality, implementing prompt caching can require significant engineering effort.",
      "raw_html": "<p class=\"pagebreak-before\">For applications with long system prompts, prompt caching can significantly reduce both latency and cost. If your system prompt is 1,000 tokens, and your application generates one million model API calls daily, a prompt cache will save you from processing approximately one billion repetitive input tokens a day! However, this isn’t entirely free. Like the KV cache, prompt cache size can be quite large and take up memory space. Unless you use a model API with this functionality, implementing prompt caching can require significant engineering effort.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Since its introduction in November 2023 by Gim et al., the prompt cache has been rapidly incorporated into model APIs. As of this writing, Google Gemini offers this functionality, with cached input tokens given a 75% discount compared to regular input tokens, but you’ll have to pay extra for cache storage (as of writing, $1.00/one million tokens per hour). Anthropic offers prompt caching that promises up to 90% cost savings (the longer the cached context, the higher the savings) and up to 75% latency reduction. The impact of prompt caching on the cost and latency of different scenarios is shown in Table 9-3.26",
      "raw_html": "<p>Since its introduction in November 2023 by <a href=\"https://oreil.ly/Pd6Pk\">Gim et al.</a>, the prompt cache has been rapidly incorporated into model APIs. As of this writing, <a contenteditable=\"false\" data-primary=\"Gemini\" data-type=\"indexterm\" id=\"id1742\"></a>Google Gemini offers this <a href=\"https://oreil.ly/pIHkL\">functionality</a>, with cached input tokens given a 75% discount compared to regular input tokens, but you’ll have to pay extra for cache storage (as of writing, $1.00/one million tokens per hour). <a contenteditable=\"false\" data-primary=\"Anthropic\" data-secondary=\"prompt caching\" data-type=\"indexterm\" id=\"id1743\"></a>Anthropic offers <a href=\"https://oreil.ly/8rtsF\">prompt caching</a> that promises up to 90% cost savings (the longer the cached context, the higher the savings) and up to 75% latency reduction. The impact of prompt caching on the cost and latency of different scenarios is shown <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html59\" data-type=\"indexterm\" id=\"id1744\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html58\" data-type=\"indexterm\" id=\"id1745\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html57\" data-type=\"indexterm\" id=\"id1746\"></a>in <a data-type=\"xref\" href=\"#ch09_table_3_1730130962971081\">Table 9-3</a>.<sup><a data-type=\"noteref\" href=\"ch09.html#id1747\" id=\"id1747-marker\">26</a></sup></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "table",
      "id": "ch09_table_3_1730130962971081",
      "raw_html": "<table id=\"ch09_table_3_1730130962971081\">\n<caption><span class=\"label\">Table 9-3. </span>Cost and latency reduced by prompt caching. Information from Anthropic (2024).</caption>\n<thead>\n<tr>\n<th>Use case</th>\n<th>Latency w/o caching <span class=\"keep-together\">(time to first token)</span></th>\n<th>Latency with caching <span class=\"keep-together\">(time to first token)</span></th>\n<th>Cost reduction</th>\n</tr>\n</thead>\n<tr>\n<td>Chat with a book (100,000-token cached prompt)</td>\n<td>11.5 s</td>\n<td>2.4 s (–79%)</td>\n<td>–90%</td>\n</tr>\n<tr>\n<td>Many-shot prompting (10,000-token prompt)</td>\n<td>1.6 s</td>\n<td>1.1 s (–31%)</td>\n<td>–86%</td>\n</tr>\n<tr>\n<td>Multi-turn conversation (10-turn convo with a long system prompt)</td>\n<td>~10 s</td>\n<td>~2.5 s (–75%) </td>\n<td>–53%</td>\n</tr>\n</table>",
      "caption": {
        "label": "Table 9-3.",
        "text": "Cost and latency reduced by prompt caching. Information from Anthropic (2024)."
      },
      "headers": [
        "Use case",
        "Latency w/o caching (time to first token)",
        "Latency with caching (time to first token)",
        "Cost reduction"
      ],
      "rows": [
        [
          "Chat with a book (100,000-token cached prompt)",
          "11.5 s",
          "2.4 s (–79%)",
          "–90%"
        ],
        [
          "Many-shot prompting (10,000-token prompt)",
          "1.6 s",
          "1.1 s (–31%)",
          "–86%"
        ],
        [
          "Multi-turn conversation (10-turn convo with a long system prompt)",
          "~10 s",
          "~2.5 s (–75%)",
          "–53%"
        ]
      ],
      "row_count": 3,
      "column_count": 4
    },
    {
      "type": "heading",
      "level": 3,
      "content": "Parallelism",
      "id": "heading-7",
      "raw_html": "<h3>Parallelism</h3>",
      "section_type": "sect3"
    },
    {
      "type": "paragraph",
      "content": "Accelerators are designed for parallel processing, and parallelism strategies are the backbone of high-performance computing. Many new parallelization strategies are being developed. This section covers only a few of them for reference. Two families of parallelization strategies that can be applied across all models are data parallelism and model parallelism. A family of strategies applied specifically for LLMs is context and sequence parallelism. An optimization technique might involve multiple parallelism strategies.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"inference optimization\" data-secondary=\"inference service optimization\" data-tertiary=\"parallelism\" data-type=\"indexterm\" id=\"ch09.html60\"></a><a contenteditable=\"false\" data-primary=\"inference service optimization\" data-secondary=\"parallelism\" data-type=\"indexterm\" id=\"ch09.html61\"></a><a contenteditable=\"false\" data-primary=\"parallelism\" data-type=\"indexterm\" id=\"ch09.html62\"></a>Accelerators are designed for parallel processing, and parallelism strategies are the backbone of high-performance computing. Many new parallelization strategies are being developed. This section covers only a few of them for reference. Two families of parallelization strategies that can be applied across all models are data parallelism and model parallelism. A family of strategies applied specifically for LLMs is context and sequence parallelism. An optimization technique might involve multiple parallelism strategies.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Replica parallelism is the most straightforward strategy to implement. It simply creates multiple replicas of the model you want to serve.27 More replicas allow you to handle more requests at the same time, potentially at the cost of using more chips. Trying to fit models of different sizes onto different chips is a bin-packing problem, which can get complicated with more models, more replicas, and more chips.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"replica parallelism\" data-type=\"indexterm\" id=\"id1748\"></a><em>Replica parallelism</em> is the most straightforward strategy to implement. It simply creates multiple replicas of the model you want to serve.<sup><a data-type=\"noteref\" href=\"ch09.html#id1749\" id=\"id1749-marker\">27</a></sup> More replicas allow you to handle more requests at the same time, potentially at the cost of using more chips. Trying to fit models of different sizes onto different chips is a bin-packing problem, which can get complicated with more models, more replicas, and more chips.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Let’s say you have a mixture of models of different sizes (e.g., 8B, 13B, 34B, and 70B parameters) and access to GPUs of different memory capabilities (e.g., 24 GB, 40 GB, 48 GB, and 80 GB). For simplicity, assume that all models are in the same precision, 8 bits:",
      "raw_html": "<p>Let’s say you have a mixture of models of different sizes (e.g., 8B, 13B, 34B, and 70B parameters) and access to GPUs of different memory capabilities (e.g., 24 GB, 40 GB, 48 GB, and 80 GB). For simplicity, assume that all models are in the same precision, 8 bits:</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "list",
      "list_type": "unordered",
      "items": [
        "If you have a fixed number of chips, you need to decide how many replicas to create for each model and what GPUs to use for each replica to maximize your metrics. For example, should you place three 13B models on a 40 GB GPU, or should you reserve this GPU for one 34B model?",
        "If you have a fixed number of model replicas, you need to decide what chips to acquire to minimize the cost. This situation, however, rarely occurs."
      ],
      "raw_html": "<ul>\n<li><p>If you have a fixed number of chips, you need to decide how many replicas to create for each model and what GPUs to use for each replica to maximize your metrics. For example, should you place three 13B models on a 40 GB GPU, or should you reserve this GPU for one 34B model?</p></li>\n<li><p>If you have a fixed number of model replicas, you need to decide what chips to acquire to minimize the cost. This situation, however, rarely occurs.</p></li>\n</ul>"
    },
    {
      "type": "paragraph",
      "content": "If you have a fixed number of chips, you need to decide how many replicas to create for each model and what GPUs to use for each replica to maximize your metrics. For example, should you place three 13B models on a 40 GB GPU, or should you reserve this GPU for one 34B model?",
      "raw_html": "<p>If you have a fixed number of chips, you need to decide how many replicas to create for each model and what GPUs to use for each replica to maximize your metrics. For example, should you place three 13B models on a 40 GB GPU, or should you reserve this GPU for one 34B model?</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "If you have a fixed number of model replicas, you need to decide what chips to acquire to minimize the cost. This situation, however, rarely occurs.",
      "raw_html": "<p>If you have a fixed number of model replicas, you need to decide what chips to acquire to minimize the cost. This situation, however, rarely occurs.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Often, your model is so big that it can’t fit into one machine. Model parallelism refers to the practice of splitting the same model across multiple machines. Fitting models onto chips can become an even more complicated problem with model parallelism.",
      "raw_html": "<p>Often, your model is so big that it can’t fit into one machine. <em>Model parallelism</em> refers to the practice of splitting the same model across multiple machines. Fitting models onto chips can become an even more complicated problem with model parallelism.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "There are several ways to split a model. The most common approach for inference is tensor parallelism, also known as intra-operator parallelism. Inference involves a sequence of operators on multidimensional tensors, such as matrix multiplication. In this approach, tensors involved in an operator are partitioned across multiple devices, effectively breaking up this operator into smaller pieces to be executed in parallel, thus speeding up the computation. For example, when multiplying two matrices, you can split one of the matrices columnwise, as shown in Figure 9-18.",
      "raw_html": "<p>There are several ways to split a model. The most common approach for inference is <em>tensor parallelism</em>, also known as <em>intra-operator parallelism</em>. Inference involves a sequence of operators on multidimensional tensors, such as matrix multiplication. In this approach, tensors involved in an operator are partitioned across multiple devices, effectively breaking up this operator into smaller pieces to be executed in parallel, thus speeding up the computation. For example, when multiplying two matrices, you can split one of the matrices columnwise, as shown in <a data-type=\"xref\" href=\"#ch09_figure_18_1730130962952949\">Figure 9-18</a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "Tensor parallelism provides two benefits. First, it makes it possible to serve large models that don’t fit on single machines. Second, it reduces latency. The latency benefit, however, might be reduced due to extra communication overhead.",
      "raw_html": "<p>Tensor parallelism provides two benefits. First, it makes it possible to serve large models that don’t fit on single machines. Second, it reduces latency. The latency benefit, however, might be reduced due to extra communication overhead.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "figure",
      "id": "ch09_figure_18_1730130962952949",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_18_1730130962952949\">\n<img alt=\"A diagram of a grid with squares and a few squares\n\nDescription automatically generated with medium confidence\" src=\"assets/aien_0918.png\"/>\n<h6><span class=\"label\">Figure 9-18. </span>Tensor parallelism for matrix multiplication.</h6>\n</div></figure>",
      "image": "aien_0918.png",
      "alt": "A diagram of a grid with squares and a few squares\n\nDescription automatically generated with medium confidence",
      "image_src_original": "assets/aien_0918.png",
      "caption": {
        "label": "Figure 9-18.",
        "text": "Tensor parallelism for matrix multiplication."
      }
    },
    {
      "type": "paragraph",
      "content": "Another way to split a model is pipeline parallelism, which involves dividing a model’s computation into distinct stages and assigning each stage to a different device. As data flows through the model, each stage processes one part while others process subsequent parts, enabling overlapping computations. Figure 9-19 shows what pipeline parallelism looks like on four machines.",
      "raw_html": "<p>Another way to split a model is <em>pipeline parallelism</em>, which involves dividing a model’s computation into distinct stages and assigning each stage to a different device. As data flows through the model, each stage processes one part while others process subsequent parts, enabling overlapping computations. <a data-type=\"xref\" href=\"#ch09_figure_19_1730130962952966\">Figure 9-19</a> shows what pipeline parallelism looks like on four machines.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "figure",
      "id": "ch09_figure_19_1730130962952966",
      "raw_html": "<figure><div class=\"figure\" id=\"ch09_figure_19_1730130962952966\">\n<img alt=\"A diagram of a layer\n\nDescription automatically generated\" src=\"assets/aien_0919.png\"/>\n<h6><span class=\"label\">Figure 9-19. </span>Pipeline parallelism enables model splits to be executed in parallel. </h6>\n</div></figure>",
      "image": "aien_0919.png",
      "alt": "A diagram of a layer\n\nDescription automatically generated",
      "image_src_original": "assets/aien_0919.png",
      "caption": {
        "label": "Figure 9-19.",
        "text": "Pipeline parallelism enables model splits to be executed in parallel."
      }
    },
    {
      "type": "paragraph",
      "content": "Figure 9-19 shows a batch can be split into smaller micro-batches. After a micro-batch is processed on one machine, its output is passed onto the next part of the model on the next machine.",
      "raw_html": "<p><a data-type=\"xref\" href=\"#ch09_figure_19_1730130962952966\">Figure 9-19</a> shows a batch can be split into smaller micro-batches. After a micro-batch is processed on one machine, its output is passed onto the next part of the model on the next machine.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "While pipeline parallelism enables serving large models on multiple machines, it increases the total latency for each request due to extra communication between pipeline stages. Therefore, for applications with strict latency requirements, pipeline parallelism is typically avoided in favor of replica parallelism. However, pipeline parallelism is commonly used in training since it can help increase throughput.",
      "raw_html": "<p>While pipeline parallelism enables serving large models on multiple machines, it increases the total latency for each request due to extra communication between pipeline stages. Therefore, for applications with strict latency requirements, pipeline parallelism is typically avoided in favor of replica parallelism. However, pipeline parallelism is commonly used in training since it can help increase throughput.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Two techniques that are less common but might warrant a quick mention to illustrate the diversity of techniques are context parallelism and sequence parallelism. They were both developed to make long input sequence processing more efficient, including context parallelism and sequence parallelism.",
      "raw_html": "<p>Two techniques that are less common but might warrant a quick mention to illustrate the diversity of techniques are <em>context parallelism</em> and <em>sequence parallelism</em>. They were both developed to make long input sequence processing more efficient, including context parallelism and sequence parallelism.</p>",
      "contains_links": false,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "In context parallelism, the input sequence itself is split across different devices to be processed separately. For example, the first half of the input is processed on machine 1 and the second half on machine 2.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"context parallelism\" data-type=\"indexterm\" id=\"id1750\"></a>In <a href=\"https://oreil.ly/On2-B\"><em>context parallelism</em></a>, the input sequence itself is split across different devices to be processed separately. For example, the first half of the input is processed on machine 1 and the second half on machine 2.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "In sequence parallelism, operators needed for the entire input are split across machines. For example, if the input requires both attention and feedforward computation, attention might be processed on machine 1 while feedforward is processed on machine 2.",
      "raw_html": "<p><a contenteditable=\"false\" data-primary=\"sequence parallelism\" data-type=\"indexterm\" id=\"id1751\"></a>In <em>sequence parallelism</em>, operators needed for the entire input are split across machines. <a contenteditable=\"false\" data-primary=\"feedforward computation\" data-type=\"indexterm\" id=\"id1752\"></a>For example, if the input requires both attention and feedforward computation, attention might be processed on machine 1 while feedforward is processed on machine <a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html62\" data-type=\"indexterm\" id=\"id1753\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html61\" data-type=\"indexterm\" id=\"id1754\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html60\" data-type=\"indexterm\" id=\"id1755\"></a>2<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html56\" data-type=\"indexterm\" id=\"id1756\"></a><a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html55\" data-type=\"indexterm\" id=\"id1757\"></a>.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "heading",
      "level": 1,
      "content": "Summary",
      "id": "heading-7",
      "raw_html": "<h1>Summary</h1>",
      "section_type": "sect1"
    },
    {
      "type": "paragraph",
      "content": "A model’s usability depends heavily on its inference cost and latency. Cheaper inference makes AI-powered decisions more affordable, while faster inference enables the integration of AI into more applications. Given the massive potential impact of inference optimization, it has attracted many talented individuals who continually come up with innovative approaches.",
      "raw_html": "<p>A model’s usability depends heavily on its inference cost and latency. Cheaper inference makes AI-powered decisions more affordable, while faster inference enables the integration of AI into more applications. Given the massive potential impact of inference optimization, it has attracted many talented individuals who continually come up with innovative approaches.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Before we start making things more efficient, we need to understand how efficiency is measured. This chapter started with common efficiency metrics for latency, throughput, and utilization. For language model-based inference, latency can be broken into time to first token (TTFT), which is influenced by the prefilling phase, and time per output token (TPOT), which is influenced by the decoding phase. Throughput metrics are directly related to cost. There’s a trade-off between latency and throughput. You can potentially reduce cost if you’re okay with increased latency, and reducing latency often involves increasing cost.",
      "raw_html": "<p>Before we start making things more efficient, we need to understand how efficiency is measured. This chapter started with common efficiency metrics for latency, throughput, and utilization. For language model-based inference, latency can be broken into time to first token (TTFT), which is influenced by the prefilling phase, and time per output token (TPOT), which is influenced by the decoding phase. Throughput metrics are directly related to cost. There’s a trade-off between latency and throughput. You can potentially reduce cost if you’re okay with increased latency, and reducing latency often involves increasing cost.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "How efficiently a model can run depends on the hardware it is run on. For this reason, this chapter also provided a quick overview of AI hardware and what it takes to optimize models on different accelerators.",
      "raw_html": "<p>How efficiently a model can run depends on the hardware it is run on. For this reason, this chapter also provided a quick overview of AI hardware and what it takes to optimize models on different accelerators.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The chapter then continued with different techniques for inference optimization. Given the availability of model APIs, most application developers will use these APIs with their built-in optimization instead of implementing these techniques themselves. While these techniques might not be relevant to all application developers, I believe that understanding what techniques are possible can be helpful for evaluating the efficiency of model APIs.",
      "raw_html": "<p>The chapter then continued with different techniques for inference optimization. Given the availability of model APIs, most application developers will use these APIs with their built-in optimization instead of implementing these techniques themselves. While these techniques might not be relevant to all application developers, I believe that understanding what techniques are possible can be helpful for evaluating the efficiency of model APIs.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "This chapter also focused on optimization at the model level and the inference service level. Model-level optimization often requires changing the model itself, which can lead to changes in the model behaviors. Inference service-level optimization, on the other hand, typically keeps the model intact and only changes how it’s served.",
      "raw_html": "<p>This chapter also focused on optimization at the model level and the inference service level. Model-level optimization often requires changing the model itself, which can lead to changes in the model behaviors. Inference service-level optimization, on the other hand, typically keeps the model intact and only changes how it’s served.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Model-level techniques include model-agnostic techniques like quantization and distillation. Different model architectures require their own optimization. For example, because a key bottleneck of transformer models is in the attention mechanism, many optimization techniques involve making attention more efficient, including KV cache management and writing attention kernels. A big bottleneck for an autoregressive language model is in its autoregressive decoding process, and consequently, many techniques have been developed to address it, too.",
      "raw_html": "<p>Model-level techniques include model-agnostic techniques like quantization and distillation. Different model architectures require their own optimization. For example, because a key bottleneck of transformer models is in the attention mechanism, many optimization techniques involve making attention more efficient, including KV cache management and writing attention kernels. A big bottleneck for an autoregressive language model is in its autoregressive decoding process, and consequently, many techniques have been developed to address it, too.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Inference service-level techniques include various batching and parallelism strategies. There are also techniques developed especially for autoregressive language models, including prefilling/decoding decoupling and prompt caching.",
      "raw_html": "<p>Inference service-level techniques include various batching and parallelism strategies. There are also techniques developed especially for autoregressive language models, including prefilling/decoding decoupling and prompt caching.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "The choice of optimization techniques depends on your workloads. For example, KV caching is significantly more important for workloads with long contexts than those with short contexts. Prompt caching, on the other hand, is crucial for workloads involving long, overlapping prompt segments or multi-turn conversations. The choice also depends on your performance requirements. For instance, if low latency is a higher priority than cost, you might want to scale up replica parallelism. While more replicas require additional machines, each machine handles fewer requests, allowing it to allocate more resources per request and, thus, improve response time.",
      "raw_html": "<p>The choice of optimization techniques depends on your workloads. For example, KV caching is significantly more important for workloads with long contexts than those with short contexts. Prompt caching, on the other hand, is crucial for workloads involving long, overlapping prompt segments or multi-turn conversations. The choice also depends on your performance requirements. For instance, if low latency is a higher priority than cost, you might want to scale up replica parallelism. While more replicas require additional machines, each machine handles fewer requests, allowing it to allocate more resources per request and, thus, improve response time.</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "However, across various use cases, the most impactful techniques are typically quantization (which generally works well across models), tensor parallelism (which both reduces latency and enables serving larger models), replica parallelism (which is relatively straightforward to implement), and attention mechanism optimization (which can significantly accelerate transformer models).",
      "raw_html": "<p>However, across various use cases, the most impactful techniques are typically quantization (which generally works well across models), tensor parallelism (which both reduces latency and enables serving larger models), replica parallelism (which is relatively straightforward to implement), and attention mechanism optimization (which can significantly accelerate transformer models).</p>",
      "contains_links": false,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "Inference optimization concludes the list of model adaptation techniques covered in this book. The next chapter will explore how to integrate these techniques into a cohesive system.",
      "raw_html": "<p>Inference optimization concludes the list of model adaptation techniques covered in this book. The next chapter will explore how to integrate these techniques into a cohesive system.<a contenteditable=\"false\" data-primary=\"\" data-startref=\"ch09.html0\" data-type=\"indexterm\" id=\"id1758\"></a></p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "1 As discussed in Chapter 7, inference involves the forward pass while training involves both the forward and backward passes.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1597\"><sup><a href=\"ch09.html#id1597-marker\">1</a></sup> As discussed in <a data-type=\"xref\" href=\"ch07.html#ch07\">Chapter 7</a>, inference involves the forward pass while training involves both the forward and backward passes.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "2 A friend, Mark Saroufim, pointed me to an interesting relationship between a model’s training cost and inference cost. Imagine you’re a model provider. Let T be the total training cost, p be the cost you’re charging per inference, and N be the number of inference calls you can sell. Developing a model only makes sense if the money you can recover from inference for a model is more than its training cost, i.e., T <= p × N. The more a model is used in production, the more model providers can reduce inference cost. However, this doesn’t apply for third-party API providers who sell inference calls on top of open source models.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1598\"><sup><a href=\"ch09.html#id1598-marker\">2</a></sup> A friend, Mark Saroufim, pointed me to an interesting relationship between a model’s training cost and inference cost. Imagine you’re a model provider. Let <em>T</em> be the total training cost, <em>p</em> be the cost you’re charging per inference, and <em>N</em> be the number of inference calls you can sell. Developing a model only makes sense if the money you can recover from inference for a model is more than its training cost, i.e., <em>T</em> &lt;= <em>p</em> × <em>N</em>. The more a model is used in production, the more model providers can reduce inference cost. However, this doesn’t apply for third-party API providers who sell inference calls on top of open source models.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "3 Anecdotally, I find that people coming from a system background (e.g., optimization engineers and GPU engineers) use memory-bound to refer to bandwidth-bound, and people coming from an AI background (e.g., ML and AI engineers) use to memory-bound to refer to memory capacity-bound.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1605\"><sup><a href=\"ch09.html#id1605-marker\">3</a></sup> Anecdotally, I find that people coming from a system background (e.g., optimization engineers and GPU engineers) use <em>memory-bound</em> to refer to <em>bandwidth-bound</em>, and people coming from an AI background (e.g., ML and AI engineers) use to memory-bound to refer to memory capacity-bound.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "4 The Roofline paper uses the term memory-bound to refer to memory-bandwidth bound.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1606\"><sup><a href=\"ch09.html#id1606-marker\">4</a></sup> The Roofline paper uses the term memory-bound to refer to memory-bandwidth bound.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "5 Prefilling effectively populates the initial KV cache for the transformer model.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1607\"><sup><a href=\"ch09.html#id1607-marker\">5</a></sup> Prefilling effectively populates the initial KV cache for the transformer model.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "6 If you run an inference service, separating your inference APIs into online and batch can help you prioritize latency for requests where latency matters the most. Let’s say that your inference server can serve only a maximum of X requests/second without latency degradation, you have to serve Y requests/second, and Y is larger than X. In an ideal world, users with less-urgent requests can send their requests to the batch API, so that your service can focus on processing the online API requests first.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1612\"><sup><a href=\"ch09.html#id1612-marker\">6</a></sup> If you run an inference service, separating your inference APIs into online and batch can help you prioritize latency for requests where latency matters the most. Let’s say that your inference server can serve only a maximum of X requests/second without latency degradation, you have to serve Y requests/second, and Y is larger than X. In an ideal world, users with less-urgent requests can send their requests to the batch API, so that your service can focus on processing the online API requests first.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "7 As discussed in “Prompt caching”, it’s common to know in advance the system prompt of an application. It’s just the exact user queries that are hard to predict.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1619\"><sup><a href=\"ch09.html#id1619-marker\">7</a></sup> As discussed in <a data-type=\"xref\" href=\"#ch09_prompt_caching_1730130963008914\">“Prompt caching”</a>, it’s common to know in advance the system prompt of an application. It’s just the exact user queries that are hard to predict. </p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "8 In the early days of chatbots, some people complained about chatbots responding too fast, which seemed unnatural. See “Lufthansa Delays Chatbot’s Responses to Make It More ‘Human’” (Ry Crozier, iTnews, May 2017). However, as people become more familiar with chatbots, this is no longer the case.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1620\"><sup><a href=\"ch09.html#id1620-marker\">8</a></sup> In the early days of chatbots, some people complained about chatbots responding too fast, which seemed unnatural. See <a href=\"https://oreil.ly/jD5Pj\">“Lufthansa Delays Chatbot’s Responses to Make It More ‘Human’”</a> (Ry Crozier, iTnews, May 2017). However, as people become more familiar with chatbots, this is no longer the case.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "9 Time between tokens (TBT) is used by LinkedIn and inter-token latency (ITL) is used by NVIDIA.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1623\"><sup><a href=\"ch09.html#id1623-marker\">9</a></sup> Time between tokens (TBT) is used by <a href=\"https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US\">LinkedIn</a> and inter-token latency (ITL) is used by <a href=\"https://oreil.ly/zHsb8\">NVIDIA</a>.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "10 An experiment by Anyscale shows that 100 input tokens have approximately the same impact on the overall latency as a single output token.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1624\"><sup><a href=\"ch09.html#id1624-marker\">10</a></sup> An experiment by Anyscale shows that 100 input tokens have approximately the same impact on the overall latency as a single output token.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "11 People have cared about FLOP/s utilization for a long time, but the term MFU was introduced in the PaLM paper (Chowdhery et al., 2022).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1637\"><sup><a href=\"ch09.html#id1637-marker\">11</a></sup> People have cared about FLOP/s utilization for a long time, but the term MFU was introduced in the PaLM paper (<a href=\"https://arxiv.org/abs/2204.02311\">Chowdhery et al., 2022</a>).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "12 Chip makers might also be doing what I call peak FLOP/s hacking. This might run experiments in certain conditions, such as using sparse matrices with specific shapes, to increase their peak FLOP/s. Higher peak FLOP/s numbers make their chips more attractive, but it can be harder for users to achieve high MFU.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1638\"><sup><a href=\"ch09.html#id1638-marker\">12</a></sup> Chip makers might also be doing what I call <em>peak FLOP/s hacking</em>. This might run experiments in certain conditions, such as using sparse matrices with specific shapes, to increase their peak FLOP/s. Higher peak FLOP/s numbers make their chips more attractive, but it can be harder for users to achieve high MFU.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "13 In the 1960s, computers could run only one-layer neural networks, which had very limited capabilities. In their famous 1969 book Perceptrons: An Introduction to Computational Geometry (MIT Press), two AI pioneers, Marvin Minsky and Seymour Papert, argued that neural networks with hidden layers would still be able to do little. Their exact quote was: “Virtually nothing is known about the computational capabilities of this latter kind of machine. We believe that it can do little more than can a low order perceptron.” There wasn’t sufficient compute power to dispute their argument, which was then cited by many people as a key reason for the drying up of AI funding in the 1970s.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1649\"><sup><a href=\"ch09.html#id1649-marker\">13</a></sup> In the 1960s, computers could run only one-layer neural networks, which had very limited capabilities. In their famous 1969 book <a href=\"https://en.wikipedia.org/wiki/Perceptrons_(book)\"><em>Perceptrons: An Introduction to Computational Geometry</em></a> (MIT Press), two AI pioneers, Marvin Minsky and Seymour Papert, argued that neural networks with hidden layers would still be able to do little. Their exact quote was: “Virtually nothing is known about the computational capabilities of this latter kind of machine. We believe that it can do little more than can a low order perceptron<em>.</em>” There wasn’t sufficient compute power to dispute their argument, which was then cited by many people as a key reason for the drying up of AI funding in the 1970s.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "14 There have been discussions on whether to rename the GPU since it’s used for a lot more than graphics (Jon Peddie, “Chasing Pixels,” July 2018). Jensen Huang, NVIDIA’s CEO, said in an interview (Stratechery, March 2022) that once the GPU took off and they added more capabilities to it, they considered renaming it to something more general like GPGPU (general-purpose GPU) or XGU. They decided against renaming because they assumed that people who buy GPUs will be smart enough to know what a GPU is good for beyond its name.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1650\"><sup><a href=\"ch09.html#id1650-marker\">14</a></sup> There have been discussions on whether to <a href=\"https://oreil.ly/mRNCP\">rename the GPU</a> since it’s used for a lot more than graphics (Jon Peddie, “Chasing Pixels,” July 2018). Jensen Huang, NVIDIA’s CEO, said in an <a href=\"https://oreil.ly/iK0tN\">interview</a> (<em>Stratechery</em>, March 2022) that once the GPU took off and they added more capabilities to it, they considered renaming it to something more general like GPGPU (general-purpose GPU) or XGU. They decided against renaming because they assumed that people who buy GPUs will be smart enough to know what a GPU is good for beyond its name.</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "15 Matrix multiplication, affectionately known as matmul, is estimated to account for more than 90% of all floating point operations in a neural network, according to “Data Movement Is All You Need: A Case Study on Optimizing Transformers” (Ivanov et al., arXiv, v3, November 2021) and “Scalable MatMul-free Language Modeling” (Zhu et al., arXiv, June 2024).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1651\"><sup><a href=\"ch09.html#id1651-marker\">15</a></sup> Matrix multiplication, affectionately known as matmul, is estimated to account for more than 90% of all floating point operations in a neural network, according to <a href=\"https://arxiv.org/abs/2007.00072\">“Data Movement Is All You Need: A Case Study on Optimizing Transformers”</a> (Ivanov et al., <em>arXiv</em>, v3, November 2021) and <a href=\"https://arxiv.org/abs/1802.04799\">“Scalable MatMul-free Language Modeling”</a> (Zhu et al., <em>arXiv</em>, June 2024).</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "16 While a chip can be developed to run one model architecture, a model architecture can be developed to make the most out of a chip, too. For example, the transformer was originally designed by Google to run fast on TPUs and only later optimized on GPUs.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1652\"><sup><a href=\"ch09.html#id1652-marker\">16</a></sup> While a chip can be developed to run one model architecture, a model architecture can be developed to make the most out of a chip, too. For example, the transformer was originally designed by Google to <a href=\"https://oreil.ly/y45q6\">run fast on TPUs</a> and only later optimized on GPUs.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "17 Lower-end to mid-range GPUs might use GDDR (Graphics Double Data Rate) memory.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1661\"><sup><a href=\"ch09.html#id1661-marker\">17</a></sup> Lower-end to mid-range GPUs might use <a href=\"https://en.wikipedia.org/wiki/GDDR_SDRAM\">GDDR</a> (Graphics Double Data Rate) memory.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "18 A main challenge in building data centers with tens of thousands of GPUs is finding a location that can guarantee the necessary electricity. Building large-scale data centers requires navigating electricity supply, speed, and geopolitical constraints. For example, remote regions might provide cheaper electricity but can increase network latency, making the data centers less appealing for use cases with stringent latency requirements like inference.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1669\"><sup><a href=\"ch09.html#id1669-marker\">18</a></sup> A main challenge in building data centers with tens of thousands of GPUs is finding a location that can guarantee the necessary electricity. Building large-scale data centers requires navigating electricity supply, speed, and geopolitical constraints. For example, remote regions might provide cheaper electricity but can increase network latency, making the data centers less appealing for use cases with stringent latency requirements like inference.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "19 Each token generation step necessitates the transfer of the entire model’s parameters from the accelerator’s high-bandwidth memory to its compute units. This makes this operation bandwidth-heavy. Because the model can produce only one token at a time, the process consumes only a small number of FLOP/s, resulting in computational inefficiency.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1683\"><sup><a href=\"ch09.html#id1683-marker\">19</a></sup> Each token generation step necessitates the transfer of the entire model’s parameters from the accelerator’s high-bandwidth memory to its compute units. This makes this operation bandwidth-heavy. Because the model can produce only one token at a time, the process consumes only a small number of FLOP/s, resulting in computational inefficiency.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "20 This also means that if your MFU is already maxed out, speculative decoding makes less sense.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1684\"><sup><a href=\"ch09.html#id1684-marker\">20</a></sup> This also means that if your MFU is already maxed out, speculative decoding makes less sense.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "21 The Jacobi method is an iterative algorithm where multiple parts of a solution can be updated simultaneously and independently.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1694\"><sup><a href=\"ch09.html#id1694-marker\">21</a></sup> The Jacobi method is an iterative algorithm where multiple parts of a solution can be updated simultaneously and independently.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "22 The number of attention computations for an autoregressive model is O(n2).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1700\"><sup><a href=\"ch09.html#id1700-marker\">22</a></sup> The number of attention computations for an autoregressive model is <em>O</em>(<em>n</em><sup>2</sup>).</p>",
      "contains_links": true,
      "contains_emphasis": true
    },
    {
      "type": "paragraph",
      "content": "23 Convolution operations are often used in image generation models like Stable Diffusion.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1716\"><sup><a href=\"ch09.html#id1716-marker\">23</a></sup> Convolution operations are often used in image generation models like Stable Diffusion.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "24 Many companies consider their kernels their trade secrets. Having kernels that allow them to run models faster and cheaper than their competitors is a competitive advantage.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1729\"><sup><a href=\"ch09.html#id1729-marker\">24</a></sup> Many companies consider their kernels their trade secrets. Having kernels that allow them to run models faster and cheaper than their competitors is a competitive advantage.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "25 Talks mentioning the prefill to decode instance ratio include “Llama Inference at Meta” (Meta, 2024).",
      "raw_html": "<p data-type=\"footnote\" id=\"id1741\"><sup><a href=\"ch09.html#id1741-marker\">25</a></sup> Talks mentioning the prefill to decode instance ratio include <a href=\"https://oreil.ly/eMQ_P\">“Llama Inference at Meta”</a> (Meta, 2024).</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "26 While llama.cpp also has prompt caching, it seems to cache only whole prompts and work for queries in the same chat session, as of this writing. Its documentation is limited, but my guess from reading the code is that in a long conversation, it caches the previous messages and processes only the newest message.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1747\"><sup><a href=\"ch09.html#id1747-marker\">26</a></sup> While llama.cpp also has <a href=\"https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#prompt-caching\">prompt caching</a>, it seems to cache only whole prompts and work for queries in the same chat session, as of this writing. Its documentation is limited, but my guess from reading the code is that in a long conversation, it caches the previous messages and processes only the newest message.</p>",
      "contains_links": true,
      "contains_emphasis": false
    },
    {
      "type": "paragraph",
      "content": "27 During training, the same technique is called data parallelism.",
      "raw_html": "<p data-type=\"footnote\" id=\"id1749\"><sup><a href=\"ch09.html#id1749-marker\">27</a></sup> During training, the same technique is called data parallelism.</p>",
      "contains_links": true,
      "contains_emphasis": false
    }
  ],
  "metadata": {
    "word_count": 13330,
    "reading_time_minutes": 66,
    "extraction_date": "2025-06-29T12:20:44.161348"
  },
  "content_summary": {
    "figures": [],
    "tables": [],
    "code_blocks": [],
    "headings": [],
    "images": []
  },
  "cross_references": {
    "internal_links": [
      {
        "target": "ch09_figure_1_1730130962952524",
        "text": "Figure 9-1",
        "context": "ization of a simple inference service is shown in Figure 9-1."
      },
      {
        "target": "ch09_figure_2_1730130962952613",
        "text": "Figure 9-2",
        "context": "pute-bound or memory bandwidth-bound, as shown in Figure 9-2. This chart is a roofline chart because it resemb"
      },
      {
        "target": "ch09_figure_3_1730130962952638",
        "text": "Figure 9-3",
        "context": "Figure 9-3 visualizes prefilling and decoding."
      },
      {
        "target": "ch09_inference_service_optimization_1730130963008735",
        "text": "“Inference Service Optimization”",
        "context": "parate machines. This technique will be discussed “Inference Service Optimization”."
      },
      {
        "target": "ch09_batching_1730130963008799",
        "text": "“Batching”",
        "context": "n’t significantly impact latency, as discussed in “Batching”. The only real difference is that an online API f"
      },
      {
        "target": "ch09_figure_4_1730130962952660",
        "text": "Figure 9-4",
        "context": "s per minute. A visualization of this is shown in Figure 9-4."
      },
      {
        "target": "ch09_table_1_1730130962971021",
        "text": "Table 9-1",
        "context": "it can be hard to achieve on specific hardware.12 Table 9-1 shows MFU for several models and accelerators."
      },
      {
        "target": "ch09_figure_5_1730130962952692",
        "text": "Figure 9-5",
        "context": "Figure 9-5 shows the MBU for the inference process using Lla"
      },
      {
        "target": "ch09_figure_6_1730130962952710",
        "text": "Figure 9-6",
        "context": "such as scalars, vectors, or tensors, as shown in Figure 9-6."
      },
      {
        "target": "ch09_table_2_1730130962971057",
        "text": "Table 9-2",
        "context": "Table 9-2 shows the FLOP/s specs for different precision fo"
      },
      {
        "target": "ch09_figure_7_1730130962952731",
        "text": "Figure 9-7",
        "context": "cts with three levels of memory, as visualized in Figure 9-7:"
      },
      {
        "target": "ch09_figure_8_1730130962952759",
        "text": "Figure 9-8",
        "context": "r, many techniques might cause model degradation. Figure 9-8 shows the same Llama models’ performance on diffe"
      },
      {
        "target": "ch09_figure_9_1730130962952786",
        "text": "Figure 9-9",
        "context": ", xt + 2, …, xt + j. The process is visualized in Figure 9-9."
      },
      {
        "target": "ch09_figure_10_1730130962952808",
        "text": "Figure 9-10",
        "context": "f how inference with reference works are shown in Figure 9-10."
      },
      {
        "target": "ch09_figure_11_1730130962952823",
        "text": "Figure 9-11",
        "context": "omising combination. The process is visualized in Figure 9-11."
      },
      {
        "target": "ch09_figure_12_1730130962952844",
        "text": "Figure 9-12",
        "context": "hen added to the KV cache, which is visualized in Figure 9-12."
      },
      {
        "target": "ch09_figure_13_1730130962952862",
        "text": "Figure 9-13",
        "context": "-based model to make them run faster, as shown in Figure 9-13."
      },
      {
        "target": "ch09_figure_14_1730130962952879",
        "text": "Figure 9-14",
        "context": "Figure 9-14 shows how much throughput improvement the PyTorch"
      },
      {
        "target": "ch09_figure_15_1730130962952896",
        "text": "Figure 9-15",
        "context": "c batching and dynamic batching are visualized in Figure 9-15."
      },
      {
        "target": "ch09_figure_16_1730130962952915",
        "text": "Figure 9-16",
        "context": "also called in-flight batching, is visualized in Figure 9-16."
      },
      {
        "target": "ch09_figure_17_1730130962952933",
        "text": "Figure 9-17",
        "context": "A prompt cache is visualized in Figure 9-17. It’s also called a context cache or prefix cache"
      },
      {
        "target": "ch09_table_3_1730130962971081",
        "text": "Table 9-3",
        "context": "st and latency of different scenarios is shown in Table 9-3.26"
      },
      {
        "target": "ch09_figure_18_1730130962952949",
        "text": "Figure 9-18",
        "context": "split one of the matrices columnwise, as shown in Figure 9-18."
      },
      {
        "target": "ch09_figure_19_1730130962952966",
        "text": "Figure 9-19",
        "context": "sequent parts, enabling overlapping computations. Figure 9-19 shows what pipeline parallelism looks like on fou"
      },
      {
        "target": "ch09_figure_19_1730130962952966",
        "text": "Figure 9-19",
        "context": "Figure 9-19 shows a batch can be split into smaller micro-bat"
      },
      {
        "target": "ch09_prompt_caching_1730130963008914",
        "text": "“Prompt caching”",
        "context": "7 As discussed in “Prompt caching”, it’s common to know in advance the system prompt"
      }
    ],
    "figure_references": [
      {
        "reference": "Figure 9-1",
        "figure_id": "9-1",
        "position": 3526
      },
      {
        "reference": "Figure 9-1",
        "figure_id": "9-1",
        "position": 3543
      },
      {
        "reference": "Figure 9-2",
        "figure_id": "9-2",
        "position": 6502
      },
      {
        "reference": "Figure 9-2",
        "figure_id": "9-2",
        "position": 6978
      },
      {
        "reference": "Figure 9-3",
        "figure_id": "9-3",
        "position": 8091
      },
      {
        "reference": "Figure 9-3",
        "figure_id": "9-3",
        "position": 8140
      },
      {
        "reference": "Figure 9-4",
        "figure_id": "9-4",
        "position": 19929
      },
      {
        "reference": "Figure 9-4",
        "figure_id": "9-4",
        "position": 19943
      },
      {
        "reference": "Figure 9-5",
        "figure_id": "9-5",
        "position": 24050
      },
      {
        "reference": "Figure 9-5",
        "figure_id": "9-5",
        "position": 24306
      },
      {
        "reference": "Figure 9-6",
        "figure_id": "9-6",
        "position": 29158
      },
      {
        "reference": "Figure 9-6",
        "figure_id": "9-6",
        "position": 29172
      },
      {
        "reference": "Figure 9-7",
        "figure_id": "9-7",
        "position": 32498
      },
      {
        "reference": "Figure 9-7",
        "figure_id": "9-7",
        "position": 33823
      },
      {
        "reference": "Figure 9-8",
        "figure_id": "9-8",
        "position": 37612
      },
      {
        "reference": "Figure 9-8",
        "figure_id": "9-8",
        "position": 37740
      },
      {
        "reference": "Figure 9-9",
        "figure_id": "9-9",
        "position": 43371
      },
      {
        "reference": "Figure 9-9",
        "figure_id": "9-9",
        "position": 43614
      },
      {
        "reference": "Figure 9-10",
        "figure_id": "9-10",
        "position": 47345
      },
      {
        "reference": "Figure 9-10",
        "figure_id": "9-10",
        "position": 47360
      },
      {
        "reference": "Figure 9-11",
        "figure_id": "9-11",
        "position": 49942
      },
      {
        "reference": "Figure 9-11",
        "figure_id": "9-11",
        "position": 49957
      },
      {
        "reference": "Figure 9-12",
        "figure_id": "9-12",
        "position": 51176
      },
      {
        "reference": "Figure 9-12",
        "figure_id": "9-12",
        "position": 51191
      },
      {
        "reference": "Figure 9-13",
        "figure_id": "9-13",
        "position": 57136
      },
      {
        "reference": "Figure 9-13",
        "figure_id": "9-13",
        "position": 57152
      },
      {
        "reference": "Figure 9-14",
        "figure_id": "9-14",
        "position": 61154
      },
      {
        "reference": "Figure 9-14",
        "figure_id": "9-14",
        "position": 61489
      },
      {
        "reference": "Figure 9-15",
        "figure_id": "9-15",
        "position": 64409
      },
      {
        "reference": "Figure 9-15",
        "figure_id": "9-15",
        "position": 64424
      },
      {
        "reference": "Figure 9-16",
        "figure_id": "9-16",
        "position": 65595
      },
      {
        "reference": "Figure 9-16",
        "figure_id": "9-16",
        "position": 65610
      },
      {
        "reference": "Figure 9-17",
        "figure_id": "9-17",
        "position": 68431
      },
      {
        "reference": "Figure 9-17",
        "figure_id": "9-17",
        "position": 68496
      },
      {
        "reference": "Figure 9-18",
        "figure_id": "9-18",
        "position": 72671
      },
      {
        "reference": "Figure 9-18",
        "figure_id": "9-18",
        "position": 72925
      },
      {
        "reference": "Figure 9-19",
        "figure_id": "9-19",
        "position": 73291
      },
      {
        "reference": "Figure 9-19",
        "figure_id": "9-19",
        "position": 73366
      },
      {
        "reference": "Figure 9-19",
        "figure_id": "9-19",
        "position": 73451
      },
      {
        "reference": "as shown in Figure 9-2",
        "figure_id": "9-2",
        "position": 6490
      },
      {
        "reference": "as shown in Figure 9-6",
        "figure_id": "9-6",
        "position": 29146
      },
      {
        "reference": "as shown in Figure 9-13",
        "figure_id": "9-13",
        "position": 57124
      },
      {
        "reference": "as shown in Figure 9-18",
        "figure_id": "9-18",
        "position": 72659
      }
    ],
    "table_references": [
      {
        "reference": "Table 9-1",
        "table_id": "9-1",
        "position": 23670
      },
      {
        "reference": "Table 9-1",
        "table_id": "9-1",
        "position": 23729
      },
      {
        "reference": "Table 9-2",
        "table_id": "9-2",
        "position": 31007
      },
      {
        "reference": "Table 9-2",
        "table_id": "9-2",
        "position": 31100
      },
      {
        "reference": "Table 9-3",
        "table_id": "9-3",
        "position": 69758
      },
      {
        "reference": "Table 9-3",
        "table_id": "9-3",
        "position": 69772
      }
    ],
    "section_references": [],
    "external_links": [
      {
        "url": "https://oreil.ly/M_aGR",
        "text": "Williams et al., 2009",
        "context": "th-bound were introduced in the paper “Roofline” (Williams et al., 2009).4 Mathematically, an operation can be classified"
      },
      {
        "url": "https://oreil.ly/K3j6t",
        "text": "arithmetic intensity",
        "context": "pute-bound or memory bandwidth-bound based on its arithmetic intensity, which is the number of arithmetic operations per"
      },
      {
        "url": "https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US",
        "text": "LinkedIn, 2024",
        "context": "after a year of deploying generative AI products (LinkedIn, 2024), it’s not uncommon to double or triple the throu"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Goodput",
        "text": "goodput",
        "context": "bad user experience. Instead, some teams focus on goodput, a metric adapted from networking for LLM applica"
      },
      {
        "url": "https://oreil.ly/ludJ2",
        "text": "nvidia-smi",
        "context": "official NVIDIA tool for monitoring GPU usage is nvidia-smi—SMI stands for System Management Interface. One m"
      },
      {
        "url": "https://oreil.ly/tOOOD",
        "text": "Databricks, 2024",
        "context": "nd Inference with Intel Gaudi 2 AI Accelerators” (Databricks, 2024)."
      },
      {
        "url": "https://oreil.ly/Yv4V7",
        "text": "Krizhevsky et al., 2012",
        "context": "cknowledged reason for the popularity of AlexNet (Krizhevsky et al., 2012) is that it was the first paper to successfully u"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
        "text": "GPUs",
        "context": "s that it was the first paper to successfully use GPUs, graphics processing units, to train neural netwo"
      },
      {
        "url": "https://oreil.ly/Xpwco",
        "text": "Google released just a few months before AlexNet",
        "context": "you’d have to use thousands of CPUs, like the one Google released just a few months before AlexNet. Compared to thousands of CPUs, a couple of GPUs"
      },
      {
        "url": "https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units",
        "text": "Advanced Micro Devices (AMD)’s newer generations of GPUs",
        "context": "tors designed to speed up AI workloads, including Advanced Micro Devices (AMD)’s newer generations of GPUs, Google’s TPU (Tensor Processing Unit), Intel’s H"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Tensor_Processing_Unit",
        "text": "Tensor Processing Unit",
        "context": "(AMD)’s newer generations of GPUs, Google’s TPU (Tensor Processing Unit), Intel’s Habana Gaudi, Graphcore’s Intelligent P"
      },
      {
        "url": "https://oreil.ly/oDQOk",
        "text": "Intel’s Habana Gaudi",
        "context": "s of GPUs, Google’s TPU (Tensor Processing Unit), Intel’s Habana Gaudi, Graphcore’s Intelligent Processing Unit (IPU), G"
      },
      {
        "url": "https://oreil.ly/6ySTY",
        "text": "Graphcore’s Intelligent Processing Unit",
        "context": "U (Tensor Processing Unit), Intel’s Habana Gaudi, Graphcore’s Intelligent Processing Unit (IPU), Groq’s Language Processing Unit (LPU), Cer"
      },
      {
        "url": "https://oreil.ly/R7gXn",
        "text": "Groq’s Language Processing Unit",
        "context": "i, Graphcore’s Intelligent Processing Unit (IPU), Groq’s Language Processing Unit (LPU), Cerebras’ Wafer-Scale Quant Processing Uni"
      },
      {
        "url": "https://oreil.ly/ACIty",
        "text": "Cerebras’ Wafer-Scale",
        "context": "nit (IPU), Groq’s Language Processing Unit (LPU), Cerebras’ Wafer-Scale Quant Processing Unit (QPU), and many more being"
      },
      {
        "url": "https://en.wikipedia.org/wiki/List_of_quantum_processors",
        "text": "Quant Processing Unit",
        "context": "uage Processing Unit (LPU), Cerebras’ Wafer-Scale Quant Processing Unit (QPU), and many more being introduced."
      },
      {
        "url": "https://oreil.ly/qSpMK",
        "text": "Desislavov et al. (2023)",
        "context": "g is specialized chips for inference. A survey by Desislavov et al. (2023) shares that inference can exceed the cost of trai"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Neural_Engine",
        "text": "Neural Engine",
        "context": "apacity. Examples of such chips include the Apple Neural Engine, AWS Inferentia, and MTIA (Meta Training and Infe"
      },
      {
        "url": "https://oreil.ly/42LSB",
        "text": "AWS Inferentia",
        "context": "es of such chips include the Apple Neural Engine, AWS Inferentia, and MTIA (Meta Training and Inference Accelerato"
      },
      {
        "url": "https://oreil.ly/XH2bh",
        "text": "MTIA",
        "context": "lude the Apple Neural Engine, AWS Inferentia, and MTIA (Meta Training and Inference Accelerator). Chips"
      },
      {
        "url": "https://oreil.ly/m8daG",
        "text": "Google’s Edge TPU",
        "context": "lerator). Chips designed for edge computing, like Google’s Edge TPU and the NVIDIA Jetson Xavier, are also typically"
      },
      {
        "url": "https://oreil.ly/PRZSQ",
        "text": "NVIDIA Jetson Xavier",
        "context": "or edge computing, like Google’s Edge TPU and the NVIDIA Jetson Xavier, are also typically geared toward inference."
      },
      {
        "url": "https://arxiv.org/abs/1802.04799",
        "text": "Chen et al. (2018)",
        "context": ". Different compute primitives. Image inspired by Chen et al. (2018)."
      },
      {
        "url": "https://oreil.ly/bNAOG",
        "text": "NVIDIA H100 SXM chips",
        "context": "FLOP/s specs for different precision formats for NVIDIA H100 SXM chips."
      },
      {
        "url": "https://en.wikipedia.org/wiki/DDR_SDRAM",
        "text": "DDR SDRAM",
        "context": "To be more specific, CPUs typically use DDR SDRAM (Double Data Rate Synchronous Dynamic Random-Acce"
      },
      {
        "url": "https://en.wikipedia.org/wiki/High_Bandwidth_Memory",
        "text": "HBM",
        "context": "ture. GPUs, particularly high-end ones, often use HBM (high-bandwidth memory), which has a 3D stacked s"
      },
      {
        "url": "https://en.wikipedia.org/wiki/CUDA",
        "text": "CUDA",
        "context": "e interested in GPU programming languages such as CUDA (originally Compute Unified Device Architecture),"
      },
      {
        "url": "https://github.com/triton-lang/triton",
        "text": "OpenAI’s Triton",
        "context": "(originally Compute Unified Device Architecture), OpenAI’s Triton, and ROCm (Radeon Open Compute). The latter is AM"
      },
      {
        "url": "https://github.com/ROCm/ROCm",
        "text": "ROCm",
        "context": "nified Device Architecture), OpenAI’s Triton, and ROCm (Radeon Open Compute). The latter is AMD’s open s"
      },
      {
        "url": "https://oreil.ly/5vRsP",
        "text": "54 billion",
        "context": "n have billions of transistors—an NVIDIA A100 has 54 billion transistors, while an NVIDIA H100 has 80 billion."
      },
      {
        "url": "https://en.wikipedia.org/wiki/Hopper_(microarchitecture)",
        "text": "80 billion",
        "context": "54 billion transistors, while an NVIDIA H100 has 80 billion. When an accelerator is used efficiently, billion"
      },
      {
        "url": "https://oreil.ly/RqY-3",
        "text": "environment",
        "context": "tion threatens to have a staggering impact on the environment, increasing the pressure on companies to invest i"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Green_data_center",
        "text": "green data centers",
        "context": "essure on companies to invest in technologies for green data centers. An NVIDIA H100 running at its peak for a year co"
      },
      {
        "url": "https://oreil.ly/5hFSF",
        "text": "Cerebras (2024)",
        "context": "ality variations. The experiment was conducted by Cerebras (2024)."
      },
      {
        "url": "https://arxiv.org/abs/1810.05270",
        "text": "Liu et al., 2018",
        "context": "can help discover promising model architectures (Liu et al., 2018). These pruned architectures, smaller than the pr"
      },
      {
        "url": "https://arxiv.org/abs/1710.01878",
        "text": "Zhu et al., 2017",
        "context": "architectures, can also be trained from scratch (Zhu et al., 2017)."
      },
      {
        "url": "https://oreil.ly/qwlHE",
        "text": "Frankle and Carbin (2019)",
        "context": "en many encouraging pruning results. For example, Frankle and Carbin (2019) showed that pruning techniques can reduce the non"
      },
      {
        "url": "https://oreil.ly/QYdG8",
        "text": "Kadous et al., 2023",
        "context": "e the same impact on latency as 100 input tokens (Kadous et al., 2023). Improving the autoregressive generation process"
      },
      {
        "url": "https://arxiv.org/abs/1811.03115",
        "text": "Stern et al., 2018",
        "context": "arallel Decoding for Deep Autoregressive Models” (Stern et al., 2018)."
      },
      {
        "url": "https://arxiv.org/abs/2302.01318",
        "text": "Chen et al., 2023",
        "context": "B-parameter draft model of the same architecture (Chen et al., 2023). The draft model can generate a token eight time"
      },
      {
        "url": "https://arxiv.org/abs/2211.17192",
        "text": "Laviathan et al., 2022",
        "context": "lity. A similar speed-up was achieved for T5-XXL (Laviathan et al., 2022)."
      },
      {
        "url": "https://oreil.ly/IaPOB",
        "text": "50 lines of code in PyTorch",
        "context": "s quality. For example, it’s possible to do so in 50 lines of code in PyTorch. It’s been incorporated into popular inference fr"
      },
      {
        "url": "https://oreil.ly/uzg1s",
        "text": "vLLM",
        "context": "porated into popular inference frameworks such as vLLM, TensorRT-LLM, and llama.cpp."
      },
      {
        "url": "https://github.com/NVIDIA/TensorRT-LLM",
        "text": "TensorRT-LLM",
        "context": "d into popular inference frameworks such as vLLM, TensorRT-LLM, and llama.cpp."
      },
      {
        "url": "https://github.com/ggerganov/llama.cpp/pull/2926",
        "text": "llama.cpp",
        "context": "erence frameworks such as vLLM, TensorRT-LLM, and llama.cpp."
      },
      {
        "url": "https://arxiv.org/abs/2304.04487",
        "text": "Yang et al., 2023",
        "context": "Lossless Acceleration of Large Language Models” (Yang et al., 2023), this technique helps achieve two times generati"
      },
      {
        "url": "https://arxiv.org/abs/2402.02057",
        "text": "Fu et al., 2024",
        "context": "ed by the same decoder, as in Lookahead decoding (Fu et al., 2024), or by different decoding heads, as in Medusa (C"
      },
      {
        "url": "https://arxiv.org/abs/2401.10774",
        "text": "Cai et al., 2024",
        "context": "4), or by different decoding heads, as in Medusa (Cai et al., 2024). In Medusa, the original model is extended with"
      },
      {
        "url": "https://oreil.ly/FWYf5",
        "text": "Eassa et al., 2024",
        "context": "generation by up to 1.9× on their HGX H200 GPUs (Eassa et al., 2024)."
      },
      {
        "url": "https://en.wikipedia.org/wiki/Jacobi_method",
        "text": "Jacobi method",
        "context": "tion and integration. Lookahead decoding uses the Jacobi method21 to verify the generated tokens, which works as"
      },
      {
        "url": "https://arxiv.org/abs/2211.05102",
        "text": "(Pope et al., 2022)",
        "context": "and context length 2048, the KV cache totals 3TB (Pope et al., 2022). This is three times the size of that model’s wei"
      },
      {
        "url": "https://arxiv.org/abs/2004.05150v2",
        "text": "Beltagy et al., 2020",
        "context": "nds only to a fixed size window of nearby tokens (Beltagy et al., 2020). This reduces the effective sequence length to a"
      },
      {
        "url": "https://arxiv.org/abs/2405.12981?ref=research.character.ai",
        "text": "Brandon et al., 2024",
        "context": "Both cross-layer attention (Brandon et al., 2024) and multi-query attention (Shazeer, 2019) reduce"
      },
      {
        "url": "https://arxiv.org/abs/1911.02150?ref=research.character.ai",
        "text": "Shazeer, 2019",
        "context": "(Brandon et al., 2024) and multi-query attention (Shazeer, 2019) reduce the memory footprint of the KV cache by r"
      },
      {
        "url": "https://arxiv.org/abs/2305.13245",
        "text": "Ainslie et al., 2023",
        "context": "Grouped-query attention (Ainslie et al., 2023) is a generalization of multi-query attention. In"
      },
      {
        "url": "https://oreil.ly/nLt6A",
        "text": "180 messages",
        "context": "ir average conversation has a dialogue history of 180 messages (2024). Given the typically long sequences, the p"
      },
      {
        "url": "https://github.com/vllm-project/vllm",
        "text": "vLLM",
        "context": "One of the fastest growing inference frameworks, vLLM, gained popularity for introducing PagedAttention"
      },
      {
        "url": "https://arxiv.org/abs/2309.06180",
        "text": "Kwon et al., 2023",
        "context": "memory sharing to improve LLM serving efficiency (Kwon et al., 2023)."
      },
      {
        "url": "https://arxiv.org/abs/2401.18079",
        "text": "Hooper et al., 2024",
        "context": "Other techniques include KV cache quantization (Hooper et al., 2024; Kang et al., 2024), adaptive KV cache compressio"
      },
      {
        "url": "https://arxiv.org/abs/2403.05527",
        "text": "Kang et al., 2024",
        "context": "clude KV cache quantization (Hooper et al., 2024; Kang et al., 2024), adaptive KV cache compression (Ge et al., 2023)"
      },
      {
        "url": "https://arxiv.org/abs/2310.01801",
        "text": "Ge et al., 2023",
        "context": "ang et al., 2024), adaptive KV cache compression (Ge et al., 2023), and selective KV cache (Liu et al., 2024)."
      },
      {
        "url": "https://oreil.ly/ixtBl",
        "text": "Liu et al., 2024",
        "context": "ession (Ge et al., 2023), and selective KV cache (Liu et al., 2024)."
      },
      {
        "url": "https://github.com/Dao-AILab/flash-attention",
        "text": "FlashAttention",
        "context": "wn kernels optimized for attention computation is FlashAttention (Dao et al., 2022). This kernel fused together ma"
      },
      {
        "url": "https://github.com/Dao-AILab/flash-attention",
        "text": "FlashAttention",
        "context": "d, new kernels need to be developed. For example, FlashAttention (Dao et al., 2022) was originally developed prima"
      },
      {
        "url": "https://arxiv.org/abs/2407.08608",
        "text": "Shah et al., 2024",
        "context": "n, FlashAttention-3 was introduced for H100 GPUs (Shah et al., 2024)."
      },
      {
        "url": "https://oreil.ly/_5Nqa",
        "text": "PyTorch, 2023",
        "context": "lama-7B through the following optimization steps (PyTorch, 2023):"
      },
      {
        "url": "https://github.com/apache/tvm",
        "text": "Apache TVM",
        "context": "Compilers can be standalone tools, such as Apache TVM and MLIR (Multi-Level Intermediate Representation"
      },
      {
        "url": "https://mlir.llvm.org",
        "text": "MLIR",
        "context": "s can be standalone tools, such as Apache TVM and MLIR (Multi-Level Intermediate Representation) or inte"
      },
      {
        "url": "https://oreil.ly/6bjVM",
        "text": "torch.compile",
        "context": "integrated into ML and inference frameworks, like torch.compile (a feature in PyTorch), XLA (Accelerated Linear A"
      },
      {
        "url": "https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra",
        "text": "XLA",
        "context": "works, like torch.compile (a feature in PyTorch), XLA (Accelerated Linear Algebra, originally developed"
      },
      {
        "url": "https://github.com/openxla/xla",
        "text": "OpenXLA",
        "context": "by TensorFlow, with an open source version called OpenXLA), and the compiler built into the TensorRT, which"
      },
      {
        "url": "https://github.com/NVIDIA/TensorRT",
        "text": "TensorRT",
        "context": "called OpenXLA), and the compiler built into the TensorRT, which is optimized for NVIDIA GPUs. AI companies"
      },
      {
        "url": "https://oreil.ly/SJ7Mb",
        "text": "Yu et al., 2022",
        "context": "hold up another, as introduced in the paper Orca (Yu et al., 2022). After a request in a batch is completed and its"
      },
      {
        "url": "https://oreil.ly/DlIPs",
        "text": "in-flight batching",
        "context": "occupancy rate. Continuous batching, also called in-flight batching, is visualized in Figure 9-16."
      },
      {
        "url": "https://arxiv.org/html/2401.09670v1",
        "text": "Zhong et al., 2024",
        "context": "to disaggregate prefill and decode. “DistServe” (Zhong et al., 2024) and “Inference Without Interference” (Hu et al.,"
      },
      {
        "url": "https://arxiv.org/abs/2401.11181",
        "text": "Hu et al., 2024",
        "context": "al., 2024) and “Inference Without Interference” (Hu et al., 2024) show that for various popular LLMs and applicati"
      },
      {
        "url": "https://en.wikipedia.org/wiki/NVLink",
        "text": "NVLink",
        "context": "clusters with high-bandwidth connections such as NVLink within a node."
      },
      {
        "url": "https://oreil.ly/Pd6Pk",
        "text": "Gim et al.",
        "context": "Since its introduction in November 2023 by Gim et al., the prompt cache has been rapidly incorporated i"
      },
      {
        "url": "https://oreil.ly/pIHkL",
        "text": "functionality",
        "context": "Is. As of this writing, Google Gemini offers this functionality, with cached input tokens given a 75% discount co"
      },
      {
        "url": "https://oreil.ly/8rtsF",
        "text": "prompt caching",
        "context": "00/one million tokens per hour). Anthropic offers prompt caching that promises up to 90% cost savings (the longer"
      },
      {
        "url": "https://oreil.ly/On2-B",
        "text": "context parallelism",
        "context": "In context parallelism, the input sequence itself is split across differ"
      },
      {
        "url": "https://oreil.ly/jD5Pj",
        "text": "“Lufthansa Delays Chatbot’s Responses to Make It More ‘Human’”",
        "context": "responding too fast, which seemed unnatural. See “Lufthansa Delays Chatbot’s Responses to Make It More ‘Human’” (Ry Crozier, iTnews, May 2017). However, as peopl"
      },
      {
        "url": "https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product?_l=en_US",
        "text": "LinkedIn",
        "context": "9 Time between tokens (TBT) is used by LinkedIn and inter-token latency (ITL) is used by NVIDIA."
      },
      {
        "url": "https://oreil.ly/zHsb8",
        "text": "NVIDIA",
        "context": "LinkedIn and inter-token latency (ITL) is used by NVIDIA."
      },
      {
        "url": "https://arxiv.org/abs/2204.02311",
        "text": "Chowdhery et al., 2022",
        "context": "ut the term MFU was introduced in the PaLM paper (Chowdhery et al., 2022)."
      },
      {
        "url": "https://en.wikipedia.org/wiki/Perceptrons_(book)",
        "text": "Perceptrons: An Introduction to Computational Geometry",
        "context": "y limited capabilities. In their famous 1969 book Perceptrons: An Introduction to Computational Geometry (MIT Press), two AI pioneers, Marvin Minsky and S"
      },
      {
        "url": "https://oreil.ly/mRNCP",
        "text": "rename the GPU",
        "context": "14 There have been discussions on whether to rename the GPU since it’s used for a lot more than graphics (Jon"
      },
      {
        "url": "https://oreil.ly/iK0tN",
        "text": "interview",
        "context": "uly 2018). Jensen Huang, NVIDIA’s CEO, said in an interview (Stratechery, March 2022) that once the GPU took"
      },
      {
        "url": "https://arxiv.org/abs/2007.00072",
        "text": "“Data Movement Is All You Need: A Case Study on Optimizing Transformers”",
        "context": "oint operations in a neural network, according to “Data Movement Is All You Need: A Case Study on Optimizing Transformers” (Ivanov et al., arXiv, v3, November 2021) and “Sc"
      },
      {
        "url": "https://arxiv.org/abs/1802.04799",
        "text": "“Scalable MatMul-free Language Modeling”",
        "context": "rs” (Ivanov et al., arXiv, v3, November 2021) and “Scalable MatMul-free Language Modeling” (Zhu et al., arXiv, June 2024)."
      },
      {
        "url": "https://oreil.ly/y45q6",
        "text": "run fast on TPUs",
        "context": "transformer was originally designed by Google to run fast on TPUs and only later optimized on GPUs."
      },
      {
        "url": "https://en.wikipedia.org/wiki/GDDR_SDRAM",
        "text": "GDDR",
        "context": "17 Lower-end to mid-range GPUs might use GDDR (Graphics Double Data Rate) memory."
      },
      {
        "url": "https://oreil.ly/eMQ_P",
        "text": "“Llama Inference at Meta”",
        "context": "ning the prefill to decode instance ratio include “Llama Inference at Meta” (Meta, 2024)."
      },
      {
        "url": "https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#prompt-caching",
        "text": "prompt caching",
        "context": "26 While llama.cpp also has prompt caching, it seems to cache only whole prompts and work fo"
      }
    ],
    "bidirectional_refs": {}
  }
}